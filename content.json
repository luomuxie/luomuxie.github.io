{"posts":[{"title":"Coffee","text":"喝了咖啡，到下午头痛 喝咖啡后出现头痛的情况可以由多种原因引起，以下是一些可能的解释： 咖啡因撤退：如果你习惯每天喝很多咖啡，并且在某一天你的摄入量减少，可能会经历咖啡因撤退症状，其中包括头痛。这是因为咖啡因具有轻微的成瘾性，身体会对其产生依赖。 脱水：咖啡有利尿作用，可以增加身体的水分排出。如果喝咖啡后没有摄入足够的水分，可能会导致脱水，而脱水是头痛的常见原因。 血糖水平变化：咖啡可以影响你的血糖水平，尤其是如果你的咖啡中加入了糖或者你在空腹时饮用。血糖水平的快速上升和下降可能导致头痛。 血管效应：咖啡因是一种血管收缩剂，它可以减少大脑血管的血流。对一些人来说，这种效应可能会引发头痛。 咖啡因过量：过量摄入咖啡因也会引起头痛，以及焦虑、失眠和心跳加快等症状。每个人对咖啡因的敏感度不同，有的人可能只要喝一小杯就会感到不适。 饮食习惯：如果你习惯在喝咖啡时搭配特定的食物，而某天没有这么做，这种变化也可能导致头痛。 应激反应：对于一些人来说，咖啡因可以提高压力水平，因为它刺激肾上腺分泌更多的肾上腺素。这种应激反应有时也会引起头痛。 如果你经常在喝咖啡后感到头痛，建议减少咖啡因的摄入量，保持充分的水分，并观察是否有所改善。此外，如果头痛持续存在，建议咨询医生，以排除其他可能的健康问题。 初步认为咖啡因导致的血管效应引起 这可能是因为咖啡因使得脑部血管收缩。对某些人来说，尤其是那些对咖啡因比较敏感的人，这种血管的变化可能会触发头痛。这种头痛在医学上有时被称为“咖啡因头痛”。 如果您认为咖啡因是导致头痛的原因，您可以尝试以下几种方法来减轻或预防头痛： 减少咖啡因摄入：渐渐减少每天的咖啡或其他含咖啡因饮料的摄入量，而不是突然停止。 水分补充：确保您喝足够的水，因为咖啡因具有利尿作用，容易导致脱水。 规律饮用：如果您每天都喝咖啡，尝试每天在相同的时间饮用相同量的咖啡，这样可以帮助防止因摄入量的波动而导致的头痛。 缓慢停止：如果您打算减少咖啡因的摄入量，建议逐渐减少，以避免撤断症状。 替代品：尝试用其他饮品替代咖啡，如脱咖啡因咖啡或茶，或者选择含咖啡因更少的饮料。 放松和减压：经常练习放松技巧，如深呼吸、瑜伽或轻度运动，可能有助于减少或管理由于咖啡因引起的头痛。 如果您采取了这些措施仍然经常出现头痛，或者头痛影响到了您的生活质量，最好咨询医生或头痛专科医生以获取更具体的建议和治疗。 半个月后 已经没有头痛现像，早上喝一杯，能在一天都能达到较好的精神 11/7 。。。 11/21 11/22 食用时间 下午14：30-15：30 7:00-8:00 7:00-8:00 食用量 约1/4杯 两个6度胶囊 两个11度胶囊 晚上入眠时间 晚上2点 +退黑素基本24前 个人状态 只有轻微头痛，加班后精神也较好 午睡后有一点困过后基本能维持一天精神","link":"/2023/11/07/2023-11-07-Coffee/"},{"title":"DynamicSpreadEffect","text":"在网上看到一个OpenGL写的很炫丽的效果，忍不住自己也实现了一把 核心代码主要是这两个方法: 123456789101112131415161718192021222324252627282930vec3 palette( float t ) { vec3 a = vec3(0.5, 0.5, 0.5); vec3 b = vec3(0.5, 0.5, 0.5); vec3 c = vec3(1.0, 1.0, 1.0); vec3 d = vec3(0.263,0.416,0.557); return a + b*cos( 6.28318*(c*t+d) );}void mainImage( out vec4 fragColor, in vec2 fragCoord ) { vec2 uv = (fragCoord * 2.0 - iResolution.xy) / iResolution.y; vec2 uv0 = uv; vec3 finalColor = vec3(0.0); for (float i = 0.0; i &lt; 1.0; i++) { uv = fract(uv*1.5) - 0.5; vec3 col = palette(length(uv0) + i*.4 + iTime*.4); float d = length(uv) * exp(-length(uv0)); d = sin(d*8. + iTime)/8.; d = abs(d); d = pow(0.01 / d, 1.2); finalColor += col * d; } fragColor = vec4(finalColor, 1.0);} 现来分析一下： 主要从OpenGL传入的参数是这两个： 12uniform vec2 iResolution;uniform float iTime; iResolution：接收屏幕的宽高 iTime：开始到当前时刻的时间（通常是秒）。用来创建动态效果 分析图中的效果： 仔细看，这个效果期实就是圆在整个画面的平铺，然后添加一些规律性的颜色变化 实现步骤： 1、首先把原点从屏幕的左下角移动到中心，并使坐标系与屏幕的宽度和高度无关 1vec2 uv = (fragCoord * 2.0 - iResolution.xy) / iResolution.y; 将坐标乘以 2 并从中减去屏幕分辨率（fragCoord * 2.0 - iResolution.xy），通常是为了将屏幕坐标从默认的 [0, width] x [0, height] 范围转换到 [-1, 1] x [-1, 1] 中心原点：坐标原点现在是屏幕中心，这对于创建对称图案很有帮助。 均匀分布：无论屏幕的分辨率或宽高比如何，图案在屏幕上都是均匀分布的。 范围标准化：坐标系被标准化到 -1 到 1 的范围内，这使得数学函数（如三角函数）的使用变得更加直观，因为这些函数经常使用这个范围内的值。 最后一步除以 iResolution.y 是为了确保在不同宽高比的屏幕上图案保持相同的比例和形状。这样，无论屏幕尺寸如何，图案在垂直方向上的大小总是相同的，这样就可以创建出独立于屏幕分辨率和宽高比的视觉效果。 2、画一个由内向外扩散的圆 1float d = length(uv) * exp(-length(uv0)); 3、然后让这个圆根据正弦起来或余弦动起来 1d = sin(d*8. + iTime)/8.; sin(d*8. + iTime)：这部分创建了一个随时间变化的波动效果。d*8. 的作用是增加波形的频率，这意味着边缘在一定的距离内会快速地上下波动。iTime 使得这个波动随时间动态变化，模拟出波形随时间的移动。 /8.：这个操作减少了波形振幅，使效果更微妙。没有这个除法，振幅将会是 -1 到 1，这会造成非常强烈的效果，可能会让图案的边缘看起来很尖锐或者变化很剧烈。 d的颜色曲线图是这样的 4、增加像素屏幕中心附近的像素值 12d = abs(d); d = pow(0.01 / d, 1.2); d = abs(d); 取 d 的绝对值，确保没有负值。正弦函数的结果在 -1 和 1 之间波动，而取绝对值后，波动将只在 0 和 1 之间。 d = pow(0.01 / d, 1.2); 这一行进一步操纵了 d 值，通过取 0.01 / d 的 1.2 次方，它极大地增强了屏幕中心附近的像素值，并且随着从中心向外的距离增加，这个效果迅速减小。这样做的结果是，屏幕中心会比边缘明亮，创建了一种光晕或辉光的效果。 4、重新计算uv，对uv平铺 1uv = fract(uv*1.5) - 0.5; fract(uv*1.5) 会取 uv 各自坐标的小数部分，这本身就是一个将图案在每个单位坐标内重复的过程。但由于 fract 的结果是从 0.0 到 1.0，这意味着重复的图案将以屏幕的左下角为起点进行排列。减去 0.5 的目的是为了将这个重复的图案的中心对齐到屏幕中心。如果不减去 0.5，那么平铺的图案将会以屏幕的左下角为中心点进行排列。 5、再循环和上色 在外围添加一个循环，和加上颜色效果 12345678vec3 palette( float t ) { vec3 a = vec3(0.5, 0.5, 0.5); vec3 b = vec3(0.5, 0.5, 0.5); vec3 c = vec3(1.0, 1.0, 1.0); vec3 d = vec3(0.263,0.416,0.557); return a + b*cos( 6.28318*(c*t+d) );} 这里定义了四个三维向量（vec3），在 RGB 颜色空间中，这些向量代表不同的颜色组成部分： a：基础颜色，对最终颜色有一个均匀的加成，这里是中灰色。 b：颜色振幅，定义颜色波动的最大范围，也是中灰色。 c：颜色频率，决定颜色随参数 t 变化的速度，这里设置为1，意味着颜色变化会随着 t 的变化而周期性变化。 d：颜色相位，决定颜色波动的起始点，不同的颜色通道可以有不同的起始点，产生不同的颜色偏移。 12return a + b*cos( 6.28318*(c*t+d) ); cos(6.28318*(c*t+d))：这里使用了余弦函数来创建一个周期为 2π（6.28318 约等于 2π）的波。这个波根据时间 t 以及颜色频率 c 和颜色相位 d 的变化而变化。由于 c 是 (1.0, 1.0, 1.0)，这意味着颜色的变化是同步的；如果 c 的各个分量不同，则颜色的变化会有所错位，产生更丰富的效果。 a + b*...：余弦函数产生的结果介于 -1 和 1 之间，与 b 相乘后，我们得到了一个在 -0.5 到 0.5 范围内变化的颜色分量，然后加上基础颜色 a。这意味着颜色将在 a-b 和 a+b 之间变化，由于 a 和 b 是一样的，所以颜色将在灰色的基础上变化。 总的来说，palette 函数生成的颜色会随着时间 t 的变化而在一个中心灰色的基础上振荡，创建了一个动态的彩色效果，使得着色器生成的图像具有随时间变化的颜色波动。 最终成功啦","link":"/2023/11/11/2023-11-11-DynamicSpreadEffect/"},{"title":"BookReadingList","text":"《如何阅读一本书》莫提默·J. 艾德勒（Mortimer J. Adler）和查尔斯·范多伦（Charles Van Doren）（正在读） 《知行合一》王阳明 《财富自由之路》李笑笑（已读，感觉可读性不大，但又有一点） 《论自由》约翰·穆勒（已读一半，剩下下似乎读不太下去了，太晦涩难懂了） 《掌控习惯》詹姆斯·克利尔（已读，很好的书） 《高效能人士的七个习惯》斯蒂芬·柯维（已读）、 《国富论》 《资本论》","link":"/2023/11/08/2023-11-08-BookReadingList/"},{"title":"Tea","text":"前提：犯困 每每上班下午就开始很困，想着以为靠个人意志和作息的调整能得到解决 偿试1、 把休息时间提前到11点开始上床睡觉，但没想到根本睡不着，在床上一直躺到12点才慢慢开始睡着，有考虑过要不要吃退黑素，但秉持着是药三分毒的心态，暂时放弃。 偿试2、 了解到要使神经放松下来，在逛街路上看到有香薰，买了，睡前熏了半个小时，没想到买的类型味道太重反而刺激了神经，根本也法睡着，无效，暂时放弃。 偿试3、 试着在下午吃点东西，或多走走，最后感觉效果不太，该困的还是困，没有精神还是没有精神 偿试喝茶 公认比较提前的两个东西就是咖啡和茶了： 咖啡偿试在另一个文章，这里就专门来记录喝茶的经历吧： 绿茶： 碧螺春（金俊眉）：","link":"/2023/11/08/2023-11-08-Tea/"},{"title":"AboutLearning","text":"当学习不再是为了生活或工作，其实这个过程是个很开心的过程 ，特别是当可以展示在别人面前的时候 学习的场合 上班时间更适合敲代码或与chatGPT交换各种对话 在外面的碎片时间，更适合用来与人进行对话思考 在家，早上上班前：","link":"/2023/11/14/2023-11-14-AboutLearning/"},{"title":"如何阅读一本书","text":"检视阅读 检视阅读一：系统性地略读或预读 这个阶段的目标是在有限的时间内获得一本书的整体理解和基本内容，它是一种更加系统和有目的的快速阅读。检视阅读的目的是帮助读者快速决定一本书是否值得进行更深入的分析阅读。通过这种方式，读者可以有效地筛选大量信息，确定哪些书籍最符合他们的需要和兴趣。这种阅读方法特别适用于研究、学习和在图书馆或书店选择书籍时。 这包括快速浏览书的封面、前言、目录、索引和尾章。重点是了解书的结构和主要论点。 检视阅读二：粗浅的阅读 粗浅阅读指的是在不完全理解书籍的所有部分的情况下进行的阅读。这种阅读方式强调的是不要过早地停下来寻找你不理解的每一个细节。换句话说，当你遇到难以理解的段落或概念时，不要立即停下来挖掘或研究，而是应该继续阅读。 这种方法的目的是获得对书籍的整体理解，而不是在第一次阅读时就深入每一个细节。通过粗浅阅读，你可以获得关于书的主要论题和结构的大致了解，这为以后可能的深入阅读奠定了基础。 所以，检视阅读包括了对书籍的快速预览，以及在这个过程中实施的粗浅阅读，后者允许你快速地跨越不理解的内容，以便在有限的时间内获得对书籍的整体印象。 阅读的四个问题： 在《如何阅读一本书》中，艾德勒和范多倫提出了在阅读任何一本书时应该考虑的四个主要问题，这些问题旨在帮助读者更深入地理解和分析书籍内容： 整体内容是什么？ 这个问题要求读者理解书的基本主题和总体结构。这包括识别主要论题、主要论点、书的结构和基本方法论。 作者详细说了什么，怎么说的？ 这里的关键是理解作者的主要论点和论据，以及这些论点是如何被组织和呈现的。这涉及到分析章节、段落和句子，以及作者使用的特定例子和证据。 书中的内容是真的吗？ 对于这个问题，读者需要独立评估作者的论点和证据。这不仅仅是接受作者的观点，而是批判性地考虑其有效性和逻辑一致性。 这本书跟你有什么关系？ 这个问题要求读者思考书的内容如何与自己的知识、经验和生活相关。它涉及到理解书的信息如何适用于你的个人情况，以及你可以如何使用这些信息。 这些问题构成了阅读的一个框架，通过它们，读者可以更系统地分析和理解所阅读的书籍。这些问题不仅适用于学术文本，也适用于非小说类和小说类作品。 分析阅读 第一阶段 关键步骤： 分类书籍的类型：确定书籍属于哪种类型（如小说、历史、科学、哲学等）。这对于理解作者的意图和书籍的结构非常重要。 概述书的主题：确定书的主要题目或问题。这意味着要理解书的主旨或中心思想。 列出书的主要部分：理解书的结构，将其分解成主要部分、章节或段落。这有助于揭示作者组织材料的方式。 确定作者的问题和解决方案：确定作者试图解决的问题，并概述他们提出的解决方案。 这个阶段的目标是为深入理解和评估书籍的内容打下坚实的基础。通过完成这些步骤，读者能够建立起对书籍内容的初步但全面的理解，为更深层次的分析和批判性评估做好准备。 第二阶段 与作者找出共通的词义 识别关键词汇：在阅读时注意书中出现的关键词汇，尤其是那些看起来是作者用来表达其主要概念和论点的词。 确定词义：查找这些关键词汇的定义。这可以通过查阅词典、参考文献，或是根据上下文来理解作者对这些词汇的特定用法。 检查词义的一致性：在整本书中检查这些关键词汇的使用是否一致。如果作者在不同的章节中使用同一词汇却有不同的含义，这需要被识别和理解。 与作者的定义和使用对齐：在理解了作者使用这些关键词汇的方式之后，确保你在思考和分析书籍内容时，也是以这种方式来理解和使用这些词汇。","link":"/2023/11/11/2023-11-11-HowToReadABook/"},{"title":"LearnOpenGL10-FrameBuffers","text":"深入理解OpenGL中的帧缓冲（Framebuffer） 引言 简要介绍帧缓冲的概念。 在图形编程和游戏开发中，渲染引擎的核心任务之一就是将3D环境转换成2D图像，以便显示在屏幕上。这个转换过程涉及大量的计算和图像处理，而这些处理的结果就存储在一个被称为帧缓冲（Framebuffer）的特殊内存区域中。 帧缓冲可以看作是一幅画布，它不仅仅存储最终要展示在屏幕上的颜色数据，还可以包含深度信息（用于判断图像前后关系），甚至模板数据（用于复杂的图像处理）。在现代图形API如OpenGL中，帧缓冲不仅仅局限于最终的屏幕显示，它还可以用于所谓的离屏渲染，这允许开发者创建复杂的图形效果，如动态阴影、反射、后处理效果等。 理解帧缓冲的工作方式和它在渲染管线中的作用，对于任何想要深入了解3D图形编程的人来说都是必不可少的。 解释帧缓冲在现代图形渲染中的重要性。 帧缓冲在现代图形渲染中的重要性可以从以下几个方面进行阐述： 核心渲染组件：帧缓冲是现代图形渲染流程的核心组成部分。它负责存储渲染管线生成的图像数据，包括颜色、深度和模板信息。无论是简单的2D图像还是复杂的3D场景，帧缓冲都是渲染结果的最终归宿。 渲染质量控制：帧缓冲通过存储深度信息，使得深度测试成为可能，从而正确处理场景中物体的遮挡关系。这对于创建视觉上准确和吸引人的3D图像至关重要。 离屏渲染与后处理：帧缓冲的灵活性使得离屏渲染（Off-Screen Rendering）成为可能。开发者可以将图像渲染到非默认的帧缓冲中，然后对这些图像应用一系列的后处理效果，如模糊、颜色校正等，从而增强最终图像的视觉效果。 多样化的图形效果：使用帧缓冲，开发者可以实现各种高级图形效果，例如阴影映射、环境映射、反射和折射效果等。这些技术在提高游戏和图形应用的视觉质量方面发挥着重要作用。 性能优化：通过智能管理帧缓冲，可以在保持高质量渲染的同时优化性能。例如，通过降低离屏渲染的分辨率，可以在不牺牲太多视觉质量的情况下提高渲染效率。 灵活性和可扩展性：现代图形API如OpenGL和Vulkan允许开发者创建多个帧缓冲，每个帧缓冲可以有不同的格式和用途，这为处理复杂的图形场景和算法提供了极大的灵活性和可扩展性。 总结来说，帧缓冲在现代图形渲染中的重要性体现在它对渲染质量的提升、对高级图形效果的支持，以及对渲染性能优化方面的贡献。理解和有效利用帧缓冲，对于任何图形程序员来说都是至关重要的。 第1部分：帧缓冲基础 帧缓冲的定义和作用 定义 帧缓冲是一个图形硬件或软件构造，用于存储渲染图像的数据。它是图形渲染过程中的一个关键组件，负责收集和保存从3D转换为2D后的像素数据。 作用 图像数据的存储： 帧缓冲主要用于存储渲染过程中生成的像素数据。这包括颜色信息（在颜色缓冲区中存储），以及可能的深度信息（在深度缓冲区中存储）和模板信息（在模板缓冲区中存储）。 渲染目标： 在渲染过程中，帧缓冲作为渲染目标。这意味着无论是简单的2D渲染还是复杂的3D场景，最终的渲染结果都会被输出到帧缓冲中。 深度测试和模板测试的基础： 帧缓冲的深度缓冲区允许进行深度测试，从而正确处理场景中物体的遮挡关系。 模板缓冲区则支持模板测试，允许对渲染进行更精细的控制，如限制渲染到特定区域。 后处理和图像效果： 帧缓冲不仅用于最终输出图像，还可以用于离屏渲染，这是一种将渲染结果输出到非显示的缓冲区的技术。这允许开发者对图像进行后处理，实现各种视觉效果，如模糊、颜色调整等。 多通道渲染： 在更高级的应用中，帧缓冲可以支持多通道渲染，允许同时输出多种不同类型的图像数据（如同时渲染颜色、法线和深度信息）。 总结来说，帧缓冲在图形渲染中的作用非常重要，它不仅是渲染结果的最终归宿，也是实现高级渲染技术的基础设施。理解帧缓冲的工作方式对于深入理解图形渲染过程至关重要。 帧缓冲的组成 颜色附件（Color Attachment） 颜色附件是帧缓冲中的一个组件，专门用于保存渲染的颜色数据。在OpenGL等图形API中，这些数据通常被存储在纹理（Texture）或渲染缓冲对象（Renderbuffer Object）中。 作用： 存储像素颜色：颜色附件的主要作用是存储每个像素的颜色值。这包括红色、绿色、蓝色以及可选的透明度（Alpha）值。 渲染目标：在图形渲染过程中，颜色附件作为渲染的目的地，意味着所有渲染的输出（比如通过片段着色器生成的颜色）都会被写入到这里。 使用和配置 创建和绑定：颜色附件可以通过创建并绑定一个纹理或渲染缓冲对象来设置。这通常涉及到OpenGL的 glGenTextures、glBindTexture、glTexImage2D 等函数来创建和配置纹理，或者 glGenRenderbuffers、glBindRenderbuffer、glRenderbufferStorage 等函数来设置渲染缓冲对象。 附加到帧缓冲：创建和配置好后，这个纹理或渲染缓冲对象需要被附加到帧缓冲对象上，通常使用 glFramebufferTexture2D 或 glFramebufferRenderbuffer 函数。 1234567891011//create a texture glGenTextures(1, &amp;textureColorbuffer); //bind the textureglBindTexture(GL_TEXTURE_2D, textureColorbuffer);//set the textureglTexImage2D(GL_TEXTURE_2D, 0, GL_RGB, SCR_WIDTH, SCR_HEIGHT, 0, GL_RGB,GL_UNSIGNED_BYTE, NULL);//set the texture parameterglTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_LINEAR);glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_LINEAR);//attach the texture to the framebufferglFramebufferTexture2D(GL_FRAMEBUFFER, GL_COLOR_ATTACHMENT0, GL_TEXTURE_2D, textureColorbuffer, 0); 深度附件（Depth Attachment） 定义：深度附件是帧缓冲中用于存储渲染过程中每个像素的深度信息的组件。深度信息通常用来决定哪些部分的物体应该显示在其他物体的前面，即处理物体之间的遮挡关系。 作用： 深度测试：在3D渲染中，深度附件允许进行深度测试（Z-Buffering），这是决定哪些像素应该被绘制到屏幕上的重要过程。它通过比较像素的深度值与深度缓冲中存储的值来决定像素是否可见。 遮挡处理：深度附件帮助渲染器正确处理场景中物体的遮挡关系，确保渲染结果的视觉正确性。 深度附件对于创建真实感强烈的3D场景至关重要，因为没有正确的深度信息，物体间的遮挡关系就无法正确渲染，从而影响整个场景的视觉效果和真实性。 理解深度附件的工作原理和它在图形渲染中的应用对于任何3D图形开发者来说都是基础且必要的技能。 深度附件在3D图形渲染中的作用不可小觑，它不仅确保了渲染的准确性，还为实现各种高级渲染效果提供了基础。 模板附件（Stencil Attachment） 定义：模板附件是帧缓冲的一部分，用于存储模板值，这些值可以在渲染过程中被用来控制像素是否应该被绘制。 作用： 像素级控制：模板测试使用模板附件中的值来确定是否以及如何绘制特定的像素。这允许在渲染过程中实现复杂的图像效果和优化。 遮罩和模板操作：模板附件常用于实现遮罩效果，例如限制渲染到特定区域、轮廓绘制、反射等。 多步渲染过程中的辅助：在多步渲染流程中，模板附件可以用来标记已经处理过的像素，以避免重复渲染。 模板附件和深度附件的使用：rbo 模板附件（Stencil Attachment）和深度附件（Depth Attachment）通常都是通过声明一个渲染缓冲对象（Renderbuffer Object）来配置的，尤其是在使用OpenGL进行3D图形渲染时。这两种附件通常与帧缓冲对象（Framebuffer Object, FBO）一起使用以存储和管理渲染过程中的深度和模板数据。 创建Renderbuffer Object： 使用 glGenRenderbuffers 创建一个或多个渲染缓冲对象。 绑定Renderbuffer： 使用 glBindRenderbuffer 将创建的渲染缓冲对象绑定到渲染缓冲目标。 设置Renderbuffer的存储： 对于深度附件，使用 glRenderbufferStorage 设置存储格式为深度格式，例如 GL_DEPTH_COMPONENT24。 对于模板附件，设置存储格式为模板格式，例如 GL_STENCIL_INDEX8。 对于同时包含深度和模板信息的渲染缓冲对象，使用深度-模板格式，如 GL_DEPTH24_STENCIL8。 附加到Framebuffer： 使用 glFramebufferRenderbuffer 将渲染缓冲对象附加到帧缓冲对象的相应附件点上，例如 GL_DEPTH_ATTACHMENT 或 GL_STENCIL_ATTACHMENT。 注意事项 深度和模板的结合：在许多应用中，深度和模板缓冲区会结合在一个渲染缓冲对象中（使用格式如 GL_DEPTH24_STENCIL8），这样做可以提高效率和性能。 性能考虑：使用渲染缓冲对象而不是纹理作为深度或模板附件时，通常可以获得更好的性能，因为渲染缓冲对象的数据不需要采样，只用于测试或写入操作。 12345678910111213141516//create a renderbuffer objectglGenRenderbuffers(1, &amp;rbo);//bind the renderbufferglBindRenderbuffer(GL_RENDERBUFFER, rbo);//set the renderbufferglRenderbufferStorage(GL_RENDERBUFFER, GL_DEPTH24_STENCIL8, SCR_WIDTH, SCR_HEIGHT);//attach the renderbuffer to the framebufferglFramebufferRenderbuffer(GL_FRAMEBUFFER, GL_DEPTH_STENCIL_ATTACHMENT, GL_RENDERBUFFER, rbo);//check the framebuffer is completeif (glCheckFramebufferStatus(GL_FRAMEBUFFER) != GL_FRAMEBUFFER_COMPLETE){ printf(&quot;framebuffer is not complete!\\n&quot;);}//unbind the framebufferglBindFramebuffer(GL_FRAMEBUFFER, 0); rbo的隐式使用： 在主渲染这个过程中，rbo 作为 framebuffer 的一部分被隐式使用。尤其是在清除和深度测试阶段，rbo 存储的深度信息被用来处理片段的深度值，确定哪些片段应该被丢弃，哪些应该被保留。 总结来说，rbo 没有被直接引用或修改，但它作为深度和模板附件在渲染过程中发挥着重要作用，特别是在深度测试和模板测试中。通过存储深度信息，rbo 帮助OpenGL确定如何处理场景中不同对象的遮挡关系。如果没有用深度测试和模板缓冲的话，实现是不用申请rbo的 。 第2部分：使用帧缓冲 创建和配置帧缓冲 如何在OpenGL中创建和设置帧缓冲。 12345678910111213141516171819202122232425262728293031323334353637void framebuffers::initFramebuffers(){ //create the framebuffer object glGenFramebuffers(1, &amp;fbo); //bind the framebuffer glBindFramebuffer(GL_FRAMEBUFFER, fbo); //create a texture glGenTextures(1, &amp;textureColorbuffer); //bind the texture glBindTexture(GL_TEXTURE_2D, textureColorbuffer); //set the texture glTexImage2D(GL_TEXTURE_2D, 0, GL_RGB, SCR_WIDTH, SCR_HEIGHT, 0, GL_RGB,GL_UNSIGNED_BYTE, NULL); //set the texture parameter glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_LINEAR); glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_LINEAR); //attach the texture to the framebuffer glFramebufferTexture2D(GL_FRAMEBUFFER, GL_COLOR_ATTACHMENT0, GL_TEXTURE_2D, textureColorbuffer, 0); //create a renderbuffer object glGenRenderbuffers(1, &amp;rbo); //bind the renderbufferz` glBindRenderbuffer(GL_RENDERBUFFER, rbo); //set the renderbuffer glRenderbufferStorage(GL_RENDERBUFFER, GL_DEPTH24_STENCIL8, SCR_WIDTH, SCR_HEIGHT); //attach the renderbuffer to the framebuffer glFramebufferRenderbuffer(GL_FRAMEBUFFER, GL_DEPTH_STENCIL_ATTACHMENT, GL_RENDERBUFFER, rbo); //check the framebuffer is complete if (glCheckFramebufferStatus(GL_FRAMEBUFFER) != GL_FRAMEBUFFER_COMPLETE) { printf(&quot;framebuffer is not complete!\\n&quot;); } //unbind the framebuffer glBindFramebuffer(GL_FRAMEBUFFER, 0);} 渲染到帧缓冲 您的理解基本正确，但有一点需要澄清：在OpenGL中，您不是“开启”或“关闭”帧缓冲，而是选择绑定到某个特定的帧缓冲对象（FBO），或者绑定回默认的帧缓冲（通常是屏幕）。具体来说： 对要绘制到帧缓冲的物体： 在绘制到帧缓冲的物体之前，您需要先绑定到相应的FBO。这可以通过 glBindFramebuffer(GL_FRAMEBUFFER, fbo) 实现，其中 fbo 是目标帧缓冲对象的ID。 在这个状态下，所有渲染命令（如 glDrawArrays 或 glDrawElements）都会将图像绘制到这个FBO中，而不是默认的帧缓冲。 对不需要绘制到帧缓冲的物体： 当您需要渲染那些不应该被绘制到FBO中的物体时，您应该将帧缓冲绑定回默认帧缓冲。这可以通过调用 glBindFramebuffer(GL_FRAMEBUFFER, 0) 实现。 在绑定到默认帧缓冲后，所有渲染操作将直接绘制到屏幕上。 这种方法使得您可以灵活地控制某些渲染内容是否应该被绘制到FBO中。这在实现如离屏渲染、后处理效果等高级图形功能时非常有用。记住，每次改变绑定的帧缓冲对象时，都需要相应地调整渲染状态，如视口大小、清除缓冲区等。 主循环中主要代码如下： 1234567// 绑定自定义的帧缓冲对象，所有接下来的渲染命令将会作用于这个FBO。//清除颜色和深度缓冲，准备离屏渲染。glBindFramebuffer(GL_FRAMEBUFFER, framebuffer);glClearColor(0.1f, 0.1f, 0.1f, 1.0f);glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT); glEnable(GL_DEPTH_TEST); 12345678//设置相机和着色器，绘制场景：shader.use();shader.setMat4(&quot;view&quot;, camera.GetViewMatrix());shader.setMat4(&quot;projection&quot;, ...);// 绘制立方体和地面DrawCubes();DrawFloor(); 渲染到屏幕 解绑FBO，准备渲染到屏幕： 12345//解绑FBO，切换回默认帧缓冲。glBindFramebuffer(GL_FRAMEBUFFER, 0);glDisable(GL_DEPTH_TEST);glClearColor(1.0f, 1.0f, 1.0f, 1.0f);glClear(GL_COLOR_BUFFER_BIT); 使用FBO内容进行渲染： 1234567//使用FBO的颜色附件作为纹理绘制全屏四边形，将FBO内容显示到屏幕上。screenShader.use();screenShader.setInt(&quot;screenTexture&quot;, 0);glBindTexture(GL_TEXTURE_2D, textureColorbuffer);glBindVertexArray(quadVAO);glDrawArrays(GL_TRIANGLES, 0, 6); gitHub链接：https://github.com/luomuxie/LearnOpenGL/blob/master/Glitter/Sources/framebuffers.cpp","link":"/2023/11/19/2023-11-19-LearnOpenGL10-FrameBuffers/"},{"title":"LearnOpenGL11-ConvolutionKernels","text":"后处理阶段扮演着至关重要的角色。它不仅关乎图像的最终外观，还对图像的质量和可用性产生深远影响。无论是在摄影、电影制作、游戏开发还是在医学成像中，后处理都是不可或缺的步骤。图像后处理涉及一系列技术，用于增强或改变已捕获图像的特性。这些处理可能包括颜色校正、对比度调整、噪声减少、锐化、模糊效果的添加等。这些技术可以修正拍摄时的不足，如不理想的光照条件或设备限制，也可以用于创造艺术效果，增强视觉冲击力。 卷积核 ​ 卷积核是图像后处理中的一项关键技术。简单来说，卷积核是一种特定的数学工具，用于对图像进行局部的像素级操作。它通过一定的数学规则（如加权平均、差分等）来处理图像中的像素，从而实现各种效果。 卷积核的应用 ​ 在后处理中，通过选择不同的卷积核，我们可以实现多种效果。例如，使用平滑的卷积核可以减少图像噪声或者软化细节；而锐化的卷积核可以增强图像的边缘，提高清晰度。更复杂的卷积核可以用于边缘检测、纹理分析甚至是模式识别。这些操作在提高图像质量、准备数据分析或者增强用户体验方面发挥着重要作用。 ​ 通过卷积核，我们能够以精确和高效的方式对图像进行复杂的处理，这在许多图像处理任务中是不可或缺的。无论是在专业的图像编辑软件中，还是在自动化的图像分析系统里，卷积核都是实现高质量图像后处理的基石。 定义及使用 卷积核是图像处理中一个非常重要的概念，它是一个通常较小的矩阵，用于对图像执行各种空间变换。在数学上，卷积是一种特殊的操作，它将两个函数（或信号）组合成一个单一的函数。在图像处理中，这意味着结合卷积核与图像的某个部分，以产生一个新的图像。 卷积核的定义 矩阵形式：卷积核通常是一个小的2D矩阵，它包含了一组数字，这些数字定义了在图像的每个像素上执行的操作。 权重集合：这个矩阵中的每个数字都是一个权重，它决定了原始图像中相应像素的影响程度。 卷积核在图像处理中的应用 滑动窗口：卷积核在图像上滑动，覆盖图像的每一个像素。对于图像上的每个像素点，卷积核都会与其覆盖的像素区域对应位置的值进行运算。 像素级操作：在每个位置，卷积核的值与其覆盖的原始图像像素值相乘，然后将这些乘积相加，得到的和成为新图像在该位置的像素值。 特定效果：通过改变卷积核的值，可以实现不同的图像处理效果。例如，一个模糊卷积核可能包含均匀分布的正值，用于平均其覆盖区域的像素值；而锐化卷积核可能包含中心位置的高正值和周围的负值，用于增加中心像素与周围像素的对比度。 边缘处理：在处理图像边缘时，卷积核可能部分位于图像之外。对于这种情况，可以通过多种方法进行处理，如扩展边缘像素，或者忽略边缘区域。 在数学上，卷积是一种线性操作，这意味着它遵循叠加原理：图像的卷积结果是各部分卷积结果之和。这一特性使得卷积成为一种非常强大的工具，适用于各种图像处理任务。 模糊效果 ​ 模糊是一种常用的技术，主要通过应用不同类型的模糊卷积核来实现。最常见的模糊卷积核包括平均模糊（Box Blur）和高斯模糊（Gaussian Blur）。以下是对这两种卷积核类型的详细介绍： 平均模糊（Box Blur） 原理：平均模糊卷积核通过计算像素及其周围像素的平均值来平滑图像。这种卷积核通常具有相同的值，表示每个像素在计算中的权重是相等的。 12345float kernel[9] = float[]( 1.0/9.0, 1.0/9.0, 1.0/9.0, 1.0/9.0, 1.0/9.0, 1.0/9.0, 1.0/9.0, 1.0/9.0, 1.0/9.0); 应用：平均模糊对于简单的图像去噪和背景模糊非常有效，但可能会导致图像细节的损失。 高斯模糊（Gaussian Blur） 原理：高斯模糊基于高斯分布（正态分布）。这种卷积核中，中心像素具有最大的权重，而权重会随着距离中心像素的增加而减少，形成一个权重递减的钟形曲线。 12345float kernel[9] = float[]( 1.0 / 16, 2.0 / 16, 1.0 / 16, 2.0 / 16, 4.0 / 16, 2.0 / 16, 1.0 / 16, 2.0 / 16, 1.0 / 16); 应用：高斯模糊产生的效果更为自然和柔和，广泛用于图像美化、去除噪声和创建景深效果。由于其保留了更多的图像细节，高斯模糊在各种应用中更为常见。 github 示例代码：https://github.com/luomuxie/LearnOpenGL/blob/master/Glitter/Shaders/frame_buffers_screen_post_processing.fs","link":"/2023/11/20/2023-11-20-LearnOpenGL11-ConvolutionKernels/"},{"title":"LearnOpenGL9-Blending","text":"混合模式作用 OpenGL的混合（Blending）功能主要用于实现图像的透明度和半透明效果。这在计算机图形学中是一个非常重要的特性，它允许我们创建更加逼真和动态的视觉效果。混合的主要应用包括： 透明和半透明效果：通过混合，可以使某些对象部分透明，从而能看到其后面的对象。这对于创建玻璃、烟雾、水面等效果非常有用。 平滑边缘：在2D和3D图形中，混合可以用来平滑物体的边缘，这在抗锯齿技术中尤其重要。 叠加效果：混合可用于将多个图像层次叠加在一起，从而创造出复杂的视觉效果。例如，在游戏和模拟中模拟光效和阴影。 UI元素：在用户界面设计中，混合常被用来创建透明的控件和其他视觉效果。 特殊效果：比如在游戏和影视制作中，用于创建爆炸、火焰、光晕等动态效果。 OpenGL中实现混合的关键是正确地设置混合函数（Blending Function），这会指定源颜色和目标颜色如何组合以产生最终颜色。通过调整这些设置，可以实现各种透明和混合效果 混合计算 Cˉresult=Cˉsource∗Fsource+Cˉdestination∗Fdestination\\bar{C}_{\\text{result}} = \\bar{C}_{\\text{source}} \\ast F_{\\text{source}} + \\bar{C}_{\\text{destination}} \\ast F_{\\text{destination}} Cˉresult​=Cˉsource​∗Fsource​+Cˉdestination​∗Fdestination​ Cˉsource\\bar{C}_{\\text{source}}Cˉsource​：源颜色向量。这是源自纹理的颜色向量。 FsourceF_{\\text{source}}Fsource​：源因子值。指定了alpha值对源颜色的影响 Cˉdestination\\bar{C}_{\\text{destination}}Cˉdestination​：目标颜色向量。这是当前储存在颜色缓冲中的颜色向量。 FdestinationF_{\\text{destination}}Fdestination​：目标因子值。指定了alpha值对目标颜色的影响。 源颜色和目标颜色 OpenGL的混合（Blending）过程中，“源颜色”（Source Color）和&quot;目标颜色&quot;（Destination Color）是两个关键概念： 源颜色（Source Color）：这是当前正在绘制的像素的颜色。当在OpenGL中绘制一个物体时，这个物体的每个像素的颜色就是源颜色。例如，如果正在绘制一个红色的半透明矩形，那么这个矩形的每个像素的颜色（红色，带有一定的透明度）就是源颜色。 目标颜色（Destination Color）：这是已经在帧缓冲区中的像素颜色，即要将新像素（源颜色）绘制到的背景色或者之前已经绘制的图像的颜色。换句话说，当在绘制一个像素时，帧缓冲区中该像素位置上已经存在的颜色就是目标颜色。 在混合过程中，源颜色和目标颜色会根据指定的混合函数进行组合，以计算出最终像素的颜色。混合函数决定了源颜色和目标颜色如何结合，这通常涉及到透明度或者alpha值。例如，如果一个像素是半透明的，那么最终的颜色将是源颜色和目标颜色的某种组合，这种组合的方式取决于设置的混合函数。 例子： 希望将这个半透明的绿色方形绘制在红色方形之上。红色的方形将会是目标颜色（所以它应该先在颜色缓冲中），我们将要在这个红色方形之上绘制这个绿色方形得到右边的图像 分析一下： 绿色方形对最终颜色贡献了60%，那么红色方块应该对最终颜色贡献了40%，即1.0 - 0.6 \\begin{equation} \\bar{C}_{\\text{result}} = \\begin{bmatrix} 0.0 \\\\ 1.0 \\\\ 0.0 \\\\ 0.6 \\end{bmatrix} \\ast 0.6 + \\begin{bmatrix} 1.0 \\\\ 0.0 \\\\ 0.0 \\\\ 1.0 \\end{bmatrix} \\ast (1 - 0.6) \\end{equation} 函数 glEnable(GL_BLEND)：开启混合 glBlendFunc(GLenum sfactor, GLenum dfactor)：设置混合方式 为了获得之前两个方形的混合结果，我们需要使用源颜色向量的alphaℎ作为源因子，使用1−alphaℎ作为目标因子。这将会产生以下的：glBlendFunc： glBlendFunc(GL_SRC_ALPHA, GL_ONE_MINUS_SRC_ALPHA); 透明度显示问题 开启混合模式的时候，明明有透明的贴图显示也没有透明 一个透明图片，当没开启混合时渲染结果如上图： 因为OpenGL的默认行为不考虑贴图的透明度（Alpha通道）来处理像素的合成。这导致了以下几个关键问题： Alpha通道被忽略：在不启用混合模式的情况下，OpenGL绘制像素时通常只考虑RGB（红绿蓝）颜色值，而忽略Alpha值（透明度）。因此，即使你的贴图中包含透明度信息，这个信息也不会被用来影响最终的像素颜色。 覆盖而非混合：不启用混合意味着新绘制的像素会直接覆盖帧缓冲区中的现有像素，而不是与之混合。这就导致透明或半透明效果无法正确表现，因为源颜色（贴图颜色）直接覆盖了目标颜色（背景色或已绘制的像素）。 Z缓冲与透明处理：如果启用了Z缓冲（深度测试），那么渲染顺序也会影响透明贴图的显示。不正确的渲染顺序可能导致被认为在“后面”的透明物体无法正确渲染。 开了混合模式后绘制问题： 发生这一现象的原因是，深度测试和混合一起使用的话会产生一些麻烦。当写入深度缓冲时，深度缓冲不会检查片段是否是透明的，所以透明的部分会和其它值一样写入到深度缓冲中。结果就是窗户的整个四边形不论透明度都会进行深度测试。即使透明的部分应该显示背后的窗户，深度测试仍然丢弃了它们。所以必须先手动将窗户按照最远到最近来排序，再按照顺序渲染。 排序后渲染结果如下：","link":"/2023/11/14/2023-11-14-LearnOpenGL9-Blending/"},{"title":"LearnOpenGL12-CubeMaps","text":"Skybox的基本原理 定义和用途： Skybox使用立方体映射（Cubemap）的基本原理相对简单而强大。立方体映射是一种将6个2D纹理组合成一个立方体的3D纹理的技术，它用于呈现3D场景的远景 实现技术： 六个面的纹理：为了创建一个Skybox，首先需要准备6个2D纹理，它们代表了虚拟世界的远景，通常包括前、后、左、右、上和下六个方向。这些纹理构成了一个完整的立方体，每个纹理对应立方体的一个面。 立方体贴图：这6个纹理被组合成一个立方体贴图（Cubemap），它是一个特殊的3D纹理。立方体贴图具有6个面，每个面都对应着一个2D纹理。这个立方体贴图实际上是一个包含6个纹理图层的3D纹理，其中每个图层代表了一个纹理面。 准备六张纹理图片，分别代表立方体贴图的六个面（右、左、上、下、前、后）。 创建立方体贴图对象，并分别绑定纹理到每个面上。 设置立方体贴图的参数，包括过滤方式和坐标环绕方式。 加载纹理图片并将其绑定到立方体贴图的对应面上。 加载cubemaps代码：https://github.com/luomuxie/LearnOpenGL/blob/master/Glitter/Sources/func.cpp 加载立方体贴图与加载普通二维贴图的主要区别 纹理目标： 对于普通二维贴图，使用的纹理目标是 GL_TEXTURE_2D。 对于立方体贴图，使用的纹理目标是 GL_TEXTURE_CUBE_MAP。 纹理坐标的差异： 在普通二维贴图中，通常使用 2D 纹理坐标 (s, t)，其中 (0, 0) 表示纹理的左下角，(1, 1) 表示右上角。 在立方体贴图中，使用 3D 纹理坐标 (x, y, z)，其中 (0, 0, 0) 表示立方体贴图的中心，而 (1, 1, 1) 表示立方体贴图的边界。通常，这个坐标系用于访问立方体贴图的不同面，例如 (1, 0, 0) 代表右侧面。 纹理坐标的使用方式： 在着色器中，对于普通二维贴图，通常使用 2D 纹理坐标进行采样，例如 texture(sampler, vec2(s, t))。 在立方体贴图中，通常使用 3D 纹理坐标进行采样，例如 texture(sampler, vec3(x, y, z))。采样时，要注意选择正确的面，通常需要将纹理坐标标准化，并指定采样方向。 环境映射技术 概念介绍 环境映射主要是通过将周围环境的图像映射到物体表面，来模拟环境对该物体表面的视觉影响。这种映射通常是通过预先渲染的环境纹理来实现的，该纹理可以是球形的（如球形映射），也可以是立方体的（如Cubemap）。环境映射并不是真正的动态反射，而是一种近似方法，用于在不需要精确计算每个反射光线的情况下模拟反射效果。 反射（Reflection） 定义：反射映射模拟光滑表面（如镜面、金属）如何反射周围环境。 实现方式：这通常通过计算物体表面上的点相对于观察者的视线方向的反射向量，然后根据这个反射向量在环境映射中采样颜色来实现。 应用：反射映射广泛应用于创建高光泽度和镜面效果的材质。 折射（Refraction） 定义：折射映射模拟光线穿过透明或半透明物体时的弯曲效果（如水面、玻璃）。 实现方式：折射映射通过计算入射光线基于物体表面法线和折射率的弯曲程度，然后根据这个弯曲的方向在环境映射中采样颜色。 应用：折射映射用于创建透明或半透明材料的视觉效果，如水、玻璃或宝石。","link":"/2023/11/23/2023-11-23-LearnOpenGL12-CubeMaps/"},{"title":"DynamicStarEff","text":"距离函数（Distance Function）或场函数（Field Function）是一种强大的概念，它在数学和计算机图形学中广泛应用。在图形学中，它们通常用于生成复杂的形状和图案。 距离函数，在图形学中，是一个给定任意点 p（通常在二维或三维空间中），计算该点到某个形状表面的最近距离的函数。对于简单的形状，如圆或球体，这个距离很容易计算。对于复杂的形状，如心形，距离函数可能需要复杂的数学公式。 如果 p 在心形内部，距离函数返回一个负值。 如果 p 正好在心形边缘上，距离函数返回零。 如果 p 在心形外部，距离函数返回一个正值 规范化坐标 解释如何将 fragCoord 转换为规范化的设备坐标（NDC），并根据屏幕比例调整 x 坐标。 将像素坐标转换为规范化设备坐标（NDC）是一个常见的操作。NDC是一个标准化的坐标系统，其中每个轴的值范围从-1到1，原点位于屏幕中心。这种转换允许着色器以独立于屏幕分辨率的方式操作，使得绘图和效果可以在不同大小的屏幕上保持一致。 以下是将 fragCoord 转换为 NDC 的步骤： 归一化坐标: 首先，我们将 fragCoord 的 x 和 y 值除以屏幕的宽度和高度（iResolution.xy），这会将坐标范围从 [0, width] 和 [0, height] 转换为 [0, 1]。 1vec2 q = fragCoord / iResolution.xy; 转换到 NDC: 接着，我们将归一化的坐标乘以2然后减去1，将坐标范围从 [0, 1] 转换为 [-1, 1]，这样原点就位于屏幕中心了。 1vec2 p = -1.0 + 2.0 * q; 调整宽高比: 由于不同的屏幕可能有不同的宽高比，直接使用上述转换可能会导致图形被拉伸。为了解决这个问题，我们需要根据屏幕宽高比调整 x 坐标。我们通过屏幕的宽度除以高度（iResolution.x / iResolution.y），来缩放 x 坐标，以保持图形的宽高比正确。 1p.x *= iResolution.x / iResolution.y; 这样，经过转换的坐标 p 将是规范化的，可以用于在屏幕上以比例正确的方式绘制图形。任何在这个坐标系下定义的图形都会自动适应不同尺寸和宽高比的屏幕 动态位置调整 详细介绍循环如何在不同的时间间隔为每个心形创建动态的位置变化。 以下是如何在循环中为每个心形创建动态位置变化的详细介绍： 时间偏移: itime 是基于全局时间 iTime 的变量，它为每个心形添加一个不同的时间偏移量。这样，每个心形都有其独特的动态行为，使得动画看起来更加自然和有趣。 1float itime = iTime + 1.2 * float(i); 水平位置变化: p0.x += sin(itime); 这一行代码使用正弦函数来调整心形的水平位置，正弦函数随着时间的变化而变化，创建了左右摇摆的效果。 1p0.x += sin(itime); 垂直位置变化: 垂直位置的变化更为复杂。首先，使用 fract(itime) 获取时间的小数部分，然后通过减去0.5、平方、乘以2等操作，创建了一个有趣的非线性运动轨迹。 12345b = fract(b);b -= 0.5;b *= b;b *= 2.0;p0.y += b; 旋转变换: p0 *= rot(8.0 * cos(itime)); 这行代码对心形应用了一个时间变化的旋转矩阵，rot 是一个函数，返回一个根据时间变化的旋转矩阵。这样，心形不仅在位置上动态变化，而且在角度上也进行旋转，增加了效果的多样性。 1p0 *= rot(8.0 * cos(itime)); rot 函数通常用于创建一个二维旋转矩阵，它可以用来旋转一个点或对象在二维空间内的位置。这个函数通常接受一个角度作为参数，返回一个可以用来旋转向量的 2x2 矩阵。 12345mat2 rot(float angle) { float s = sin(angle); float c = cos(angle); return mat2(c, -s, s, c);} 这个矩阵是通过将角度的正弦和余弦值排列成一个 2x2 矩阵得到的。在二维空间中，旋转矩阵可以表示为： \\begin{equation} \\begin{bmatrix} \\cos(\\theta) &amp; -\\sin(\\theta) \\\\ \\sin(\\theta) &amp; \\cos(\\theta) \\end{bmatrix} \\end{equation} 心形距离和绘制: 在调整了位置和旋转之后，使用 star5 函数（其实应为心形的距离函数）来确定当前片段是否在心形内部。如果在内部，则计算并应用颜色。 1234float d = star5(p0, 0.5, 0.5);if (d &lt; 0.0) { col = palette(itime - 8.0 * d);} 颜色的动态分配 分析 palette 函数如何根据时间变化分配颜色，以及如何使用距离值 d 来调整颜色。 123vec3 palette(float a) { return 0.5 + 0.5 * sin(vec3(0, 1, 2) + a);} 正弦函数的应用: sin(vec3(0, 1, 2) + a)：这里，sin 函数应用于一个三维向量 vec3(0, 1, 2)，然后再加上输入参数 a。向量中的每个分量（0, 1, 2）分别被加到 a 上，这意味着每个颜色通道（红、绿、蓝）的正弦波都有不同的相位偏移。 颜色通道的相位偏移: 由于红、绿、蓝通道的相位偏移分别为 0、1、2，这会导致每个颜色通道的正弦波形相位错开。这种相位偏移确保了每个颜色通道随时间变化的模式不同，从而创造出丰富多彩的动态颜色效果。 颜色值的范围调整: 正弦函数 sin 的输出范围是 [-1, 1]。为了将这个范围调整到 [0, 1]（颜色值的标准范围），函数对正弦波的结果先加上 0.5，再乘以 0.5。这样，最终的颜色值将在 0 到 1 之间变化 边缘平滑 计算 od（外部距离）来平滑心形的边缘，并赋予它们不同的颜色。这种方法通常用于在光栅化图形中实现抗锯齿效果，使得边缘看起来更加平滑。 距离值的调整: 在代码中，od 是根据心形的距离函数 d 计算得到的，这个距离函数返回每个像素点到心形边缘的最短距离。 od 是这个距离的绝对值减去一个小的常数（如 0.015），这样做的目的是创建一个边缘区域，其中 od 为负值表示接近心形的边缘。 1float od = abs(d) - 0.015; 边缘检测: 当 od 小于 0 时，表示当前像素位于心形边缘的附近。在这个区域内，可以应用特殊的颜色或效果来平滑边缘。 颜色应用: 在边缘区域（od &lt; 0），可以选择一个与心形内部或背景不同的颜色，或者使用某种颜色混合技术，如线性插值（lerp）。 这种颜色处理有助于减少锯齿现象，使边缘看起来更平滑。 123if (od &lt; 0.0) { col = vec3(q.y); // 示例：使用与垂直位置相关的颜色} 这种技术在图形渲染中非常常见，尤其是在低分辨率或需要高质量渲染效果的情况下。 颜色校正与输出 1col = sqrt(col); 对颜色进行平方根处理，即进行伽马校正（Gamma Correction），是一种重要的颜色管理技术。它用于调整图像的亮度表示，使其更接近人眼对亮度的非线性感知。这种校正有助于在不同的显示设备上保持颜色的一致性和准确性。 效果 通过伽马校正，可以使图像的亮度分布更贴近人眼的感知，从而提高视觉质量。 它特别对提升暗区域的细节和色彩渐变的平滑性有效，减少了色带（Banding）现象。","link":"/2023/11/27/2023-11-27-DynamicStarEff/"},{"title":"TODO","text":"2023-12-06 保持专注，提高效率，一个时间只做一件事情 30分钟间轮换 背单词，继续开始打卡，看美剧20分钟（在家），看论坛 看OpenGL的几何函数，实现相关代码 今天完成中国小院的相关配置 晚上回家就洗澡，然后给蛋蛋狗蛋吃肉肉，然后看美剧，然后规划明天早上任务 11点前准备睡觉 2023-12-08 2023-12-012 保持专注，提高效率，一个时间只做一件事情 背单词，看美剧（中午时间看，看谢），看英文论坛 看OpenGL,做一个总结，思考以这个录个视频的可能性 以女性的角色思考问题，然后做一些更深入的思考 做项目进步安排 今晚回家视频要完成粗剪 思考以自己学习的角度来拍视频的可能性 学习爬虫，爬网上数据来喂gpt 了解gpts 人的可塑性是很强的，千万不要用定义的语言去形容一个人 2023-12-013 保持专注，提高 效率，一个时间只做一件事 背单词，看英文，看美剧(在中午吃饭和吃晚餐的时候) 写opengl，看opengl,做blog总结 coding 尽量做到能快速打出来，不要依赖提示，不然以后做视频会有问题 今天的功做，做完场景除了机器外的部分，开始物品分类 晚上11点前睡午觉 教育的合理性 2023-12-016 保持专注，提高效率，一个时间只做一件事情 背单词，年美剧，看论坛 查找gpt 博客生成和整理 看新的opengl内容， 准备几何着色器内容（整理成博客） 晚上把背景音乐配上，做完这个视频 创建一个关于做视频的相关总结","link":"/2023/12/06/2023-12-06-TODO/"},{"title":"LearnOpenGL13-GeometryShader","text":"图元 （Primitive）是计算机图形学中的一个基本概念，指的是构成图形的最基本的几何单位。在OpenGL和其他图形API中，图元是组成三维场景和对象的基石。以下是一些关于图元的关键点： 基本类型 点（Points）：最简单的图元，仅表示一个坐标位置。 线（Lines）：由两个点连接而成，表示一条直线段。 三角形（Triangles）：由三个点构成，是最常用的图元类型，因为它可以构成任何复杂的三维形状。 几何着色器概述 几何着色器及其在渲染管线中的位置。 概念：几何着色器是图形处理管线中的一种可编程着色器，它处理每个图元（如点、线或三角形）上的顶点集合。 功能：它的主要功能是读取一个图元（由一组顶点组成）并生成新的顶点，从而产生新的图元。几何着色器可以改变图元的数量和类型（例如，从一个点生成一条线，或从一条线生成一个三角形） 突出几何着色器的功能：动态生成新顶点和图元。 增强的灵活性：几何着色器能够在运行时生成新的图元，如点、线和三角形，这为图形渲染提供了极大的灵活性。 渲染管线概览：OpenGL的渲染管线可以分为几个阶段：顶点处理、图元装配、光栅化、片元处理和像素操作。各阶段处理不同的渲染任务。顶点着色器后：几何着色器位于顶点着色器之后。顶点着色器处理单个顶点的数据（如位置、颜色等），并传递给几何着色器。图元装配前：几何着色器在图元装配阶段之前。在这个阶段，来自顶点着色器的顶点会被组装成图元，几何着色器可以在这些图元上操作，生成新的顶点和图元。片元着色器前：几何着色器处理完成后，生成的图元会被送入光栅化阶段，然后进入片元着色器。片元着色器处理这些图元上的每个片元（像素），计算最终颜色等信息。 三、编写几何着色器 代码示例。 12345678910111213141516171819202122232425#version 330 corelayout(points) in;layout(triangle_strip, max_vertices = 3) out;void main() { // 定义三角形的大小 float size = 0.05; // 输入点的位置 vec4 position = gl_in[0].gl_Position; // 生成三角形的三个顶点 gl_Position = position + vec4(-size, -size, 0.0, 0.0); EmitVertex(); gl_Position = position + vec4(size, -size, 0.0, 0.0); EmitVertex(); gl_Position = position + vec4(0.0, size, 0.0, 0.0); EmitVertex(); // 结束当前图元的生成 EndPrimitive();} layout(points) in; 表示几何着色器的输入将是点。 layout(triangle_strip, max_vertices = 3) out; 定义了输出图元类型为三角带，并且每个图元最多有3个顶点。 EmitVertex()、EndPrimitive()函数的作用。 EmitVertex() 函数 作用：EmitVertex()函数用于在几何着色器内部发射（即生成）一个新的顶点。 使用时机：在设置好一个顶点的所有必要属性（如gl_Position）之后，调用EmitVertex()来确认这个顶点的创建。 重复使用：在一个几何着色器调用中，可以多次调用EmitVertex()来生成多个顶点，从而构成一个或多个新的图元。 EndPrimitive() 函数 作用：EndPrimitive()函数用于标记一个图元的结束。 使用时机：在生成完一个图元的所有顶点后，调用EndPrimitive()来告知OpenGL一个图元已经完成。 开始新图元：调用EndPrimitive()之后，可以开始生成下一个图元的顶点。在下一个图元的最后一个顶点后再次调用EndPrimitive()。 note：上面例子中发射出的三个点，原始是顶点并不作为图元的最终部分 例子：爆破物体 接口块的定义 123456in VS_OUT //VS_OUT 自定义的接口块名称，用于组织和传递特定的着色器数据{ vec2 TexCoords;} fs_in;//fs_in是在当前着色器下使用的实例名 叉乘来获取法向量 如果我们能够获取两个平行于三角形表面的向量a和b，我们就能够对这两个向量进行叉乘来获取法向量了 123456vec3 GetNormal(){ vec3 a = vec3(gl_in[0].gl_Position) - vec3(gl_in[1].gl_Position); vec3 b = vec3(gl_in[2].gl_Position) - vec3(gl_in[1].gl_Position); return normalize(cross(a, b));} 沿着法线向量进行位移 1234567vec4 explode(vec4 position, vec3 normal){ float magnitude = 2.0; vec3 direction = normal * ((sin(time) + 1.0) / 2.0) * magnitude; return position + vec4(direction, 0.0);} vec3 direction = normal * ((sin(time) + 1.0) / 2.0) * magnitude; 计算顶点移动的实际方向和幅度。 这里使用 sin(time) 创建了一个随时间变化的值，产生动态效果。 ((sin(time) + 1.0) / 2.0) 的结果在0到1之间变化，用于调节方向向量的长度。 将这个结果乘以法线向量和幅度 magnitude，计算出顶点的移动方向和距离。 ps:关于光栅化这个名字的由来： “光栅”：这个词来自于“Raster”，指的是像素构成的网格（即屏幕的像素阵列）。光栅化这个术语就是从这个概念中衍生出来的。 “化”过程：这个词表示的是一个转换过程，即将向量图形（由几何图元组成）转换为光栅图像（由像素组成）的过程。 光栅化阶段（Rasterization）在计算机图形学中被这样命名是因为它涉及到将几何图形原语（如点、线、三角形）转换为光栅图像中的像素点的过程。这个过程的名称来源于“光栅”这个概念，它指的是组成图像的像素网格。定义：光栅图像是由像素阵列组成的图像，每个像素都有自己的颜色值。屏幕显示：现代显示设备（如计算机显示器和手机屏幕）通常都是基于光栅的，意味着它们显示的图像是由行列排列的像素组成的。 光栅化过程 几何到像素：在光栅化阶段，几何图元（在经过顶点着色器和可能的几何着色器处理之后的）被映射到这个像素网格上，转换为一系列像素。 像素决定：这个过程包括决定哪些像素应该代表图元，以及这些像素的初步颜色和其他属性（如深度）。 为什么重要 可视化：光栅化是将抽象的数学表示（几何图元）转换为用户屏幕上可见图像的关键步骤。 性能考虑：这是一个性能敏感的过程，因为屏幕上可能有数百万个像素点需要计算。 总的来说，光栅化阶段之所以被这样命名，是因为它代表了一种将几何图形数据转换成光栅图像（像素网格）的关键过程，这一过程对于在基于光栅的显示设备上创建和显示图形至关重要。","link":"/2023/12/16/2023-12-16-LearnOpenGL13-GeometryShader/"},{"title":"LearnOpenGL14-AntiAliasing","text":"unity 的shader如何开启抗锯齿 OpenGL中的MSAA 一个包含N个样本的多重采样缓冲。这可以在创建窗口之前调用glfwWindowHint来完成。 1glfwWindowHint(GLFW_SAMPLES, 4); Unity中的MSAA 在Unity中，开启多重采样抗锯齿（MSAA，Multi-Sample Anti-Aliasing）的方法与OpenGL中使用glfwWindowHint设置GLFW_SAMPLES的方式有所不同。Unity提供了一些内置的选项来配置和启用MSAA，这些选项可以在编辑器中设置，也可以通过脚本在运行时动态设置。 通过Unity编辑器设置MSAA 图形设置（Graphics Settings）： 打开Unity编辑器。 转到 Edit -&gt; Project Settings -&gt; Quality。 在Quality设置中，您可以为不同的质量等级设置抗锯齿级别。 在“Anti Aliasing”选项中选择希望的MSAA级别（例如2x, 4x, 8x）。 相机设置： 选中场景中的相机对象。 在相机的检视面板（Inspector）中，找到“Allow MSAA”选项并确保它被勾选（这通常是默认设置）。 通过脚本动态设置MSAA 如果您想在运行时动态控制MSAA，可以使用Unity的API来编程实现。 12345678910using UnityEngine;public class MSAASetting : MonoBehaviour{ void Start() { // 设置MSAA级别为4x QualitySettings.antiAliasing = 4; }} 这段脚本可以附加到任何一个在场景中的GameObject上。 在 Start 方法中，通过 QualitySettings.antiAliasing 属性设置MSAA的级别。可以设置为0（关闭MSAA），2，4，或8等。 注意事项 MSAA对渲染性能有一定影响，特别是在较高的采样率下。 在某些特定的渲染路径或条件下，MSAA可能不可用或效果不明显，比如使用了某些后处理效果时。 通过以上方法，您可以在Unity中根据项目的需要设置MSAA，以改善游戏或应用的图像质量。🦌🌟🖥️🎨🎮","link":"/2023/12/18/2023-12-18-LearnOpenGL14-AntiAliasing/"},{"title":"CityOfMachinery","text":"Once upon a time, in a distant land, there was a unique city built entirely of machinery(机械). This was not your typical(典型的) city, as it was a living, breathing entity, a testament to the power of biochemistry(生物化学) and the creative composition(构造) of its original founders. In the heart of the city, standing like a principal(主要的) monument, was a bridge(桥) of grand design. Each day, a procession(队伍) of people would cross it, heading towards the city’s core where a giant, influential(有影响力的) gadget(小装置) pulsated, providing the energy that powered the city. Our story centers around one particular resident of this city, a man named John. John was a welder(焊工), a profession of significant importance here. Despite the mechanized city, John had a soft spot for antiques(古董), which he considered a crucial(关键的) connection to the past. John had a remarkable foresight(预见), the ability to see things before they happened. This trait made him an essential part of the city. One day, while working, he suddenly felt restless(不安的). He looked at the gadget and noticed its motion(运动) was deteriorating(恶化). He couldn’t just stand by and tolerate(忍受) this, it could lead to disaster. He swiftly made his way to the bridge. To his surprise, he noticed the residents moving in an orderly fashion, each with a punctual(准时的) pace, oblivious to the impending danger. He saw the city’s mayor, a woman known for her decisive(果断的) nature and fluent(流利的) speeches. She wore a bitter(痛苦的) grin(咧嘴笑), obviously something was amiss. John rushed up to her, “We need to shut the gadget down. The city is about to explode(爆炸)!” The mayor was taken aback, “John, are you sure about this?” He nodded, “I have no concrete proof, but I’ve never been wrong before.” The mayor was not known to be an individual who would easily swallow(轻易接受) such news. She consulted the city’s database(数据库), which was designed to cater(满足) to all their needs. Yet, it showed no signs of danger. John confessed(承认), “The database wouldn’t pick it up. It’s something I can feel.” Doubt clouded the mayor’s eyes, but she knew John’s reputation. With a decisive tone(语气), she ordered the shutdown of the gadget, “We’ll assume(假设) you’re right, John.” He could see she was ashamed(羞愧的) for not noticing the signs herself. But there was no time to dwell on that. She initiated the draft(草案) of the sanction(制裁), a civil(文明的) process of the city, which included a series of coded commands. They hurriedly input the codes, the gadget slowly humming down to a halt. Meanwhile, outside the city, a storm was brewing. A hurricane(飓风) was approaching, threatening to consume(消耗) them all. But the city’s people were not worried. The city had survived such calamities before, and they retained(保留) the belief in their protective barriers. But there was one thing they hadn’t counted on. With the gadget down, the city’s barriers were weakening. As the storm hit, a powerful wind sucked(吸入) up a lonely pint(品脱) of water from the nearby river, growing into a colossal wave. The barriers crumbled. John’s eyes widened as the wave towered over them. He felt a chill run down his spine. Then he noticed something. His bride(新娘) to be, a woman of immense beauty and courage, was standing at the latter(后面的) end of the bridge, looking at him. Time seemed to slow. As the wave was about to crash onto the city, he saw her extend her hand towards him. He ran, pushing past the crowd, violating(违反) the city’s rules in his desperation. As he reached her, he grasped(抓住) her hand. They both knew they couldn’t outrun the wave. But at least they were together. The wave hit, the city was flooded, and the world around them seemed to crumble. But they held on to each other, standing firm against the disaster. And then, it was over. The city had been severely damaged, but it still stood. The people, though shaken, survived. In the aftermath, the editorial(社论) office of the city’s main newspaper decided to celebrate their survival. They issued a special issue detailing John’s heroic actions. They praised his foresight and his decision to shut down the gadget, actions which saved many lives. The city began the process of rebuilding. They started with the machinery, fixing and irrigating(灌溉) the vital systems. The ordeal had brought a change in them. They had learned the importance of being in touch with their instincts, something they had forgotten in their relentless pursuit of technology. Hence(因此), the story of John and his city serves as a reminder to us all, a tale of courage and resilience against overwhelming odds. The city of machinery may have faltered, but it did not fail, and it would rise again, stronger than ever. It was a testament to the indomitable spirit of its people and their ability to adapt and overcome any challenge.","link":"/2023/07/29/CityOfMachinery/"},{"title":"DeterminationInTheGarage","text":"In a mature(成熟的) city, nestled amongst the pine(松树) trees, sat a small garage(车库). It was not your ordinary garage. For it was a place of unusual determination(决心) and an incubator for bizarre(奇怪的) yet coherent(一致的) ideas, founded by a group of minority(少数族群) inventors who derived(得出) their inspiration from their ancestors(祖先) and the tales of their deeds(行为). The leader of this group was a man named Samuel. Samuel was a man of immense imagination(想象力). He had a knack for finding loopholes(漏洞) in existing technology, which was reflected in the custom(定制的) inventions that flowed(流动) from their garage, each marked with Samuel’s unique description(描述). One day, they were visited by a solemn(庄重的) man named Gerald. Gerald was a representative of a big firm(公司) that wanted to invest in their technology. But there was a catch. Gerald wanted them to change their approach and make it more consistent(一致的) with the industry’s standard terminology(术语) and infrastructure(基础设施). The group was in a bind. They needed the financial reinforcement(增援) to sustain(维持) their work. Still, they were not willing to lose their unique touch. They called for a thorough(彻底的) discussion. They sat around a big table, the diagram(图表) of their latest invention laid out. The atmosphere was as tense as a rook(秃鹫) perched on a fence(围栏) overlooking its prey. Their conversation was interrupted by a sudden noise. A damn(该死的) faulty(有故障的) helicopter(直升机) was causing a raucous outside. They went out to check, each of them grabbing a towel(毛巾) to protect themselves from the dust being kicked up. Amid the chaos, they saw a beggar(乞丐) seeking shelter from the dust storm. They invited him in. In the comfort of the garage, the beggar noticed their design. His eyes lit up, and he asked if he could add something. He took a piece of chalk and began to draw on their blueprint. It was a bizarre and unexpected moment, a true coincidence(巧合). Yet they let him proceed. The beggar turned out to be a retired biology(生物学) professor. His name was Howard, and he had dedicated his life to the study of a rare desert(沙漠) species, the dwarf(矮小的) desert lizard. He pointed out a mechanism the lizards used to regulate their body temperature. He suggested integrating that concept into their design. The group was taken aback. Samuel decided to put the proposal into a test. They spent the next weeks implementing the changes, the hum of their machines echoing like a chorus(合唱) in the garage. They were ready to present their revised design to Gerald. The changes had indeed brought about a remarkable improvement. Gerald was impressed, and he agreed to fund their project, proclaiming(宣布) their invention as a groundbreaking feat. But their journey was far from smooth. They faced a significant setback(挫折) when their prototype failed during a test run. Their hopes seemed to wane like a herd(群) of deer retreating into the woods. Yet, they did not give up. They chose to persevere(坚持下去). Samuel motivated his team, “We need to turn this setback into a step forward.” They worked relentlessly, through nights and days. Their hands were dyed(染色) with grease and dirt, but their spirit never wavered. The sounds of the tools chopping(砍) and clanking, the hum of the machines, and the determination in their eyes, created a picturesque(如画的) scene. They finally managed to hatch(孵化) a perfect prototype. The news spread like a wildfire. The local dairy(奶制品) farmer, who was once skeptical of their work, felt obliged(有义务的) to congratulate them. The group felt a sense of accomplishment. They were no longer the cursed(被诅咒的) inventors working in a small garage; they had become the town’s pride. As a bonus(奖金), the editorial(社论) team of the city’s newspaper decided to feature their story. They celebrated their determination and the sheer will to overcome their challenges. They were no longer just a bunch of inventors working in a garage; they were now an inspirational story for their city and a beacon of hope for young inventors everywhere. A year later, on a calm summer evening, they all gathered around a bonfire. Samuel raised a toast to their success and the paw(爪子) print of the desert lizard etched onto the machine, a token of their fateful encounter with the beggar who turned out to be their savior. As the flames crackled, they could almost see the ghost(幽灵) of their past selves, toiling away in the garage. They had achieved their dream, not by following the aggressive(侵略性的) and ruthless methods of the industry but by executing(执行) their ideas with a firm belief in their unique approach. They had staked(赌注) their all and emerged victorious. They stood as a testament to the power of perseverance and imagination, demonstrating that even the most significant obstacles could be overcome with determination and a bit of luck. Their story served as a reminder to all inventors, to not get dizzy(眩晕) with success, but to continue pushing boundaries, to never stop exploring, to keep the spark of creativity alive. And thus, they became the guardians of their city, the protectors of its creative spirit, and the heroes of their own incredible tale.","link":"/2023/07/29/DeterminationInTheGarage/"},{"title":"InvertedHullOutlines","text":"原理 “Inverted Hull”（反向包围壳）方法的实现原理如下： 在模型的边缘产生轮廓线效果。首先，这个方法创建了一个与原始模型相同的包围壳（hull），但是将其沿着模型的法线方向向外扩展。具体来说，在顶点着色器中，每个顶点的位置被加上沿法线方向扩展的距离，这个距离由轮廓线的厚度参数控制。 在渲染过程中，背面剔除（Cull Front）被应用于扩展后的包围壳。这意味着，只有扩展包围壳的背面（朝向摄像机的一面）会被渲染。原始模型的前面将不受影响。 在渲染扩展包围壳时，片段着色器会使用指定的轮廓线颜色填充背面。这样，在原始模型边缘的地方，轮廓线将被绘制出来。 由于原始模型和扩展包围壳的深度值相近，轮廓线会被绘制在原始模型的边缘，从而实现轮廓线效果。 这种方法通常在许多情况下可以实现轮廓线效果，但在复杂的几何形状、模型法线不一致或遮挡关系较复杂的情况下可能出现不理想的效果。当模型的内部部分被扩展包围壳的背面所遮挡时，可能会在非边缘部分产生轮廓线。 缺点 在这个轮廓线Shader中，轮廓线的效果是通过将模型的法线向外推移，然后绘制该模型的背面（Cull Front），从而在原始模型的边缘显示出轮廓线。在大多数情况下，这种方法可以产生较好的效果。然而，在某些情况下，这种方法可能会导致在非边缘部分产生轮廓线。原因有以下几点： 几何形状的复杂性：对于复杂的几何形状，尤其是那些具有很多相互靠近的部分的形状，可能会出现非边缘部分的轮廓线。因为在将顶点沿法线方向推移时，相邻的顶点可能会相互穿越，导致轮廓线在错误的位置出现。 法线方向的不一致：如果模型的法线方向不一致，可能会导致非边缘部分出现轮廓线。例如，如果模型的一部分法线朝向模型内部，而另一部分法线朝向模型外部，当将顶点沿法线方向推移时，可能会导致非边缘部分出现轮廓线。 为了解决这个问题，可以尝试以下方法： 优化模型：优化模型的几何形状，确保相互靠近的部分有足够的间隔，以避免轮廓线在非边缘部分出现。 检查并修复法线：检查模型的法线方向，确保它们是一致的。如果发现问题，可以使用3D建模软件修复法线。 使用不同的轮廓线绘制方法：如果问题仍然存在，可以尝试使用不同的轮廓线绘制方法。例如，您可以使用基于屏幕空间的轮廓线着色器，它是通过计算屏幕空间中的深度差异来绘制轮廓线，这种方法通常能够更准确地在边缘部分绘制轮廓线。然而，这种方法的性能开销可能会更高，因为它需要在屏幕空间进行计算。 代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107Shader &quot;Tutorial/020_InvertedHull/Surface&quot; { Properties { _Color (&quot;Tint&quot;, Color) = (0, 0, 0, 1) _MainTex (&quot;Texture&quot;, 2D) = &quot;white&quot; {} _Smoothness (&quot;Smoothness&quot;, Range(0, 1)) = 0 _Metallic (&quot;Metalness&quot;, Range(0, 1)) = 0 [HDR] _Emission (&quot;Emission&quot;, color) = (0,0,0) _OutlineColor (&quot;Outline Color&quot;, Color) = (0, 0, 0, 1) _OutlineThickness (&quot;Outline Thickness&quot;, Range(0,1)) = 0.1 _OutlineMinThickness (&quot;Outline Min Thickness&quot;, Range(0,1)) = 0.001 _OutlineMaxThickness (&quot;Outline Max Thickness&quot;, Range(0,1)) = 0.1 } SubShader { //the material is completely non-transparent and is rendered at the same time as the other opaque geometry Tags{ &quot;RenderType&quot;=&quot;Opaque&quot; &quot;Queue&quot;=&quot;Geometry&quot;} CGPROGRAM #pragma surface surf Standard fullforwardshadows #pragma target 3.0 sampler2D _MainTex; fixed4 _Color; half _Smoothness; half _Metallic; half3 _Emission; //input struct which is automatically filled by unity struct Input { float2 uv_MainTex; }; //the surface shader function which sets parameters the lighting function then uses void surf (Input i, inout SurfaceOutputStandard o) { //read albedo color from texture and apply tint fixed4 col = tex2D(_MainTex, i.uv_MainTex); col *= _Color; o.Albedo = col.rgb; //just apply the values for metalness, smoothness and emission o.Metallic = _Metallic; o.Smoothness = _Smoothness; o.Emission = _Emission; } ENDCG //The second pass where we render the outlines Pass{ Cull Front CGPROGRAM //include useful shader functions #include &quot;UnityCG.cginc&quot; //define vertex and fragment shader #pragma vertex vert #pragma fragment frag //tint of the texture fixed4 _OutlineColor; float _OutlineThickness; float _OutlineMinThickness; float _OutlineMaxThickness; //the object data that's put into the vertex shader struct appdata{ float4 vertex : POSITION; float4 normal : NORMAL; }; //the data that's used to generate fragments and can be read by the fragment shader struct v2f{ float4 position : SV_POSITION; }; //the vertex shader v2f vert(appdata v){ v2f o; float3 objectScale = float3( length(unity_ObjectToWorld._11_21_31), length(unity_ObjectToWorld._12_22_32), length(unity_ObjectToWorld._13_23_33) ); float thickness = lerp(_OutlineMinThickness, _OutlineMaxThickness, _OutlineThickness); float objectSize = length(objectScale) / 3.0; float relativeThickness = thickness * objectSize; // o.position = UnityObjectToClipPos(v.vertex + normalize(v.normal) * _OutlineThickness); o.position = UnityObjectToClipPos(v.vertex + normalize(v.normal) * relativeThickness); return o; } //the fragment shader fixed4 frag(v2f i) : SV_TARGET{ return _OutlineColor; } ENDCG } } FallBack &quot;Standard&quot;} 为什么一定要通过法线外移： 123o.position = UnityObjectToClipPos(v.vertex + normalize(v.normal) * _OutlineThickness);//为什么要通过法线外移，不能直接改成： o.position = UnityObjectToClipPos(v.vertex + _OutlineThickness); 使用法线方向来外移顶点是为了确保轮廓线沿着模型的表面扩展。法线表示了模型表面的方向，因此沿着法线移动顶点可以使轮廓线紧贴模型表面。如果直接使用o.position = UnityObjectToClipPos(v.vertex + _OutlineThickness);，这将不会考虑模型的表面方向，而是在模型的每个顶点上添加一个固定的偏移量。 这种方法可能导致轮廓线不再紧贴模型表面，而是在空间中随意偏移。在某些情况下，这可能导致轮廓线出现不正确或不自然的效果。因此，使用法线方向来外移顶点是创建轮廓线效果的更合适方法。","link":"/2023/05/07/InvertedHullOutlines/"},{"title":"LeanrOpenGL2-Texture","text":"基本概念: 纹理是一种可以应用到模型表面的图像数据。通过UV坐标映射，纹理的像素被映射到3D模型的表面 UV映射 将纹理映射到对象上的过程称为纹理映射或UV映射。在这个过程中，模型的每个顶点都会分配一个UV坐标，这些坐标对应于纹理图像上的位置。UV是二维纹理坐标的名称，其中U是水平方向，V是垂直方向。UV坐标的U方向从左到右，V方向从下到上。因此，原点（0,0）位于纹理的左下角，而（1,1）则位于右上角。 环绕模式 纹理的环绕模式决定了当UV坐标超出0到1的范围时会发生什么。常见的环绕模式包括： Clamp：UV坐标被限制在0到1的范围内。 Repeat：纹理在UV坐标超出范围时重复。 Mirror：纹理在UV坐标超出范围时镜像。 纹理过滤 GL_LINEAR（也叫线性过滤） Linear Filtering是一种纹理过滤方法，用于确定如何在纹理映射到3D对象时对纹理像素进行插值。当纹理被映射到比其本身分辨率更大或更小的表面时，将需要确定如何对这些像素进行插值以适应新的空间。Linear Filtering正是用于此目的的一种技术。 如何工作 Linear Filtering通过计算纹理坐标附近的四个像素的加权平均值来工作。权重基于纹理坐标与每个像素中心之间的距离。 放大（Magnification）: 当纹理被映射到比其自身更大的表面时，Linear Filtering会考虑最接近所需纹理坐标的四个像素，并计算它们的加权平均值。结果是一个平滑的过渡，没有突然的颜色变化。 缩小（Minification）: 当纹理被映射到比其自身更小的表面时，情况更为复杂。简单地使用最近的四个像素可能会导致明显的混叠和视觉伪影。因此，缩小时通常使用Mipmap技术，其中存储了不同分辨率的纹理版本。然后使用Linear Filtering在相应Mipmap级别的像素之间进行插值。 优点与缺点 优点: 产生更平滑和自然的视觉效果。 减少了锯齿和像素化的外观。 缺点: 计算上稍微复杂一些，可能比使用点过滤（Point Filtering）更消耗资源。 在某些情况下，可能会导致纹理显得过于模糊。 总之，Linear Filtering是一种常用的纹理过滤技术，特别适用于需要平滑渲染和避免像素化效果的场景。不过，它可能不适合所有应用，特别是当资源有限或目标是保留更粗糙、像素化的外观时。 GL_NEAREST（也叫邻近过滤） GL_NEAREST是OpenGL中的一种纹理过滤方法，也称为&quot;最近邻过滤&quot;或&quot;点过滤&quot;。当纹理被映射到比其自身分辨率更大或更小的表面时，GL_NEAREST会选择最近的纹理坐标对应的纹理像素。 在放大(Magnification)的情况下，GL_NEAREST会返回最接近所需纹理坐标的纹理像素。这种简单的插值方法可能会导致像素化的外观，特别是当纹理被显著放大时。 在缩小(Minification)的情况下，GL_NEAREST会选择最接近所需纹理坐标的纹理像素。这可能会导致混叠（Aliasing）的问题，因为许多纹理像素可能会被忽略，特别是当纹理被显著缩小时。 优点 这种过滤方法的一个优点是，它的性能开销非常小，因为它只需要访问一个纹理像素。 缺点 然而，这种方法的缺点是，它可能会产生较差的图像质量，特别是在纹理被显著缩小或放大的情况下。 因此，GL_NEAREST更适合用于需要快速渲染和不需要高分辨率纹理的场景。 下图是从GL_LINEAR转到GL_NEAREST 纹理单元 在OpenGL中，纹理单元是一种硬件资源，用于管理纹理的读取和采样。当在渲染过程中使用多个纹理时，纹理单元允许在一个shader中同时访问多个纹理。 每个纹理单元可以被视为一个纹理的插槽，允许你将纹理绑定到该插槽并通过shader访问它。一个典型的现代GPU可能会有多达16个或更多的纹理单元。 在Unity的Shader语言（HLSL/CG）中，可以通过sampler2D类型来定义一个采样器，并使用内置函数如tex2D与指定的纹理坐标一起，从纹理中采样颜色。 在OpenGL GLSL中，你可以使用sampler2D、sampler3D等类型，并使用如texture函数来从纹理中采样颜色。","link":"/2023/08/03/LeanrOpenGL2-Texture/"},{"title":"LearnOpenGL3_Camera","text":"为什么从世界坐标，转到摄像机的坐标系就能实现视图变换了 视图变换的目的是将3D世界中的物体从世界坐标系转换到摄像机坐标系。这个转换为何能实现视图变换，我们可以从以下几个方面来理解： 1. 摄像机的位置和方向 在世界坐标系中，摄像机可以位于任何位置，并朝任何方向。但在摄像机坐标系中，摄像机始终位于原点，并朝向-z轴。通过将物体从世界坐标系转换到摄像机坐标系，我们实际上是在将整个场景移动和旋转，使摄像机的位置和方向与新坐标系的原点和方向对齐。 2. 视点的模拟 当物体被转换到摄像机坐标系中时，它们的位置和方向是相对于摄像机的。这意味着我们现在看到的物体是从摄像机的视点观察的。这就是为什么这个变换被称为“视图”变换的原因：它模拟了从摄像机的视点查看场景的效果。 3. 后续的投影 视图变换后，物体位于摄像机坐标系中，我们可以进一步应用投影变换。投影变换将3D摄像机坐标系中的物体投影到2D平面上，模拟了摄像机镜头的效果。这一步是在视图变换的基础上进行的，因为投影必须考虑摄像机的位置和方向。 总结 视图变换通过将物体从世界坐标系转换到摄像机坐标系，实现了场景的移动和旋转，使摄像机的位置和方向与新坐标系的原点和方向对齐。这个转换模拟了从摄像机的视点观察场景的效果，并为后续的投影变换提供了基础。 通过将物体转换到与摄像机视角相对应的坐标系，我们可以实现3D场景的逼真渲染，使观察者感觉自己就在场景中，可以自由移动和观察。这是3D图形渲染中的基本概念，用于创建沉浸式和逼真的虚拟世界。 构建摄像机/观察空间 观察空间是一个坐标系统，其中摄像机位于原点，摄像机的方向定义了新坐标系统的轴。 以下是创建观察空间的步骤概述： 摄像机位置：定义摄像机在世界空间中的位置。 摄像机方向：计算摄像机的方向向量，通常是通过摄像机位置和目标位置的差来计算。 右轴：通过摄像机方向向量和一个初始的上向量的叉乘来计算右向量，代表摄像机空间的x轴的正方向。 上轴：通过右向量和方向向量的叉乘来计算上向量，代表摄像机空间的y轴的正方向。 这些向量共同定义了观察空间，可以用于构建LookAt矩阵，该矩阵将世界坐标转换为相对于摄像机位置和方向的观察坐标。这是3D图形中常用的技术，允许我们从摄像机的视角查看场景，就像我们在现实世界中看到的一样。 通过使用LookAt矩阵，你可以实现各种摄像机效果，如第一人称摄像机、第三人称摄像机等，从而增强3D应用程序的真实感和沉浸感。 1、摄像机位置： 定义摄像机在世界空间中的位置。是我们自己构建的是吗？ 是的，定义摄像机在世界空间中的位置是你自己构建的一部分。摄像机的位置是一个3D向量，表示摄像机在世界坐标系中的位置。你可以根据需要自由选择这个位置。 例如，你可以将摄像机放置在世界空间的原点上，或者将其放置在场景的任何其他位置。摄像机的位置将决定你从哪个角度观察场景。 在OpenGL和GLM库中，你可以这样定义摄像机的位置： 1glm::vec3 cameraPos = glm::vec3(0.0f, 0.0f, 3.0f); 这里，摄像机位于世界空间的(0.0, 0.0, 3.0)位置。你可以根据场景的具体需求调整这些值。 摄像机的位置是3D图形中许多计算的基础，例如计算摄像机的方向、构建观察矩阵等。因此，正确设置摄像机的位置对于创建逼真的3D视觉效果至关重要。 2、初始向量上向量为什么不能直接作为上轴 右轴：通过摄像机方向向量和一个初始的上向量的叉乘来计算右向量，代表摄像机空间的x轴的正方向。 在这一句中，这个初始的上向量，不就是4的上轴了吗 你的观察是正确的，但让我详细解释一下这个过程。 在计算右轴时，我们确实使用了一个初始的上向量，但这个初始的上向量并不是最终的摄像机上轴。这个初始的上向量通常是一个全局的上向量，例如(0.0f, 1.0f, 0.0f)，代表世界空间中的y轴正方向。 以下是计算过程的步骤： 选择初始上向量：选择一个全局的上向量，例如(0.0f, 1.0f, 0.0f)。 计算右轴：使用初始上向量和摄像机方向向量的叉乘来计算右向量。这个右向量代表摄像机空间的x轴正方向。 计算真实的上轴：使用右向量和摄像机方向向量的叉乘来计算真实的上向量。这个上向量代表摄像机空间的y轴正方向。 代码示例： 123glm::vec3 up = glm::vec3(0.0f, 1.0f, 0.0f); // 初始上向量glm::vec3 cameraRight = glm::normalize(glm::cross(up, cameraDirection)); // 右轴glm::vec3 cameraUp = glm::cross(cameraDirection, cameraRight); // 真实的上轴 这个过程确保了右轴、上轴和摄像机方向向量彼此正交，并且符合右手坐标系。这是构建观察空间所必需的。 反例： 当然，让我们通过一个更通用的例子来说明为什么不能直接使用初始的上向量作为摄像机的上轴。 假设摄像机方向向量指向一个斜下方的点，例如： 1glm::vec3 cameraDirection = glm::normalize(glm::vec3(1.0f, -1.0f, -1.0f)); 现在，我们选择世界空间中的标准上向量作为初始的上向量： 1glm::vec3 up = glm::vec3(0.0f, 1.0f, 0.0f); 如果我们直接使用这个初始的上向量作为摄像机的上轴，那么摄像机的上方向将与世界空间的y轴正方向对齐。然而，由于摄像机是朝着斜下方看的，所以摄像机的真实上方向实际上是稍微倾斜的。 现在，让我们按照之前描述的方法计算右轴和真实的上轴： 12glm::vec3 cameraRight = glm::normalize(glm::cross(up, cameraDirection));glm::vec3 cameraUp = glm::cross(cameraDirection, cameraRight); 通过这种方法计算得到的真实上轴将与摄像机的方向和右轴垂直，反映了摄像机的真实上方向。 如果我们直接使用初始的上向量作为摄像机的上轴，那么得到的三个轴可能不会彼此垂直，从而破坏了观察空间的完整性。通过使用摄像机方向向量和右轴的叉乘来计算真实的上轴，我们可以确保得到的三个轴彼此垂直，并且与摄像机的实际方向和方位对齐。 这个例子说明了为什么我们不能直接使用初始的上向量作为摄像机的上轴，而是需要通过计算来得到真实的上轴。这确保了观察空间的完整性和准确性。 3、那上向量不是能算是真实的上轴，为什么它可以用来计算右轴 你提出了一个很好的问题，让我们深入探讨一下。 当我们计算右轴时，我们使用初始上向量和摄像机方向向量的叉乘。这个初始上向量通常是世界空间中的y轴正方向，例如(0.0f, 1.0f, 0.0f)。我们选择这个向量是因为它是一个方便的、已知的方向，通常与大多数摄像机方向不平行。 现在，为什么我们可以使用这个初始上向量来计算右轴，即使它不是摄像机的真实上轴？ 叉乘的性质：叉乘的结果是一个向量，该向量垂直于参与叉乘的两个向量。因此，使用摄像机方向向量和初始上向量的叉乘来计算右轴可以确保得到的右轴与摄像机方向向量垂直。 选择合适的初始上向量：我们选择的初始上向量通常与摄像机方向向量不平行（除非摄像机完全朝上或朝下）。这确保了叉乘的结果不是零向量，并且定义了摄像机空间的x轴正方向。 真实上轴的计算：一旦我们有了右轴，我们可以使用摄像机方向向量和右轴的叉乘来计算真实的上轴。这个真实的上轴与摄像机方向向量和右轴都垂直，因此它反映了摄像机的真实上方向。 总的来说，我们使用初始上向量来计算右轴，因为它是一个方便的、已知的方向，可以确保得到的右轴与摄像机方向向量垂直。然后，我们使用右轴和摄像机方向向量的叉乘来计算真实的上轴，确保所有三个轴彼此垂直，并且与摄像机的实际方向和方位对齐。这个过程确保了观察空间的完整性和准确性。","link":"/2023/08/12/LearnOpenGL3-Camera/"},{"title":"Shader Billboard","text":"广告牌功能的实现 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374Shader &quot;Unity Shaders Book/Chapter 11/Billboard&quot; { Properties { _MainTex (&quot;Main Tex&quot;, 2D) = &quot;white&quot; {} _Color (&quot;Color Tint&quot;, Color) = (1, 1, 1, 1) _VerticalBillboarding (&quot;Vertical Restraints&quot;, Range(0, 1)) = 1 } SubShader { // Need to disable batching because of the vertex animation Tags {&quot;Queue&quot;=&quot;Transparent&quot; &quot;IgnoreProjector&quot;=&quot;True&quot; &quot;RenderType&quot;=&quot;Transparent&quot; &quot;DisableBatching&quot;=&quot;True&quot;} Pass { Tags { &quot;LightMode&quot;=&quot;ForwardBase&quot; } ZWrite Off Blend SrcAlpha OneMinusSrcAlpha Cull Off CGPROGRAM #pragma vertex vert #pragma fragment frag #include &quot;Lighting.cginc&quot; sampler2D _MainTex; float4 _MainTex_ST; fixed4 _Color; fixed _VerticalBillboarding; struct a2v { float4 vertex : POSITION; float4 texcoord : TEXCOORD0; }; struct v2f { float4 pos : SV_POSITION; float2 uv : TEXCOORD0; }; v2f vert (a2v v) { v2f o; // Suppose the center in object space is fixed float3 center = float3(0, 0, 0); float3 viewer = mul(unity_WorldToObject,float4(_WorldSpaceCameraPos, 1)); float3 normalDir = viewer - center; normalDir.y =normalDir.y * _VerticalBillboarding; normalDir = normalize(normalDir); float3 upDir = abs(normalDir.y) &gt; 0.999 ? float3(0, 0, 1) : float3(0, 1, 0); float3 rightDir = normalize(cross(upDir, normalDir)); upDir = normalize(cross(normalDir, rightDir)); float3 centerOffs = v.vertex.xyz - center; float3 localPos = center + rightDir * centerOffs.x + upDir * centerOffs.y + normalDir * centerOffs.z; o.pos = UnityObjectToClipPos(float4(localPos, 1)); o.uv = TRANSFORM_TEX(v.texcoord,_MainTex); return o; } fixed4 frag (v2f i) : SV_Target { fixed4 c = tex2D (_MainTex, i.uv); c.rgb *= _Color.rgb; return c; } ENDCG } } FallBack &quot;Transparent/VertexLit&quot;} 使用场景 广告牌技术主要用于优化3D场景的性能和渲染效率。它可以应用于各种场景元素，特别是那些距离摄像机较远且对视觉质量要求不高的对象。如： 精灵和粒子系统：广告牌常用于渲染精灵和粒子系统。例如，火焰、烟雾、雨滴和雪花等效果往往使用广告牌技术实现，以确保这些效果始终面向摄像机，同时减少渲染的复杂度。 树木和植被：广告牌技术经常用于渲染远离摄像机的树木和植被。当树木和植被距离摄像机较远时，我们可以用一张包含树木或植被纹理的广告牌来代替复杂的3D模型。这样可以减少渲染的三角面数量，从而提高性能。 角色和道具：在某些情况下，广告牌技术也可以应用于角色和道具的渲染。例如，在游戏中，远离摄像机的非主要角色或小型道具可以用广告牌表示，以降低渲染负担。 背景元素：在3D场景中，广告牌技术可以用于渲染远处的背景元素，例如云朵、山脉和城市天际线等。这些元素通常位于远离摄像机的地方，使用广告牌技术可以节省渲染资源。 实现原理 广告牌（Billboarding）技术的实现原理是通过调整3D场景中的二维面片（Quad），使其始终面向摄像机。这使得我们可以在这些面片上贴上纹理（Texture），从而模拟复杂的3D物体或特效。广告牌技术的主要优势在于它可以大幅降低渲染负担，提高性能。 以下是广告牌技术的实现步骤： 创建一个四边形（Quad）：首先，我们需要一个四边形作为广告牌的基本形状。这个四边形通常是一个简单的2D矩形，由两个三角形组成。 面向摄像机：为了使四边形始终面向摄像机，我们需要在顶点着色器（Vertex Shader）中计算一个新的顶点位置。这个新位置需要考虑摄像机的位置和广告牌的中心位置。通过计算四边形中心到摄像机的向量，我们可以得到一个面向摄像机的法线向量。然后，我们可以通过这个法线向量计算出四边形的右向量和上向量，从而构建一个局部坐标系。这个局部坐标系会随着摄像机的移动而自动调整，确保四边形始终面向摄像机。 顶点位置变换：有了这个局部坐标系，我们可以将原始的顶点位置转换为新的顶点位置。这个转换过程通常涉及将顶点位置减去四边形的中心位置，然后使用局部坐标系的基向量进行旋转。最后，我们将旋转后的顶点位置加回四边形的中心位置，得到最终的顶点位置。 纹理贴图：为了在广告牌上显示我们需要的内容，我们需要为四边形分配一个纹理。在片元着色器（Fragment Shader）中，我们可以使用纹理坐标（UV Coordinates）采样纹理，并将纹理颜色与片元颜色相乘，从而实现广告牌的渲染。 通过以上步骤，广告牌技术可以实现在3D场景中的二维面片始终面向摄像机，从而在视觉上模拟复杂的3D物体或特效，同时提高性能。","link":"/2023/04/23/Billboard/"},{"title":"LearnOpenGL5-BasicLighting","text":"环境光 (Ambient Lighting) 环境光是光照模型中的一个组件，其目的是模拟现实世界中无处不在的背景光。在现实生活中，光不仅仅是直接从光源射出并照亮物体，而且还会从物体反射、散射到其他物体上，这使得即使在没有直接光线照射的地方，物体仍然可以看到一些光线。 例如，想象一个房间只有一个窗户，窗户的外面是太阳光。虽然太阳光只直接照亮房间的部分区域，但房间的其他部分并不是完全黑暗的，因为光线从地板、墙壁和其他物体上反射并散射到了那些没有直接受到太阳光照射的区域。这种间接的、全方位的光照就是环境光的现实基础。 在计算机图形学的基础光照模型中，真正模拟所有这些间接的光线反射和散射是非常复杂的，特别是在需要实时渲染的应用中。为了简化这一问题，环境光被引入作为一个简化的模型，为场景中的所有物体提供一个常数的亮度，以模拟这种无处不在的背景光。 在实际的编程实现中，环境光通常通过以下方式来计算： Iambient = Ilight X Kambient Iambient 是最终的环境光亮度 Ilight 是光源的环境光强度。 Kambient是物体的环境光反射系数，通常为一个介于0和1之间的值，表示物体如何反射环境光。 这种方法的一个主要缺点是它不考虑场景中的物体对环境光的遮挡。这就是为什么更高级的光照方法，如全局照明，尝试模拟这些遮挡效应，以获得更真实的渲染结果。 延伸 模拟环境光的遮挡效应通常需要更复杂的光照技术。其中一些主要的方法和技术如下： 全局照明 (Global Illumination): 这是一组算法和技术，旨在模拟光如何在场景中的物体之间反弹。其中最著名的全局照明算法有 Radiosity 和光线追踪 (Ray Tracing)。 环境光遮挡 (Ambient Occlusion, AO): AO 是一种技术，用于模拟小范围内的环境光遮挡。基本思想是检查物体表面的每个点周围的其他物体，以确定这个点有多少光被阻挡。结果是，凹陷或接近其他物体的地方通常会更暗，因为它们受到更多的遮挡。 屏幕空间环境光遮挡 (Screen Space Ambient Occlusion, SSAO): 这是一种实时渲染中常用的技术，它在屏幕空间内模拟AO，通常比传统的AO方法更快。 Photon Mapping: 这是一种模拟光如何在场景中反弹的方法。它通过追踪光子（光的粒子）来工作，这些光子从光源发射并在与物体相交时存储信息。 Path Tracing: 这是一种基于光线追踪的技术，它模拟光线如何从相机发射并在场景中多次反弹。每次反弹都会收集光照信息，从而模拟间接照明效果。 当考虑实时应用（如视频游戏）时，性能是一个重要的问题。因此，常常会使用近似和优化技术来实现上述方法。对于不需要实时的应用，例如电影制作或静态渲染，可以使用更精确（但计算密集型）的方法来获得最佳的光照效果。 总的来说，模拟环境光的遮挡和反弹是一个复杂的任务，需要权衡计算复杂性、实时性和渲染质量。 漫反射 (Diffuse Lighting) 漫反射是当光线击中物体表面后，光线会均匀地、各向异性地散射。这意味着光线不会像在镜面反射中那样只沿一个方向反射，而是会在许多方向上散射。这是大多数物体（特别是那些没有光泽的物体）显示其固有颜色的主要方式。 漫反射的计算 漫反射计算公式。 Idiffuse=Ilight×Kdiffuse×max⁡(0,N⋅L)I_{diffuse} = I_{light} \\times K_{diffuse} \\times \\max(0, \\mathbf{N} \\cdot \\mathbf{L}) Idiffuse​=Ilight​×Kdiffuse​×max(0,N⋅L) 这个公式描述了在一个特定点上漫反射的亮度是如何被计算的。 IlightI_{light} Ilight​ 这是光源的亮度。这可以是一个RGB颜色值，其中R、G、B分别代表红、绿、蓝的亮度。一个光源的亮度可以是常数，例如一个常亮的点光源，或者它可以基于距离、方向或其他因素而变化。 IdiffuseI_{diffuse} Idiffuse​ 这是物体的漫反射材质系数。它也是一个RGB颜色值，表示物体的固有颜色。例如，红色的苹果可能有一个高的红色分量和较低的绿色和蓝色分量。 N⋅L\\mathbf{N} \\cdot \\mathbf{L} N⋅L N是物体表面上点的法线向量。法线是垂直于表面的一个单位向量。 L是从物体表面上的点指向光源的向量。 这两个向量的点积会得到一个标量值，这个值表示两个向量之间的角度的余弦值。 当光完全垂直照在物体表面上时（即法线向量与指向光源的向量之间的角度为0度），点积的值为1，表示光的强度是最大的。当光与物体表面形成一个锐角时，点积的值逐渐减小，表示光的强度减弱。当光线与物体表面平行时，点积的值为0，表示没有光照亮该表面。 Max 使用max函数确保当光线在物体表面的背面（即点积为负数）时，漫反射亮度为0。这是因为，从几何上看，如果光线在物体的背面，那么这个点就不应该受到光源的照射。 关于法线 法线（normal）这个概念并不是专门为了漫反射而创建的。法线在数学和计算机图形学中是一个基础概念，用于描述一个表面上某一点的方向。这个方向垂直于该点处的表面。 法线在很多计算机图形学的应用中都是非常关键的，其中包括： 光照计算：如我们之前讨论的，法线用于计算一个表面点如何与光互动，不仅限于漫反射，还包括环境光、镜面反射等。 碰撞检测：在物理模拟和游戏中，法线可以用于确定物体在碰撞后应该如何反弹。 纹理映射：在高级的渲染技术中，如法线映射（normal mapping），法线用于模拟表面的细节，使其看起来更加真实，即使在没有实际几何细节的情况下。 曲面细分：在曲面建模和细分中，法线用于帮助决定如何拆分或细分一个表面，以得到更平滑或更精细的结果。 总之，法线是计算机图形学中的一个核心概念，它在很多方面都有应用，而不仅仅是在漫反射光照中。 镜面反射(Specular) “Specular”（镜面反射）描述的是当光线照射到一个物体上，特别是一个有光泽的物体上时，它如何被反射。这种反射通常会产生一个明亮的高光，使物体看起来有光泽或闪亮。 如图。在镜面反射的计算中，关键的部分是确定反射光线 ( R ) 与观察者视线 ( V ) 之间的关系。当 ( R ) 与 ( V ) 非常接近或对齐时，观察者会看到一个明亮的高光点。 具体来说，高光的亮度是基于 ( R ) 和 ( V ) 之间的夹角的。夹角越小，亮度越高。此外，材质的光滑度或粗糙度也会影响高光的大小和分散程度。光滑的材质会产生小而集中的高光，而粗糙的材质会产生更分散的高光。 所以，确实可以说镜面反射的效果大致与 ( R ) 落在 ( V ) 上的“大小”或接近程度有关。 公式为： Ispecular=Ilight×Kspecular×(R⋅V)nI_{specular} = I_{light} \\times K_{specular} \\times (R \\cdot V)^n Ispecular​=Ilight​×Kspecular​×(R⋅V)n “Specular”（镜面反射）描述的是当光线照射到一个物体上，特别是一个有光泽的物体上时，它如何被反射。这种反射通常会产生一个明亮的高光，使物体看起来有光泽或闪亮。 其中： I_light 是光源的强度。 K_specular 是物体的镜面反射系数。 R 是反射向量。 V 是从表面点到观察者的向量。 n 是物体的光泽度，决定高光的大小和尖锐度。 反射向量 ( R ) 可以通过入射向量 ( L )（从光源指向物体的向量）和法线 ( N ) 来计算。反射向量表示光从物体表面反射的方向。 反射向量 ( R ) 的计算公式是： R=2(N⋅L)N−LR = 2(N \\cdot L)N - L R=2(N⋅L)N−L 其中： R 是反射向量 L 是入射向量 N 是法线，通常它是单位向量。 请注意，为了确保反射向量正确，通常会对 ( L ) 和 ( N ) 进行归一化，即使它们变为单位向量。这样可以确保计算的正确性。","link":"/2023/10/12/LearnOpenGL5-BasicLighting/"},{"title":"LearnOpenGL4-Colors","text":"颜色相乘原理 当光线照射到物体上时，物体的颜色由它对不同颜色分量的光反射和吸收决定。在计算物体的最终颜色时，通常使用光源的颜色和物体的颜色相乘来模拟这一过程。 具体来说： 光源的颜色（例如，光源的发出的光）通常以RGB（红、绿、蓝）格式表示。 物体的颜色也以RGB格式表示。 当光线照射到物体上时，物体会根据其颜色属性对光线的不同分量进行吸收。吸收的颜色分量将不再可见，而被吸收的能量将被转化为热量。 剩下的未被吸收的光线分量被物体反射回来，成为我们所观察到的颜色。这个反射的颜色也以RGB格式表示。 最终，我们看到的物体颜色是光源颜色和物体颜色之间的乘积。这意味着如果物体吸收了某种颜色分量，那么这种颜色分量将不会出现在最终的物体颜色中。 这个过程可以通过将光源颜色与物体颜色相乘来模拟。例如，如果光源是红色(1, 0, 0)，物体是绿色(0, 1, 0)，那么乘积(1, 0, 0) * (0, 1, 0) = (0, 0, 0)，这意味着物体会吸收光源的红色分量，最终呈现为黑色。 这就是为什么在计算光照和渲染物体颜色时，通常使用这种基本的颜色相乘原理 例子： 123glm::vec3 lightColor(0.0f, 1.0f, 0.0f);glm::vec3 toyColor(1.0f, 0.5f, 0.31f);glm::vec3 result = lightColor * toyColor; // = (0.0f, 0.5f, 0.0f); 这个玩具吸收了光线中一半的绿色值，但仍然也反射了一半的绿色值。玩具现在看上去是深绿色(Dark-greenish)的。我们可以看到，如果我们用绿色光源来照射玩具，那么只有绿色分量能被反射和感知到，红色和蓝色都不能被我们所感知到。","link":"/2023/09/06/LearnOpenGL4-Colors/"},{"title":"OpenGL6_LightCaster","text":"一个标准的光照模型 Light = ambient+diffuse+specular 顶点函数 1234567891011121314151617181920#version 330 corelayout (location = 0) in vec3 aPos;layout (location = 1) in vec3 aNormal;layout (location = 2) in vec2 aTexCoords;uniform mat4 model;uniform mat4 view;uniform mat4 projection;out vec3 Normal;out vec3 FragPos;out vec2 TexCoords;void main(){ gl_Position = projection*view*model*vec4(aPos, 1.0); Normal = aNormal; FragPos = vec3(model * vec4(aPos, 1.0)); TexCoords = aTexCoords;} 片段函数： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061#version 330 core//create a Material structstruct Material{ //create a float for the shininess float shininess; //create a sampler2D for the diffuse map sampler2D diffuse; //create a specular map sampler2D specular; };//create a Light structstruct Light{ //create a vec3 for the position vec3 position; //create a vec3 for the ambient color vec3 ambient; //create a vec3 for the diffuse color vec3 diffuse; //create a vec3 for the specular color vec3 specular;};out vec4 FragColor;//create norm variablein vec3 Normal;//create FragPos variablein vec3 FragPos;//create TexCoords variablein vec2 TexCoords;//create materialuniform Material material;//create lightuniform Light light;//create viewPos(camera's position)uniform vec3 viewPos;void main(){ // ambient vec3 ambient = light.ambient * texture(material.diffuse, TexCoords).rgb; // diffuse vec3 norm = normalize(Normal); vec3 lightDir = normalize(light.position - FragPos); float diff = max(dot(norm, lightDir), 0.0); vec3 diffuse = light.diffuse * diff * texture(material.diffuse, TexCoords).rgb; // specular vec3 viewDir = normalize(viewPos - FragPos); vec3 reflectDir = reflect(-lightDir, norm); float spec = pow(max(dot(viewDir, reflectDir), 0.0), material.shininess); vec3 specular = light.specular * spec * texture(material.specular, TexCoords).rgb; vec3 result = ambient + diffuse + specular; FragColor = vec4(result, 1.0);} Directional Light（平行光） 平行光是指来自很远的光源的光线，这些光线彼此平行，与光源的具体位置无关。例如，虽然太阳不是无限远，但其距离足够远，以至于其光线可以被视为平行光。在计算光照时，所有来自平行光源的光线都具有相同的方向，不论物体和/或观察者的位置，对于场景中的每个物体，光的方向都是一致。 所以平行光的重要区别即是使用 1vec3 lightDir = normalize(DirLight.direction); 来替代了： 1vec3 lightDir = normalize(DirLight.position - FragPos); Point Light（点光源） 点光源（Point Light）是一种从单一点向所有方向发射光线的光源，类似于裸露的灯泡或蜡烛。这种光源在其发射点附近最亮，随着距离增加而减弱。衰减公式如下： Fatt=1.0Kc+Kl⋅d+Kq⋅d2F_{att} = \\frac{1.0}{K_c + K_l \\cdot d + K_q \\cdot d^2} Fatt​=Kc​+Kl​⋅d+Kq​⋅d21.0​ $ F_{\\textit{att}} $是衰减因子，用来乘以光的强度来得到考虑衰减后的光强。 $ K_{\\textit{c}} $ 是常量衰减项，通常为1.0。 $ K_{\\textit{l}} $ 是线性衰减项。 $ K_{\\textit{q}} $ 是二次衰减项。 $ d $ 是从光源到片段的距离。 123//cal attenuationfloat distance = length(light.position - fragPos);float attenuation = 1.0 / (light.constant + light.linear * distance + light.quadratic * (distance * distance));","link":"/2023/10/25/LearnOpenGL6-LightCaster/"},{"title":"LearnOpenGL7-PerspectiveProjection","text":"视锥（Frustum） 这是一个截断的金字塔，其中的“Near Plane”和“Far Plane”定义了可见的前后界限。任何在“Near Plane”之前或“Far Plane”之后的物体都不会被渲染。 视锥的四个边界（l, r, b, t）分别代表： l：left，左边界 r：right，右边界 b：bottom，下边界 t：top，上边界 这些边界与近裁剪面（Near Plane）相交，定义了在屏幕上看到的实际区域。 透视投影矩阵（Perspective Projection Matrix） 在图的下方，一个矩阵P。这是透视投影矩阵，用于将物体从视锥映射到规范化的设备坐标（Normalized Device Coordinates，简称NDC）。 矩阵推导 1、标准化视锥: 首先希望将视锥规范化，这意味着希望将它转换成一个顶部和底部宽度为2，深度范围从-1到1的立方体。这可以通过缩放和平移完成。 1、缩放 S=[2r−l00002t−b00002n−f00001]S = \\begin{bmatrix} \\frac{2}{r - l} &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; \\frac{2}{t - b} &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; \\frac{2}{n - f} &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix} S=⎣⎢⎢⎢⎡​r−l2​000​0t−b2​00​00n−f2​0​0001​⎦⎥⎥⎥⎤​ x方向的缩放因子是为了确保经过变换后，x轴上的坐标范围被映射到[-1, 1]。这一缩放因子是基于视锥体在x轴上的宽度确定的。 考虑视锥体在x方向的宽度是(r - l)，其中r是右边界，l是左边界。为了使这个范围映射到[-1, 1]，需要将其宽度变为2。所以，缩放因子应该是目标宽度除以实际宽度，即： 缩放因子=2r−l\\text{缩放因子} = \\frac{2}{r - l} 缩放因子=r−l2​ 这样，当乘以这个缩放因子后，整个x轴上的范围(r - l)就被映射到了2的宽度，即[-1, 1]。同样的逻辑也适用于y方向和z方向，只不过每个方向的缩放因子是基于其相应的宽度确定的。 同理也可得出y和Z 但Z上的分量为什么是负的呢： 在大多数计算机图形系统中，尤其是OpenGL，使用的是右手坐标系。在这种坐标系中，当你从原点沿z轴的正方向看去，物体离你越远，它的z值就越大。但在深度测试中，通常希望物体离摄像机越近，其深度值越小。 为了解决这个矛盾，将摄像机空间（也叫眼睛空间）的z轴翻转，使得当物体远离摄像机时，其z值变得更小。这是通过将z值取负来实现的，因此在透视投影矩阵中，z分量是负的。 2、平移 平移，就是把整个视锥的中心点平移到原上图中坐标的（0,0,0），使视锥居中 T=[100−(r+l)/2010−(t+b)/2001−(f+n)/20001]T = \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; -(r + l)/2 \\\\ 0 &amp; 1 &amp; 0 &amp; -(t + b)/2 \\\\ 0 &amp; 0 &amp; 1 &amp; -(f + n)/2 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix} T=⎣⎢⎢⎢⎡​1000​0100​0010​−(r+l)/2−(t+b)/2−(f+n)/21​⎦⎥⎥⎥⎤​ **以x方向为例：**目标是将视锥体在x方向上平移到原点，使其变得对称。要达到这个目的，需要计算左（l）和右（r）边界的中点。 考虑以下步骤： 找到x方向上的中点：(l + r) / 2 要将这个中点移到原点，需要在x方向上应用相反的移动，即-(l + r) / 2 同理可以得到y 和z 这两个矩阵相乘：P = S x T。 P=[2r−l0−r+lr−l002t−b−t+bt−b0002n−f−f+nf−n0001]P = \\begin{bmatrix} \\frac{2}{r - l} &amp; 0 &amp; -\\frac{r + l}{r - l} &amp; 0 \\\\ 0 &amp; \\frac{2}{t - b} &amp; -\\frac{t + b}{t - b} &amp; 0 \\\\ 0 &amp; 0 &amp; \\frac{2}{n - f} &amp; -\\frac{f + n}{f - n} \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix} P=⎣⎢⎢⎢⎡​r−l2​000​0t−b2​00​−r−lr+l​−t−bt+b​n−f2​0​00−f−nf+n​1​⎦⎥⎥⎥⎤​ 3、深度映射 首先需要理解，为什么要对深度进行映射。 在计算机图形学中，深度值通常被用于深度测试，以确定哪个片段在最终的渲染图像中是可见的。深度缓冲区中的值通常是介于0和1之间的，因此希望将所有可见的几何图形的深度范围（从近平面到远平面）映射到这个区间。 为此，需要对Z值进行适当的缩放和平移，以使其落入所需的范围。 透视投影的Z值变换: 在透视投影中，近平面和远平面的深度不是线性的。这是因为透视效应导致距离摄像机更远的对象投影到屏幕上的面积更小。 需要一个非线性的深度映射，使得物体在摄像机附近时深度变化更快，而在摄像机远处时深度变化更慢。这样，可以更有效地利用深度缓冲区的精度。 Z值映射的公式: 为了得到上述效果，使用以下公式来计算新的z值和w值: Znew=−2fnf−n×1ZoriginalZ_{\\text{new}} = \\frac{-2fn}{f - n} \\times \\frac{1}{Z_{\\text{original}}} Znew​=f−n−2fn​×Zoriginal​1​ Wnew=−(f+n)f−n×1ZoriginalW_{\\text{new}} = \\frac{-(f + n)}{f - n} \\times \\frac{1}{Z_{\\text{original}}} Wnew​=f−n−(f+n)​×Zoriginal​1​ 其中, 是摄像机视图空间中的原始z值，而( f ) 和( n ) 分别是远平面和近平面的深度。 ZoriginalZ_{\\text{original}}Zoriginal​是摄像机视图空间中的原始z值，而$ f $ 和nnn分别是远平面和近平面的深度。 当进行透视除法（即将x, y, z分别除以w）时，得到:上述公式会生成介于-1和1之间的Z值，这正是在裁剪空间中所期望的。 Zclip=ZnewWnewZ_{\\text{clip}} = \\frac{Z_{\\text{new}}}{W_{\\text{new}}} Zclip​=Wnew​Znew​​ 为什么使用这个公式: 上述公式确保了两件事情： 近平面的深度映射到-1 远平面的深度映射到1 这样，所有在近平面和远平面之间的几何图形都会被映射到-1和1之间，这使得它们可以正确地进行裁剪和深度测试。 这就是透视投影中深度映射的基本原理和推导过程。 4、透视除法: 在得到新的坐标后，为了获得透视效果，需要进行透视除法。这意味着将x、y和z坐标除以w’。 在OpenGL中，透视除法是在将顶点从裁剪空间变换到规范化设备坐标（NDC）空间时自动执行的。这是在光栅化阶段之前的最后一个阶段。 5、z轴翻转: 在OpenGL中，为了与右手坐标系一致，需要翻转z轴。这是通过以下矩阵来完成的： [1000010000−100001]\\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; -1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\\\ \\end{bmatrix} ⎣⎢⎢⎢⎡​1000​0100​00−10​0001​⎦⎥⎥⎥⎤​ 6、结果将上述矩阵相乘，可以得到最终的透视投影矩阵P： [2n/(r−l)0(r+l)/(r−l)002n/(t−b)(t+b)/(t−b)000(f+n)/(f−n)2fn/(f−n)00−10]\\begin{bmatrix} 2n/(r - l) &amp; 0 &amp; (r + l)/(r - l) &amp; 0 \\\\ 0 &amp; 2n/(t - b) &amp; (t + b)/(t - b) &amp; 0 \\\\ 0 &amp; 0 &amp; (f + n)/(f - n) &amp; 2fn/(f - n) \\\\ 0 &amp; 0 &amp; -1 &amp; 0 \\\\ \\end{bmatrix} ⎣⎢⎢⎢⎡​2n/(r−l)000​02n/(t−b)00​(r+l)/(r−l)(t+b)/(t−b)(f+n)/(f−n)−1​002fn/(f−n)0​⎦⎥⎥⎥⎤​ 2、Z值映射的公式推理 Z值映射的公式并非简单地套入一个数学曲线。它是基于线性代数和几何图形学的原理来推导出的。为了更深入地解释这一点，将从更高层次上分析这一问题。 首先，考虑摄像机/观察空间中的一个点P(x, y, z)，其中z是从摄像机/眼睛位置测得的深度。 目标是将这个点投影到一个近平面上，并为这个点分配一个新的z值，这个z值应该在整个投影过程中是非线性的，这样就可以充分利用深度缓冲区的精度。 投影到近平面: 使用透视除法，点P(x, y, z)的屏幕空间坐标为: ( X = x/z ) ( Y = y/z ) 这一步已经将点投影到了近平面上。 非线性深度映射: 现在，要做的是为这个点分配一个新的z值。希望这个z值的范围在[0,1]之间（或[-1,1]，取决于你的图形API），并且它应该是非线性的。 推导新的Z值: 在摄像机空间中，点P的z坐标是一个负数，因为摄像机通常朝向-Z方向。可以使用以下公式来获得新的z值: ( Z’ = A/z + B ) 其中，A和B是常数，需要确定它们的值，以确保近平面映射到0（或-1），远平面映射到1。 设定两个方程： 当z = -n时，( Z’ = 0 ) （或-1） 当z = -f时，( Z’ = 1 ) 解这两个方程，可以得到A和B的值。通过一些代数运算，可以得到： A=−(f+n)(f−n)A = -\\frac{(f + n)}{(f - n)}A=−(f−n)(f+n)​ B=−2fn(f−n)B = -\\frac{2fn}{(f - n)}B=−(f−n)2fn​ 因此，新的Z值为: Z′=(−2fn×zf−n)−(f+n)(f−n)Z' = (-2fn \\times \\frac{z}{f - n}) - \\frac{(f + n)}{(f - n)}Z′=(−2fn×f−nz​)−(f−n)(f+n)​ 通过上述步骤，可以得到深度值的非线性映射。这确保了深度缓冲区的更有效使用，因为在摄像机附近的物体有更高的深度精度。 3、透视公式 ( Z’ = A/z + B ) ​ 公式 ( Z’ = A/z + B ) 的推导实际上是基于透视投影的需求。为了理解这个公式，可以考虑以下几点： 线性深度不足以满足透视效果：在透视投影中，离摄像机越近的物体应该看起来越大，离摄像机越远的物体应该看起来越小。因此，深度（即z值）对于透视效果的影响是非线性的。这就是为什么不能仅仅使用线性的深度映射。 透视分布：为了保持这种透视关系，并在z缓冲中有效地使用深度值，需要一个公式，它可以为近的物体分配更多的深度范围，为远的物体分配较少的深度范围。公式 ( Z’ = A/z ) 可以实现这个效果，因为随着z值的增加，Z’的增量逐渐减小。 归一化深度值：为了确保深度值适应特定的范围，如[0, 1]或[-1, 1]，加入常数B来进行偏移。这也解释了为什么有 “+ B” 这个项。 这个公式的出现并不是随意的，而是为了满足计算机图形学中透视投影的特定需求。 4、透视除法 透视除法是3D图形渲染中一个非常重要的步骤，它将齐次坐标从投影空间转换到屏幕空间。简单来说，当在3D空间中进行变换时，通常使用4x4的矩阵和齐次坐标（即一个额外的维度w，所以坐标是(x, y, z, w)）来进行计算。这个额外的w分量是为了能够执行诸如透视投影这样的非线性变换。 透视除法涉及到用w分量来除x、y和z分量。具体来说，当你有一个点P(x, y, z, w)，透视除法后的坐标是： 123x' = x / wy' = y / wz' = z / w 透视除法的关键之处在于，当执行透视投影时，w分量不再是1，而是与点到摄像机的距离成正比。这意味着，一个点离摄像机越远，经过透视除法后，它在屏幕上的坐标x’和y’会变得越小，从而实现了透视效果。 例如，考虑一个远离摄像机的点，它的w值可能会增加，这样当用w来除时，得到的x’和y’值会相对较小，导致该点在屏幕上看起来离中心更远，从而产生了透视效果。 透视除法是将点从齐次坐标（投影空间）转换到屏幕空间的关键步骤，它提供了透视效果，使得远离摄像机的物体看起来更小。 [abcd]\\begin{bmatrix} a &amp; b \\\\ c &amp; d \\end{bmatrix} [ac​bd​]","link":"/2023/10/28/LearnOpenGL7-PerspectiveProjection/"},{"title":"LearnOpenGL8-StencilTesting","text":"模板测试（Stencil Testing）是3D图形渲染中的一个高级技术，它利用模板缓冲区（Stencil Buffer）来控制像素是否应该被绘制。这个技术可以用于多种渲染效果，如轮廓绘制、裁剪（Clipping）、阴影生成等。下面是一个对模板测试的总结： 1、模板缓冲区的作用 模板缓冲区是帧缓冲的一部分，可以看作是一个额外的图像，它为每个像素提供额外的信息。这些信息可以用于决定是否以及如何绘制一个像素。 1、基本原理： 写入模板值： 在渲染过程中，可以选择是否在模板缓冲区中为每个像素写入一个值。 模板测试： 在绘制像素之前，会根据模板缓冲区中的值和预设的模板测试条件来决定是否绘制该像素。 2、模板测试的步骤： 清除模板缓冲区： 在渲染开始前，通常需要清除模板缓冲区。 设置模板测试条件： 通过glStencilFunc设置模板测试的条件。 渲染与写入模板： 在第一次渲染中，根据设定的条件决定哪些像素将更新模板缓冲区的值。 使用模板值来控制渲染： 在后续的渲染步骤中，利用模板值来决定是否和怎样绘制像素。 3、OpenGL中的模板操作： glStencilMask： 控制模板缓冲区中哪些位可以被更新。 glStencilMask函数在OpenGL中用于控制哪些位可以被写入模板缓冲区。当调用这个函数时，为它提供一个位掩码，这个掩码决定了哪些位可以被更新。 具体来说： glStencilMask(0xFF): 这个调用允许所有位被写入模板缓冲区。在这里，0xFF是一个16进制数，其二进制表示为11111111，表示模板缓冲区的每一位都可以被更新。 glStencilMask(0x00): 这个调用不允许任何位被写入模板缓冲区。其二进制表示为00000000，表示所有的位都被屏蔽了，因此模板缓冲区的值在这个掩码下不会发生改变。 在使用模板测试进行轮廓绘制时，通常会设置glStencilMask(0xFF)来允许在第一遍绘制时写入模板缓冲区。然后，在绘制放大的轮廓对象之前，会调用glStencilMask(0x00)，这样在绘制放大的对象时就不会修改模板缓冲区的内容。这确保了仅有第一遍绘制的物体影响了模板缓冲区的内容，第二遍绘制（放大的物体）时只是利用模板缓冲区的值来决定是否绘制像素，但不改变模板缓冲区的值。 使用glStencilMask是管理和控制模板测试细节的重要手段之一。 看一些例子： 默认掩码: glStencilMask(0xFF) 或 glStencilMask(255)：允许所有8个位被修改。这是典型的8位模板缓冲区的默认设置。 禁止写入: glStencilMask(0x00)：禁止对模板缓冲区的任何位进行修改。 选择性掩码: glStencilMask(0xF0)：只有模板缓冲区的高四位能被修改（1111 0000）。 glStencilMask(0x0F)：只有模板缓冲区的低四位能被修改（0000 1111）。 交替位: glStencilMask(0xAA)：模板缓冲区的每个偶数位可以被修改（1010 1010）。 glStencilMask(0x55)：模板缓冲区的每个奇数位可以被修改（0101 0101）。 在更复杂的场景中，可能会使用这种掩码来维护模板缓冲区中的某些位的值，而其他位可以随着不同的渲染步骤而改变。例如，在一个多通道渲染过程中，可能希望一部分位保留一些标记信息，而其他位则用于执行标准的模板测试和写入操作。通过使用不同的掩码，可以保护某些位不被某次特定的渲染操作修改，从而实现更高级的渲染技巧。 在实际使用OpenGL进行图形渲染时，模板缓冲区和glStencilMask可以用于许多高级技术。这里举一些实际应用的例子：1. 多层渲染效果 假设想在一个3D场景中创建一个具有多层次效果的玻璃窗，其中一些图案只出现在特定的区域。可以使用glStencilMask来限制某些位，以便在这些位上绘制特定的图案。2. 镜像效果如果要在场景中加入水面或镜面，并在其上产生反射，可以先使用模板测试来绘制水面/镜面区域，然后通过调整glStencilMask来保护这些区域，之后进行反射对象的绘制。3. 阴影体积,在生成阴影时，可以使用模板缓冲区来累加阴影体积。这里，glStencilMask可以用于控制哪些位负责记录阴影体积的累加。4. 门户渲染,在一个有门户系统的场景（比如多个房间通过门连接），可以使用模板缓冲区来确保只有通过门口可见的区域被渲染。这可以通过在渲染每个房间之前设置glStencilMask来实现，只有对应的门户位被设为可写。5. 分级透明度,在渲染具有不同透明度层级的对象时，可以使用模板缓冲区来确保按正确的顺序渲染这些层。例如，先渲染不透明物体，然后使用glStencilMask来渲染半透明和全透明物体。6. 裁剪区域使用模板缓冲区来实现复杂的裁剪区域，比如文字或其他图案裁剪。可以设置glStencilMask来裁剪场景的特定部分，然后只渲染这些部分。 实际的应用场景可能需要结合多个OpenGL状态和操作，glStencilMask提供了对模板缓冲区写入操作的更细致的控制，使得这些复杂效果的实现成为可能。 glStencilFunc：** 设置模板测试的条件。 glStencilFunc(GLenum func, GLint ref, GLuint mask)此函数决定如何使用模板缓冲值来决定片段是否应被丢弃或保留。 参数解释： func 指定一个比较函数，该函数用于将参考值与存储在模板缓冲中的值进行比较。 GL_NOTEQUAL: 表示只有当参考值不等于模板缓冲中的值时，片段才会被保留。 ref 提供的参考值，将与模板缓冲中的值进行比较。 1: 在这里，参考值是1。 mask 一个掩码，与参考值和模板缓冲中的值按位与运算。 0xFF: 代表255或二进制中的11111111。这意味着考虑所有的位。 glStencilOp：** 当模板测试和/或深度测试通过或失败时，指定模板缓冲区的更新操作。 glStencilOp(GLenum sfail, GLenum dpfail, GLenum dppass)此函数定义了当模板测试失败或成功时模板缓冲中的值应该如何更新。 参数解释： sfail 如果模板测试失败，应该采取什么行动。 GL_KEEP: 如果模板测试失败，保持当前模板缓冲中的值不变。 dpfail 如果模板测试成功，但深度测试失败，应该采取什么行动。 GL_KEEP: 如果深度测试失败，保持当前模板缓冲中的值不变。 dppass 如果模板测试和深度测试都成功，应该采取什么行动。 GL_REPLACE: 如果两个测试都通过，则使用当前的模板测试的参考值替换模板缓冲中的值。 4、常见应用 裁剪： 使用模板缓冲区来限制渲染区域。 轮廓绘制： 通过在模板缓冲区中标记物体的位置，然后渲染扩大的版本来生成轮廓。 镜面反射： 利用模板测试来渲染反射物体。 阴影体积： 在光照计算中利用模板测试来生成阴影。 多通道渲染： 使用模板缓冲区来实现多个渲染通道。 模板测试是一个强大的工具，它通过对每个像素的细粒度控制来实现复杂的视觉效果。虽然理解起来可能有些复杂，但一旦掌握，它可以极大地增强3D图形程序的视觉效果和性能。 5、例子：轮廓绘制 准备工作: 开启模板缓冲功能。 设置模板测试规则: 在开始绘制任何物体之前，首先要设置模板测试的规则。 比如，可以设置规则为：“只渲染那些与先前标记的部分不重叠的区域”。 绘制原始物体: 接下来，使用想要的颜色或材质绘制物体。 由于模板测试规则已经设置，所以模板缓冲区会相应地被更新。 绘制放大的物体: 现在，稍微放大这个物体。 由于已经设置了模板测试的规则，所以只有那些与原始立方体不重叠的部分才会被画出来。这部分就是轮廓。 结果: 现在应该能在屏幕上看到一个有轮廓的立方体。 核心代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113// ----------------------------- glEnable(GL_DEPTH_TEST); glEnable(GL_STENCIL_TEST); glDepthFunc(GL_LESS); glStencilFunc(GL_NOTEQUAL, 1, 0xFF); glStencilOp(GL_KEEP, GL_KEEP, GL_REPLACE); float scale = 1.1; while (!glfwWindowShouldClose(window)) { //set the time float currentFrame = glfwGetTime(); deltaTime = currentFrame - lastFrame; lastFrame = currentFrame; //process the input processInput(window); //clear the color buffer and the depth buffer //enable the depth test //glEnable(GL_DEPTH_TEST); glClearColor(0.11f, 0.11f, 0.11f, 1.0f); glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT |GL_STENCIL_BUFFER_BIT); //set the shader shader.use(); glm::mat4 view = camera.GetViewMatrix(); glm::mat4 projection = glm::perspective(glm::radians(camera.Zoom), (float)SCR_WIDTH / (float)SCR_HEIGHT, 0.1f, 100.0f); shader.setMat4(shader.PROJECTION, projection); shader.setMat4(shader.VIEW, view); //draw the floor---------------------------------------------------------------------------- //set the model matrix glm::mat4 model = glm::mat4(2.0f); model = glm::translate(model, glm::vec3(0.0f, -1.0f, 0.0f)); shader.setMat4(shader.MODEL, model); // glStencilMask(0x00); glBindVertexArray(planeVAO); glBindTexture(GL_TEXTURE_2D, floorTexture); glDrawArrays(GL_TRIANGLES, 0, 6); // 1st. render pass, draw objects as normal, writing to the stencil buffer // ------------------------------------------------------------------------------ glStencilFunc(GL_ALWAYS, 1, 0xFF); glStencilMask(0xFF); // cube1 shader.use(); glBindVertexArray(cubeVAO); glActiveTexture(GL_TEXTURE0); glBindTexture(GL_TEXTURE_2D, cubeTexture); model = glm::mat4(1.0f); model = glm::translate(model, glm::vec3(-1.0f, 0.0f, -1.0f)); shader.setMat4(shader.MODEL, model); glDrawArrays(GL_TRIANGLES, 0, 36); glStencilFunc(GL_NOTEQUAL, 1, 0xFF); glStencilMask(0x00); glDisable(GL_DEPTH_TEST); shaderSingleColor.use(); shaderSingleColor.setMat4(shaderSingleColor.VIEW, view); shaderSingleColor.setMat4(shaderSingleColor.PROJECTION, projection); //cubes's border glBindVertexArray(cubeVAO); glBindTexture(GL_TEXTURE_2D, cubeTexture); model = glm::mat4(1.0f); model = glm::translate(model, glm::vec3(-1.0f, 0.0f, -1.0f)); model = glm::scale(model, glm::vec3(scale, scale, scale)); shaderSingleColor.setMat4(shaderSingleColor.MODEL, model); glDrawArrays(GL_TRIANGLES, 0, 36); //cube 2 glStencilFunc(GL_ALWAYS, 1, 0xFF); glStencilMask(0xFF); glEnable(GL_DEPTH_TEST); shader.use(); shader.setMat4(shader.PROJECTION, projection); shader.setMat4(shader.VIEW, view); model = glm::mat4(1.0f); model = glm::translate(model, glm::vec3(0.0f, 0.0f, 0.0f)); shader.setMat4(shader.MODEL, model); glDrawArrays(GL_TRIANGLES, 0, 36); glStencilFunc(GL_NOTEQUAL, 1, 0xFF); glStencilMask(0x00); //glDisable(GL_DEPTH_TEST); shaderSingleColor.use(); shaderSingleColor.setMat4(shaderSingleColor.VIEW, view); shaderSingleColor.setMat4(shaderSingleColor.PROJECTION, projection); model = glm::mat4(1.0f); model = glm::translate(model, glm::vec3(0.0f, 0.0f, 0.0f)); model = glm::scale(model, glm::vec3(scale, scale, scale)); shaderSingleColor.setMat4(shaderSingleColor.MODEL, model); glDrawArrays(GL_TRIANGLES, 0, 36); glBindVertexArray(0); //reset the stencil test glStencilMask(0xFF); glStencilFunc(GL_ALWAYS, 0, 0xFF); glEnable(GL_DEPTH_TEST); //swap the buffer glfwSwapBuffers(window); //process the event glfwPollEvents(); } 独立边框 在渲染每个立方体之前设置glStencilFunc和glStencilMask，以便在模板缓冲区中为每个立方体写入不同的值。 在渲染边框时，使用与立方体相匹配的模板值来确保边框只在正确的立方体周围绘制。 12345678910111213141516171819202122232425262728// ... [其他代码] ...// 1st cube - Setup stencil buffer write for first cubeglStencilFunc(GL_ALWAYS, 1, 0xFF); // Set any stencil to 1glStencilMask(0xFF); // Write to stencil buffer// Draw first cube as normal// ... [绘制第一个立方体的代码] ...// Draw border for 1st cubeglStencilFunc(GL_NOTEQUAL, 1, 0xFF); // Pass test if stencil value is not 1glStencilMask(0x00); // Don't write anything to stencil bufferglDisable(GL_DEPTH_TEST);// ... [绘制第一个立方体的边框代码] ...// 2nd cube - Setup stencil buffer write for second cubeglStencilFunc(GL_ALWAYS, 2, 0xFF); // Set any stencil to 2glStencilMask(0xFF); // Write to stencil bufferglEnable(GL_DEPTH_TEST);// Draw second cube as normal// ... [绘制第二个立方体的代码] ...// Draw border for 2nd cubeglStencilFunc(GL_NOTEQUAL, 2, 0xFF); // Pass test if stencil value is not 2glStencilMask(0x00); // Don't write anything to stencil buffer// ... [绘制第二个立方体的边框代码] ...// ... [其他代码] ... 这里为什么要用2呢，可以直接清理第一个的模板然后用1吗? 可以选择清除模板缓冲区并再次使用值 1，但这通常不是优选的做法，因为清除模板缓冲区可能会影响性能，并且如果正在渲染的场景较复杂时，也会更加复杂。相反，使用不同的值来区分不同的对象是更高效且更常见的做法。这样可以在一个渲染通道中渲染多个对象，然后为每个对象渲染它们的边框，而不需要在每次绘制新对象时都清空模板缓冲区。 例如，如果有多个物体，每个物体的边框都要用不同的颜色或风格渲染，可以为每个物体分配一个唯一的模板值，然后在渲染边框时检查对应的模板值来决定使用哪种颜色或风格。 因此，使用值 1、2、3 (8位，0-255的范围)等来标记不同的物体是标准做法。这样做的主要好处是不需要在每个渲染步骤之间清除模板缓冲区，这样可以保持渲染效率并简化代码逻辑。","link":"/2023/11/05/LearnOpenGL8-StencilTesting/"},{"title":"PolarCoordinates","text":"什么是极坐标 极坐标（Polar coordinates）是一种描述平面上点位置的坐标系统。在极坐标系统中，一个点的位置由它与原点的距离和它与参考方向（通常是正向 x 轴）的夹角来表示。 极坐标系统的坐标表示通常以 (r, θ) 的形式，其中 r 是点到原点的距离（极径），θ 是点与参考方向的夹角（极角）。极径 r 通常是非负数，而极角 θ 则通常用弧度表示，可以是任意实数。 相比之下，笛卡尔坐标系统使用直角坐标表示点的位置，即以水平和垂直的坐标轴（通常是 x 轴和 y 轴）的数值来表示点的位置。 极坐标系统在某些情况下可以更方便地描述点的位置和进行某些操作。例如，极坐标可以更直观地表示圆形、环形、螺旋等几何形状，以及进行旋转、缩放和扭曲等变换操作。在图形学、物理学、工程学等领域，极坐标系统常被用于描述和处理与圆形、圆环、径向对称等相关的问题。 极坐标的转换 123456789#ifndef POLAR_COORDINATES#define POLAR_COORDINATESfloat2 toPolar(float2 cartesian){ float distance = length(cartesian); float angle = atan2(cartesian.y, cartesian.x); return float2(angle / UNITY_TWO_PI, distance);}#endif 角度除以2π。这个除法的目的是将角度值归一化，并使其范围从-0.5到0.5，而不是常见的-π到π范围。这种归一化可以使角度在处理中更加方便，特别是在着色器编程或其他常用归一化值的应用中，如纹理坐标的映射等。相比之下，原始的角度范围(-π到π)可能更难以处理和控制。 如果直接根据以上的公式转换uv，会得出以下的渲染效果 123456//the fragment shader functionfixed4 frag (v2f i) : SV_Target { //get polar coordinates float2 uv = toPolar(i.uv); return float4(frac(uv), 0, 1); //test output} 在这里，只能看到1/4的圆。这是因为，正如前面所描述的，是围绕着“中心”计算角度，而在这里中心位于右下角。幸运的是，在笛卡尔空间中移动中心点非常容易，只需在转换之前从每个轴上减去0.5。由于HLSL会自动将标量转换为每个值都设置为该标量的向量，可以直接写作uv - 0.5。这样，空间范围将变为-0.5到0.5，因此让我其乘以2，以便得到-1到1的范围，并完整显示0-1空间。 123456789//the fragment shader functionfixed4 frag (v2f i) : SV_Target { //make input uvs centered and scaled to -1 to 1 i.uv -= 0.5; i.uv *= 2; //get polar coordinates float2 uv = toPolar(i.uv); return float4(frac(uv), 0, 1); //test output} 12345678910111213141516//the fragment shader functionfixed4 frag (v2f i) : SV_Target { //make input uvs centered and scaled to -1 to 1 i.uv -= 0.5; i.uv *= 2; //get polar coordinates float2 uv = toPolar(i.uv); //move discontinuity in coordinates to 0 uv.x = frac(uv.x); //tile Image // uv.x *= 3; // sample the texture and apply tint fixed4 col = tex2D(_MainTex, uv) * _Color; //return the final color to be drawn on screen return col;} 对于整个图像来说，完全的旋转可能会过于夸张并且会导致图像被拉伸,通过将其平铺多次以获得更好的结果，通过将uv.x乘以3，使图像在x轴上重复3次。这样做可以使得图像的纹理在坐标范围内重复出现，从而创建出更丰富的视觉效果。 1uv.x *= 3; 当在极坐标转换后的纹理坐标中，角度值跳跃从-1.5到1.5的时候，会出现奇怪的无缝问题。这是因为在这些像素点上，纹理采样时会根据内部的局部导数判断我们是否在很远的地方查看纹理，并选择较低的mipmap级别。解决这个问题的&quot;正确&quot;方法是自己计算mipmap级别，然后将其传递给tex2Dlod函数,原文提供了另一篇优秀文章的链接，介绍了这个问题的解决方法：https://bgolus.medium.com/distinctive-derivative-differences-cce38d36797b。 另一种解决方法是将这个无缝问题移动到不太明显的地方。在这种情况下，可以只取输出的x坐标的小数部分，因为默认情况下它的范围是-0.5到0.5，经过处理后会变成0到1，而无缝问题就位于第一张图像的起始处，即0度位置。通过这种处理方式，可以将无缝问题的边缘移动到不太明显的位置，从而减少其对图像的影响。 1uv.x = frac(uv.x); 文章参考于：https://www.ronja-tutorials.com/post/053-polar-coordinates/","link":"/2023/06/23/PolarCoordinates/"},{"title":"TheAdventureAndLegendOfTheEnchantedOre","text":"Once upon a time, in a small village nestled amidst a picturesque countryside, there was an unusual aroma lingering in the air. The odor (气味) of freshly baked bread wafted through the narrow streets, enticing the villagers with its irresistible scent. Among the villagers, there lived a young boy named Oliver, who possessed a unique talent. He had the ability to rub (擦) any two objects together and create sparks. This peculiar skill made him both admired and liable (有责任的) for any mishaps caused by his playful experiments. Oliver’s favorite place to indulge in his inventive endeavors was the old blacksmith’s forge (锻造场). He would spend hours there, tinkering with pieces of metal, shaping them into marvelous creations. His enthusiasm for craftsmanship was outward (向外的), and he dreamt of one day becoming a renowned artisan. One sunny day, while working in the forge, Oliver noticed a beam (横木) of light shining through a crack in the wall. Intrigued, he followed the beam to a hidden compartment where an ancient book lay untouched for centuries. The book contained intricate illustrations on how to reproduce (再现) mythical creatures thought to exist only in fables (寓言). Excitement filled Oliver’s figure (身影) as he flipped through the pages, studying the detailed instructions on how to bring these creatures to life. He discovered that the key ingredient for their existence was a rare mineral called enchanted ore (附魔矿石), which could only be found deep within the heart of a treacherous mountain. Determined to embark on this extraordinary journey, Oliver made preparations for his expedition. With a slam (砰) of his hammer, he crafted a sturdy accent (重音) to aid him in climbing the rugged terrain. His wrinkled (起皱的) hands meticulously packed supplies, knowing the path ahead would be challenging. As the sun began to hatch (孵化) over the horizon, Oliver set foot on his quest, armed with a map and fueled by his unwavering spirit. He navigated through dense forests, crossed swift rivers, and climbed treacherous cliffs with the agility of a seasoned explorer. Finally, after days of arduous travel, Oliver reached the mountain’s peak. He could hear the distant sound of grinding (磨碎) rocks as he approached the entrance to the enchanted ore mine. With caution and anticipation, he initiated (开始) his descent into the depths of the earth. The mine revealed a vast cavern filled with a shallow (浅的) pool of sparkling water. Resting on its surface were fragments of enchanted ore, glistening like precious gems. Oliver carefully collected the mineral, knowing it held the power to bring his fabled creatures to life. With the enchanted ore in his possession, Oliver made his way back to the village, his heart filled with a mix of accomplishment and curiosity. Upon his return, he gathered the villagers and shared his incredible tale. He described the creatures he had seen in the ancient book, captivating their imagination. Using the enchanted ore, Oliver followed the book’s instructions to recreate the creatures. As he chanted the necessary incantations, the air crackled with energy, and before their eyes, the mythical beings emerged from the realm of imagination into reality. A majestic phoenix soared through the sky, while a wise dragon roamed the countryside, guarding the village with its fiery breath. The village became a hub of biology (生物学) enthusiasts, as scholars from far and wide flocked to study these remarkable creatures. Oliver’s name echoed through the ages as the young boy who resisted the temptation (诱惑) to keep the enchanted ore for himself and instead shared its magic with the world. And so, the once ordinary village became an extraordinary place, where the extraordinary happened as if by automatic (自动的) design. Oliver’s courage, creativity, and unwavering spirit had forever changed the fate of the village and its inhabitants, leaving behind a legacy that would be told in tales for generations to come. 中文 从前，在一个坐落在风景如画的乡村的小村庄里，空气中弥漫着一种不寻常的气味。新鲜烤面包的气味飘过狭窄的街道，用其诱人的香气吸引着村民们。 在村子里住着一个名叫奥利弗的年轻男孩，他有着独特的天赋。他可以擦拭任何两个物体，然后产生火花。这种奇特的技能使他备受钦佩，但也使他有责任（有责任的）对他的玩乐性实验造成的任何意外负责。 奥利弗最喜欢从事他的发明工作的地方是老铁匠的锻造场。他会在那里待上几个小时，修补金属碎片，将它们塑造成奇妙的作品。他对手艺的热情向外散发着，他梦想有一天成为一名著名的工匠。 一个阳光明媚的日子，当他在锻造场工作时，奥利弗注意到一束光线从墙上的裂缝中射入。他好奇地跟着光线，发现了一个隐藏的隔间，里面有一本古老的书，已经被搁置了几个世纪。这本书上有关于再现寓言中传说中生物的复杂插图。 奥利弗翻阅着书页，兴致勃勃地学习如何让这些生物重新活过来。他发现这些生物存在的关键是一种稀有的矿石，被称为附魔矿石，只能在危险的山脉深处找到。 为了踏上这不寻常的旅程，奥利弗开始准备他的探险。他用锤子砰地一声，打造了一个坚固的重音，帮助他攀爬崎岖的地形。他皱纹斑驳的手仔细地整理着供给品，因为他知道前方的道路将充满挑战。 当太阳开始孵化出地平线时，奥利弗踏上了他的任务，带着一张地图和坚定的精神。他穿过茂密的森林，越过湍急的河流，用一名经验丰富的探险家的敏捷攀爬险峻的悬崖。 终于，在数天的艰苦旅行后，奥利弗抵达了山峰。他听到远处石头的磨碎声，当他走近附魔矿石矿井的入口时。小心翼翼地收集着这种矿石，他知道它拥有让传说中的生物重生的力量。 拥有附魔矿石后，奥利弗回到村庄，心中充满了成就感和好奇心。回到村子后，他召集了村民，与他们分享了他难以置信的故事。他描述了古老书中的生物，引起了他们的想象。 利用附魔矿石，奥利弗按照书中的指示重新创造了这些生物。当他吟唱必要的咒语时，空气中充满了能量，在他们眼前，这些神秘的生物从想象的领域走进了现实。一只雄伟的凤凰飞翔在天空中，一条智慧的龙在乡间漫游，用其火焰呼吸守护着村庄。 村庄成为生物学爱好者的聚集地，因为来自各地的学者涌来研究这些非凡的生物。奥利弗的名字在历代传颂，作为一个年轻的男孩，他抵制了诱惑将附魔矿石留给自己，而是将其魔力与世界分享。 于是，这个曾经平凡的村庄变成了一个非凡的地方，非凡的事情似乎如自动（自动的）般发生。奥利弗的勇气、创造力和坚定不移的精神永远改变了村庄和居民的命运，留下了一个代代相传的传奇。 单词 odor - 气味 rub - 擦拭 liable - 有责任的 forge - 锻造 outward - 向外的 beam - 光束，横梁 reproduce - 再现，繁殖 figure - 数字，图形，人物 ore - 矿石 fable - 寓言 slam - 猛力关闭，砰然关闭 accent - 口音，重音 wrinkle - 皱纹 hatch - 孵化 preposition - 介词 grind - 磨碎 initiate - 开始，发起 shallow - 浅的 pigeon - 鸽子 gaze - 凝视 biology - 生物学 temptation - 诱惑 automatic - 自动的","link":"/2023/07/27/TheAdventureAndLegendOfTheEnchantedOre/"},{"title":"判断点是否在平面内","text":"射线法 射线法是一种常用于判断点是否在多边形内部的方法。以下是如何使用射线法的示例： 假设我们有一个非凸多边形P，由顶点A(0,0)，B(4,0)，C(4,4)，D(2,2)和E(0,4)组成。我们想判断一个点F(1,1)是否在这个多边形内。 选择射线: 我们可以从点F发出一条水平射线，向右延伸。选择水平或垂直射线通常会简化计算，但理论上射线可以朝任何方向。 计算交点: 现在我们计算这条射线与多边形的边交叉的次数。要注意的是，如果射线与多边形的顶点相交，我们只计算它一次（除非这个顶点是多边形的凹点，这种情况较为复杂，通常需要特别处理）。 F与AB相交: 否 F与BC相交: 否 F与CD相交: 是 F与DE相交: 否 F与EA相交: 否 所以，射线与多边形交叉了1次。 判断奇偶: 因为交叉次数是奇数，所以点F位于多边形P的内部。 如果我们选取一个明显在多边形外部的点G(5,5)，并重复上述过程，我们会发现射线与多边形交叉了0次（偶数次），所以点G在多边形外部。 请注意，射线法有一些特殊情况，可能需要特别处理。例如，如果射线与多边形的一个凹顶点相交，或者与多边形的边完全重合，可能需要特别的逻辑来正确处理。不过在大多数情况下，射线法是一种简单有效的判断点是否在多边形内部的方法。 使用Unity实现 12345678910111213141516171819202122232425262728293031323334353637383940414243444546using UnityEngine;public class PointInPolygonChecker : MonoBehaviour{ public GameObject objectToCheck; // 你想检查的对象 private PolygonCollider2D polygonCollider; void Start() { polygonCollider = GetComponent&lt;PolygonCollider2D&gt;(); } void Update() { Vector2[] vertices = polygonCollider.GetPath(0); // 获取多边形的第一条路径的顶点 Vector2 pointToCheck = objectToCheck.transform.position; // 获取对象的位置 if (IsPointInPolygon(pointToCheck, vertices)) { Debug.Log(&quot;Point is inside the polygon!&quot;); } else { Debug.Log(&quot;Point is outside the polygon!&quot;); } } bool IsPointInPolygon(Vector2 point, Vector2[] vertices) { bool inside = false; int j = vertices.Length - 1; for (int i = 0; i &lt; vertices.Length; i++) { if ((vertices[i].y &lt; point.y &amp;&amp; vertices[j].y &gt;= point.y || vertices[j].y &lt; point.y &amp;&amp; vertices[i].y &gt;= point.y) &amp;&amp; (vertices[i].x &lt;= point.x || vertices[j].x &lt;= point.x)) { inside ^= (vertices[i].x + (point.y - vertices[i].y) / (vertices[j].y - vertices[i].y) * (vertices[j].x - vertices[i].x) &lt; point.x); } j = i; } return inside; }} 代码分析 在这个代码中，添加了一个公共字段objectToCheck，你以在Inspector中将其设置为想要检查的游戏对象。然后，在Update方法中，使用该对象的位置作为要检查的点。 请注意，这里假设多边形和要检查的对象都位于同一个平面内，且objectToCheck的位置的z坐标与多边形的z坐标相同（或至少足够接近，以便在2D空间内进行合适的比较）。如果这些假设不成立，可能需要进一步修改代码来正确处理3D空间中的坐标。 12if ((vertices[i].y &lt; point.y &amp;&amp; vertices[j].y &gt;= point.y || vertices[j].y &lt; point.y &amp;&amp; vertices[i].y &gt;= point.y) &amp;&amp; (vertices[i].x &lt;= point.x || vertices[j].x &lt;= point.x)) 这个if条件包含了两个主要的部分 这个if条件包含了两个主要的部分，我们可以按照你给的示例代码逐个解释它们。 检查y坐标是否在边界范围内: vertices[i].y &lt; point.y &amp;&amp; vertices[j].y &gt;= point.y：这部分检查当前边界的一个顶点是否在点的下方，而另一个顶点是否在点的上方。 vertices[j].y &lt; point.y &amp;&amp; vertices[i].y &gt;= point.y：这部分检查当前边界的一个顶点是否在点的上方，而另一个顶点是否在点的下方。 用更通俗的话说，这部分代码检查边界是否与点的水平线相交。 2.检查x坐标是否小于边界之一的x坐标: ​ 如果一个线段的两个端点都在所要检查的点的右侧，那么它与从该点向左延伸的水平线不会相交。但是，如果线段的至少一个端点在所要检查的点的左侧或与其垂直对齐，那么线段可能与从该点向左延伸的水平线相交。 ​ 这个条件用于过滤掉那些明显不会与从点向左延伸的水平线相交的线段，从而减少了不必要的计算。只有当线段至少有一个端点在所要检查的点的左侧时，才会进一步考虑该线段，并使用先前解释的数学表达式来确定线段是否实际与水平线相交，并据此更新inside变量的值。 1inside ^= (vertices[i].x + (point.y - vertices[i].y) / (vertices[j].y - vertices[i].y) * (vertices[j].x - vertices[i].x) &lt; point.x); 这行代码中使用了一些数学和编程技巧，让我们逐个分解。 计算交点的x坐标: (point.y - vertices[i].y) / (vertices[j].y - vertices[i].y)计算点的y坐标与线段两端之间的相对位置。 然后，乘以(vertices[j].x - vertices[i].x)得到与y坐标相对应的x坐标的差异。 最后，加上vertices[i].x得到交点的x坐标。 比较交点的x坐标与点的x坐标: 通过比较交点的x坐标和点的x坐标，我们可以知道交点是在点的左边还是右边。 如果交点在点的左边，则表达式的结果为true，否则为false。 使用异或（XOR）操作更新inside变量: ^=是异或赋值操作符。异或是一个逻辑操作，如果两个操作数不同，则结果为true，否则为false。 在这种情况下，如果交点在点的左边，则inside的值会切换。如果交点在点的右边，inside的值不变。 因此，如果交叉次数是奇数，inside最终为true；如果是偶数，则为false。 简单地说，这行代码计算了从点向左延伸的水平线与多边形边界的交点的x坐标，并检查交点是否在点的左侧。然后，它使用异或操作来计算交叉次数，并据此判断点是否在多边形内部。","link":"/2023/07/30/%E5%88%A4%E6%96%AD%E7%82%B9%E6%98%AF%E5%90%A6%E5%9C%A8%E5%B9%B3%E9%9D%A2%E5%86%85/"},{"title":"Sprite Outlines2","text":"效果如下： 代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899Shader &quot;Tutorial/049_SpriteOutline&quot;{ Properties{ _Color (&quot;Tint&quot;, Color) = (0, 0, 0, 1) _OutlineColor (&quot;OutlineColor&quot;, Color) = (1, 1, 1, 1) _OutlineWidth (&quot;OutlineWidth&quot;, Range(0, 1)) = 1 _MainTex (&quot;Texture&quot;, 2D) = &quot;white&quot; {} } SubShader{ Tags{ &quot;RenderType&quot;=&quot;Transparent&quot; &quot;Queue&quot;=&quot;Transparent&quot; } Blend SrcAlpha OneMinusSrcAlpha ZWrite off Cull off Pass{ CGPROGRAM #include &quot;UnityCG.cginc&quot; #pragma vertex vert #pragma fragment frag sampler2D _MainTex; float4 _MainTex_ST; float4 _MainTex_TexelSize; fixed4 _Color; fixed4 _OutlineColor; float _OutlineWidth; struct appdata{ float4 vertex : POSITION; float2 uv : TEXCOORD0; fixed4 color : COLOR; }; struct v2f{ float4 position : SV_POSITION; float2 uv : TEXCOORD0; float3 worldPos : TEXCOORD1; fixed4 color : COLOR; }; v2f vert(appdata v){ v2f o; o.position = UnityObjectToClipPos(v.vertex); o.worldPos = mul(unity_ObjectToWorld, v.vertex); o.uv = TRANSFORM_TEX(v.uv, _MainTex); o.color = v.color; return o; } float2 uvPerWorldUnit(float2 uv, float2 space){ float2 uvPerPixelX = abs(ddx(uv)); float2 uvPerPixelY = abs(ddy(uv)); float unitsPerPixelX = length(ddx(space)); float unitsPerPixelY = length(ddy(space)); float2 uvPerUnitX = uvPerPixelX / unitsPerPixelX; float2 uvPerUnitY = uvPerPixelY / unitsPerPixelY; return (uvPerUnitX + uvPerUnitY); } fixed4 frag(v2f i) : SV_TARGET{ //get regular color fixed4 col = tex2D(_MainTex, i.uv); col *= _Color; col *= i.color; float2 sampleDistance = uvPerWorldUnit(i.uv, i.worldPos.xy) * _OutlineWidth; //float2 sampleDistance = _MainTex_TexelSize.xy * _OutlineWidth; //sample directions #define DIV_SQRT_2 0.70710678118 float2 directions[8] = {float2(1, 0), float2(0, 1), float2(-1, 0), float2(0, -1), float2(DIV_SQRT_2, DIV_SQRT_2), float2(-DIV_SQRT_2, DIV_SQRT_2), float2(-DIV_SQRT_2, -DIV_SQRT_2), float2(DIV_SQRT_2, -DIV_SQRT_2)}; //generate border float maxAlpha = 0; for(uint index = 0; index&lt;8; index++){ float2 sampleUV = i.uv + directions[index] * sampleDistance; maxAlpha = max(maxAlpha, tex2D(_MainTex, sampleUV).a); } //apply border col.rgb = lerp(_OutlineColor.rgb, col.rgb, col.a); col.a = max(col.a, maxAlpha); return col; } ENDCG } }} 原理： 这种方法用于为纹理创建轮廓或边框效果，特别适用于风格化渲染或突出场景中的某些元素。该过程涉及在UV坐标周围的多个点采样纹理，并使用Alpha通道信息来确定轮廓。以下是该方法的逐步分解： 为目标对象创建一个新的着色器或材质，以便在其上应用轮廓效果。 在着色器中，根据原始UV坐标对纹理进行采样。这可以通过使用类似HLSL中的tex2D或GLSL中的texture2D函数来实现。 为Alpha通道设置阈值，以区分可见和不可见像素。 在原始UV坐标周围定义一个内核或一组相邻点，以便对纹理进行采样。这可以通过为U和V坐标设置偏移量来实现。 遍历相邻点，并在每个点处对纹理进行采样。 将每个样本的Alpha值与阈值进行比较。如果Alpha值高于阈值，则记住找到的最高Alpha值。 遍历所有相邻点后，检查原始像素是否不可见（即其Alpha值低于阈值），并且在相邻像素中找到的最高Alpha值高于阈值。 如果满足这两个条件，则通过将输出像素颜色设置为所需的轮廓颜色（通常为黑色或与原始纹理形成对比的颜色），并将Alpha值设置为1（完全不透明）来填充轮廓。 否则，使用原始纹理颜色和Alpha值作为输出像素。 这种方法将通过检测原始像素及其相邻像素之间的Alpha通道值差异，在纹理的可见部分周围创建轮廓或边框效果。当纹理的Alpha通道中可见和不可见像素之间存在明显区别时，轮廓会更加突出。 4、遍历相邻点，并在每个点处对纹理进行采样。 主要通过这句 12345#define DIV_SQRT_2 0.70710678118float2 directions[8] = {float2(1, 0), float2(0, 1), float2(-1, 0), float2(0, -1), float2(DIV_SQRT_2, DIV_SQRT_2), float2(-DIV_SQRT_2, DIV_SQRT_2), float2(-DIV_SQRT_2, -DIV_SQRT_2), float2(DIV_SQRT_2, -DIV_SQRT_2)}; 在片段函数中，我们首先创建一个方向数组，用于存储我们希望进行采样的方向。你可以牺牲一些速度来获得更多的灵活性，通过使用sin和cos函数来获得这些方向，但这取决于你的选择。我选择在8个方向上进行采样，包括四个主要方向以及对角线方向。重要的是，对角线方向也应该有长度1，如果我们只使用(1, 1)，它们的长度将是sqrt(2)（通过勾股定理很容易得到这个结果，即sqrt(1² + 1²)），我们需要将每个分量除以sqrt(2)，即使用1 / sqrt(2)，这样就可以了。 上面的代码定义了一个名为directions的数组，它包含了8个方向。这些方向分别是上、下、左、右以及对角线方向。DIV_SQRT_2是1除以sqrt(2)的结果，用于计算对角线方向的单位向量。这样，我们可以在这8个方向上对纹理进行采样，以实现轮廓效果。 1、计算轮廓宽度 1、以纹素大小为缩放 1float2 sampleDistance = _MainTex_TexelSize.xy * _OutlineWidth; 把上这句开启： 这句代码是用于计算轮廓宽度的实际采样距离，以纹理坐标（UV坐标）为单位。我们通过将纹理的纹素大小（_MainTex_TexelSize.xy）与轮廓宽度属性（_OutlineWidth）相乘来实现这一点。 _MainTex_TexelSize.xy：这是一个包含纹理的纹素大小（宽度和高度）的二维向量。它表示一个纹素在UV空间中所占的尺寸。_MainTex_TexelSize.x表示一个纹素在U轴上的尺寸，_MainTex_TexelSize.y表示一个纹素在V轴上的尺寸。这个变量是由Unity自动生成的，对应于_MainTex纹理。 _OutlineWidth：这是一个浮点数，表示轮廓的宽度，以纹理像素为单位。这个值可以在Unity的材质面板上调整，范围为0到10。 将这两个值相乘，我们可以得到一个二维向量sampleDistance，它表示轮廓宽度在UV空间中的实际采样距离。这样，在后面的循环中，我们就可以使用这个距离来计算周围采样点的UV坐标，从而实现不同宽度的轮廓效果。 2、以世界空间宽度为缩放 1float2 sampleDistance = uvPerWorldUnit(i.uv, i.worldPos.xy) * _OutlineWidth; ​ 只需要获取每个世界距离的uv距离，然后像目前使用纹素大小那样将其与轮廓宽度相乘，那么让我们为此编写一个函数。可以通过屏幕空间偏导数（更为人熟知的ddx，ddy和fwidth）来计算这个值。 偏导数使我们能够获得每个屏幕像素的uv变化以及每个屏幕像素的世界空间位置变化。我们必须获得uv变化的绝对值，以免意外获得负值，同时还要获得世界位置变化的长度，以便在相机旋转时获得正确的距离。 通过这些值，我们可以通过将每像素的uv除以每像素的单位数，得到x和y轴上的每单位uv值。在x和y方向上获取这些值后，我们只需将这两个值相加并返回它。 123456789float2 uvPerWorldUnit(float2 uv, float2 space){ float2 uvPerPixelX = abs(ddx(uv)); float2 uvPerPixelY = abs(ddy(uv)); float unitsPerPixelX = length(ddx(space)); float unitsPerPixelY = length(ddy(space)); float2 uvPerUnitX = uvPerPixelX / unitsPerPixelX; float2 uvPerUnitY = uvPerPixelY / unitsPerPixelY; return (uvPerUnitX + uvPerUnitY);} 这个函数接受两个输入参数：uv（纹理坐标）和space（世界空间位置）。它首先计算每个屏幕像素的uv变化（uvPerPixelX和uvPerPixelY），然后计算每个屏幕像素的世界空间位置变化（unitsPerPixelX和unitsPerPixelY）。接着，我们可以通过将每像素的uv变化除以每像素的世界空间位置变化来计算x和y轴上的每单位uv值（uvPerUnitX和uvPerUnitY）。最后，将这两个值相加并返回结果。 通过这个函数，我们可以根据世界空间宽度计算轮廓宽度，这样一来，当相机旋转或者距离变化时，轮廓宽度将保持一致。 7、使用最大Alpha使判断边沿 使用最大Alpha值来判断边缘的方法基于这样一个观察：在边缘处，一个低Alpha值（透明或半透明）的像素周围通常至少有一个较高Alpha值（不透明）的像素。在这种情况下，我们将遍历周围的采样点，并寻找最大的Alpha值。边缘检测的关键在于比较原始像素的Alpha值和周围采样点的最大Alpha值。 以下是整个过程的详细说明： 遍历周围的采样点，并找到最大Alpha值（maxAlpha）。 使用lerp()函数根据原始像素的Alpha值在原始颜色（col.rgb）和轮廓颜色（_OutlineColor.rgb）之间进行插值。当原始像素的Alpha值较低时（接近透明），插值结果将更接近轮廓颜色。 将col.a设置为max(col.a, maxAlpha)。这样，如果原始像素的Alpha值较低（透明或半透明），而周围采样点的最大Alpha值较高（不透明），则边缘处的像素将具有更高的Alpha值。 通过这种方法，我们可以找到边缘，并在边缘处使Alpha值变为不透明。这样一来，即使原始像素的Alpha值较低，轮廓部分仍然可见。这种方法的关键在于比较原始像素的Alpha值和周围采样点的最大Alpha值，从而识别出边缘并在边缘处生成可见的轮廓。","link":"/2023/05/07/Sprite-Outlines2/"},{"title":"读《掌控习惯》","text":"如何培养好习惯 第一定律：让它显而意见 填写习惯记分卡：记录下当前习惯，并持续留意它们。 应用执行意图：在特定时间和地点明确制定执行某项习惯的行为：“我将于【时间】在【地点】【行为】”。 应用习惯叠加：将新的习惯与已有的习惯相结合：“继【当前习惯】之后，我将会【新习惯】”。 设计你的环境：确保良好习惯的提示在你的环境中清晰明了。 第二定律：让它更具吸引力 利用对奖励的期待心理，与喜好绑定：为良好习惯设置适当的奖励机制，增加其吸引力。 第四定律：让它令人愉悦 即时满足、有回报的行为会被重复，避免受到惩罚的动作。 顺应人性，给能为长远带来回报的事情添加一点即时快乐，给不良的事情添加一点痛苦。 天天保持好习惯的习惯跟踪法： 创建视觉提示，提醒你采取行动。 建立内在激励机制，清楚地看到你的进步轨迹，并且不想失去它。 每当记录下一个成功的习惯实例时，享受到满足感。 习惯追踪提供了视觉证据，证明你正在塑造成为你想成为的那类人，这本身就是一种令人愉悦的即时、内在满足的形式。 如何破除坏习惯： 第一定律反用：让它脱离视线 降低坏习惯的出现频率，将其提示清除出你的环境。 第二定律反用：让它缺乏吸引力 第三定律反用：让它难以施行 增加与不良行为的相关阻力，提高戒除坏习惯的难度。 第四定律反用：让它令人厌恶 通过遵循以上方法，你将能够更好地培养良好习惯，同时有效地摆脱坏习惯。良好习惯的养成需要持之以恒的努力和耐心，但只要坚持下去，你将在未来的生活中获得积极的回报，成为更好的自己。","link":"/2023/07/27/%E8%AF%BB%E3%80%8A%E6%8E%8C%E6%8E%A7%E4%B9%A0%E6%83%AF%E3%80%8B/"},{"title":"SpriteOutline","text":"​ 由于项目需求，需要实现一个同一预制体下所有子物体精灵组合外描边的功能 ，如下图的效果： 本来以为是很简单的一个需求，这个网络上应该有很多成熟的方案。然后开始了两周的不断试错及踩坑功能。 方案一踩坑： 首先找到的是资源商店里面的这个资源： https://assetstore.unity.com/packages/vfx/shaders/photoshop-like-2d-sprite-outline-stroke-effect-105207?locale=zh-CN 这个方案的原理也很简单，就是遍历子所有子物体，读取象素信息，然后共同按照计算好的区域写入到一个Texture中，最后把Texture赋值给sprite。 这里由此产生了几个问题： 问题1： 由于是新new写入的Texture，在项目中会打断图集，DC会成倍产生 问题2： 在测试性能中发现场景中的物体超过50个(每个下有三个SpriteRender子物体)电脑上的运行时间直接超过了1s,在查了Profiler后发现是下面这个循环代码很慢。 12345678for (int y = 0; y &lt; height; y++) { for (int x = 0; x &lt; width; x++) { int index = width * y + x; if (pixels [index].a &gt; 0) { texture.SetPixel (x+(int)vo.offset.x , y+(int)vo.offset.y, pixels [index]); } }} 主要是这句 1texture.SetPixel (x+(int)vo.offset.x , y+(int)vo.offset.y, pixels [index]); 查了网上相关资源，建议不要在for里面使用SetPixel方法，建议使用SetPixels32或GetRawTextureData(https://docs.unity3d.com/ScriptReference/Texture2D.GetRawTextureData.html)替代,尝试使用SetPixels32，第一个Sprite的像素信息写入是正常的，但后面写入的像素信息都不正常，想不到相关处理方案优化陷入僵局。 问题3： 这个方案处理出来的边沿锯齿问题还是严重，不够平滑，锯齿表现严重，研究了抗锯齿的方案，未能很好的处理这个问题（这个挖个坑，以后研究一下）放大如下图： 对Shader不是很熟悉，也研究了许久也算是一种思路启发，现在贴上： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153// Unity built-in shader source. Copyright (c) 2016 Unity Technologies. MIT license (see license.txt)Shader &quot;Sprites/_OutlinePro&quot;{ Properties { _Size (&quot;Size&quot;, Int) = 1 _BlurSize (&quot;Blur Size&quot;, Int) = 0 _Color (&quot;Color&quot;, Color) = (1,1,1,1) _BlurAlphaMultiplier (&quot;Blur Alpha Multiplier&quot;, Float) = 0.7 _BlurAlphaChoke (&quot;Blur Alpha Choke&quot;, Float) = 1 _AlphaThreshold (&quot;Alpha Threshold&quot;, Float) = 0.05 _Buffer (&quot;Buffer&quot;, Int) = 0 [PerRendererData] _MainTex (&quot;Sprite Texture&quot;, 2D) = &quot;white&quot; {} //[HideInInspector] _Color (&quot;Tint&quot;, Color) = (1,1,1,1) //[MaterialToggle] PixelSnap (&quot;Pixel snap&quot;, Float) = 0 //[HideInInspector] _RendererColor (&quot;RendererColor&quot;, Color) = (1,1,1,1) [HideInInspector] _Flip (&quot;Flip&quot;, Vector) = (1,1,1,1) //[PerRendererData] _AlphaTex (&quot;External Alpha&quot;, 2D) = &quot;white&quot; {} //[PerRendererData] _EnableExternalAlpha (&quot;Enable External Alpha&quot;, Float) = 0 } SubShader { Tags { &quot;Queue&quot; = &quot;Transparent&quot; &quot;IgnoreProjector&quot; = &quot;True&quot; &quot;RenderType&quot; = &quot;Transparent&quot; &quot;PreviewType&quot; = &quot;Plane&quot; &quot;CanUseSpriteAtlas&quot; = &quot;True&quot; } Cull Off Lighting Off ZWrite Off Blend One OneMinusSrcAlpha Pass { CGPROGRAM #pragma vertex SpriteVert #pragma fragment OutlineSpriteFrag //#pragma target 4.0 #pragma multi_compile_instancing //#pragma multi_compile _ PIXELSNAP_ON //#pragma multi_compile _ ETC1_EXTERNAL_ALPHA #include &quot;UnitySprites.cginc&quot; float4 _MainTex_TexelSize; int _Size; int _BlurSize; //float4 _Color; float _BlurAlphaMultiplier; float _BlurAlphaChoke; float _AlphaThreshold; int _Buffer; float2 _inTexcoord; float4 _pixelTexcoord; float _pixelAlpha; int _blurThickness; int _strokeThickness; fixed4 _clearColor; fixed4 _outColor; float Distance(int x1, int y1, int x2, int y2) { int deltaX = x2-x1; int deltaY = y2-x1; int valTemp = deltaX * deltaX + deltaY * deltaY; return rsqrt(valTemp)*valTemp; } float InverseLerp(float a, float b, float value) { return clamp ((value - a) / (b - a), 0, 1); } //是否存在此象素 bool HasPixelAt(int x, int y) { _pixelTexcoord.x = _inTexcoord.x + (x * _MainTex_TexelSize.x); _pixelTexcoord.y = _inTexcoord.y + (y * _MainTex_TexelSize.y); _pixelAlpha = tex2Dlod (_MainTex, _pixelTexcoord).a; return (_pixelAlpha &gt;= _AlphaThreshold); } //这里主要是差值当前像素的透明度，和颜色设置 bool TrySetPixelAt(int x, int y) { if (!HasPixelAt (x, y)) return false; //获得当前透明象素到最近的不透明象素的距离 float pixelDistance = Distance (0, 0, x, y); pixelDistance -= _Buffer; if (pixelDistance &lt;= 0 || pixelDistance &gt; _Size) return true; //当前像素在所在_strokeThickness, _Size间的位置 float distancePercent = InverseLerp (_strokeThickness, _Size, pixelDistance); //distancePercent = lerp (distancePercent, 1 - distancePercent, _InvertBlur); //chokeScalar = lerp (chokeScalar, distancePercent, _InvertBlur); float blurAlphaMultiplier = lerp (_BlurAlphaMultiplier, _BlurAlphaMultiplier / _Size, distancePercent); float blurAlphaChoke = max (0.95, (pixelDistance - _strokeThickness) * _BlurAlphaChoke ) /0.95; float alphaScalar = ceil (saturate (pixelDistance - _strokeThickness)); // Only blur pixels greater than the stroke thickness. float finalColor = _Color.a * blurAlphaMultiplier / blurAlphaChoke; //更新透明度 _outColor.a = lerp (_Color.a,finalColor, alphaScalar); _outColor.rgb = _Color.rgb * _outColor.a; return true; } fixed4 OutlineSpriteFrag(v2f IN) : SV_Target { _inTexcoord = IN.texcoord; //模糊的厚度 _blurThickness = min (_BlurSize, _Size - 1); //线条厚度 _strokeThickness = _Size - _blurThickness; //maxGridRadius = _Size + _Buffer 线条至模糊范围 int i, j, gridSize, dir = -1, maxGridRadius = _Size + _Buffer; for (int gridRadius = 1; gridRadius &lt;= maxGridRadius; gridRadius++) { gridSize = gridRadius*2; j= 0; for (i = 0; i &lt; gridSize; i++) { j += i * dir; dir *= -1; //return false为重叠部分，不填充颜色直接返回颜色，不进入计算 if (TrySetPixelAt (0, 0)) return _clearColor; //对四个方向都做出差值 if (TrySetPixelAt (-gridRadius, j) || TrySetPixelAt (j, gridRadius) || TrySetPixelAt (gridRadius, -j) || TrySetPixelAt (-j, -gridRadius)) return _outColor; } } return _outColor; } ENDCG } }} 方案一总结： 由于要使用GetPixels读取像素信息重新生成一个Texture并重新写入，GetPixels过程中会从GPU重新复制一个Texture到CPU中，在内存中会有两份数据同时存在，这个方案合适于当前要显示的数量不多的请况下 更改思路： 根据方案一的总结，重新思考了解决方向，舍弃把所有像素写在一张贴图上的想法，直接备份两分，一份不描边，一份描边，描边的放在后面。 由于只用考虑描边功能，感觉应该是挺好实现的。于是有了以下 方案二： 在翻github，找到这么一个方案，就是上下左右四个方向，用四个pass分别做偏移最后做一个混合，先贴上代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214// Upgrade NOTE: replaced 'mul(UNITY_MATRIX_MVP,*)' with 'UnityObjectToClipPos(*)'Shader &quot;Sprites/Outline&quot;{ Properties { [PerRendererData] _MainTex (&quot;Sprite Texture&quot;, 2D) = &quot;white&quot; {} _Color (&quot;Tint&quot;, Color) = (1,1,1,1) [MaterialToggle] PixelSnap (&quot;Pixel snap&quot;, Float) = 0 // Add values to determine if outlining is enabled and outline color. [PerRendererData] _OutlineSize (&quot;Outline&quot;, Float) = 0 [PerRendererData] _OutlineColor(&quot;Outline Color&quot;, Color) = (1,1,1,1) } CGINCLUDE #include &quot;UnityCG.cginc&quot; struct appdata_t { float4 vertex : POSITION; float4 color : COLOR; float2 texcoord : TEXCOORD0; }; struct v2f { float4 vertex : SV_POSITION; fixed4 color : COLOR; float2 texcoord : TEXCOORD0; }; fixed4 _Color; v2f vert_outline(appdata_t IN) { v2f OUT; OUT.vertex = UnityObjectToClipPos(IN.vertex); OUT.texcoord = IN.texcoord; OUT.color = IN.color * _Color; #ifdef PIXELSNAP_ON OUT.vertex = UnityPixelSnap (OUT.vertex); #endif return OUT; } sampler2D _MainTex; sampler2D _AlphaTex; float4 _MainTex_TexelSize; //magic var float _OutlineSize; //outline size fixed4 _OutlineColor; // outlie color fixed4 SampleSpriteTexture (float2 uv) { fixed4 color = tex2D (_MainTex, uv); return color; } ENDCG SubShader { Tags { &quot;Queue&quot;=&quot;Transparent&quot; &quot;IgnoreProjector&quot;=&quot;True&quot; &quot;RenderType&quot;=&quot;Transparent&quot; &quot;PreviewType&quot;=&quot;Plane&quot; &quot;CanUseSpriteAtlas&quot;=&quot;True&quot; } Cull Off Lighting Off ZWrite Off Blend One OneMinusSrcAlpha //outline down Pass { Offset 1, 1 CGPROGRAM #pragma vertex vert #pragma fragment frag #pragma multi_compile _ PIXELSNAP_ON #pragma shader_feature ETC1_EXTERNAL_ALPHA v2f vert(appdata_t IN) { return vert_outline(IN); } fixed4 frag(v2f IN) : SV_Target { fixed4 texColor = SampleSpriteTexture (IN.texcoord + fixed2(0,_MainTex_TexelSize.y*_OutlineSize)) * IN.color; fixed4 c = _OutlineColor * texColor.a; return c; } ENDCG } //outline up Pass { Offset 1, 1 CGPROGRAM #pragma vertex vert #pragma fragment frag #pragma multi_compile _ PIXELSNAP_ON #pragma shader_feature ETC1_EXTERNAL_ALPHA v2f vert(appdata_t IN) { return vert_outline(IN); } fixed4 frag(v2f IN) : SV_Target { fixed4 texColor = SampleSpriteTexture (IN.texcoord + fixed2(0,- _MainTex_TexelSize.y* _OutlineSize)) * IN.color; fixed4 c = _OutlineColor * texColor.a; return c; } ENDCG } //outline left Pass { Offset 1, 1 CGPROGRAM #pragma vertex vert #pragma fragment frag #pragma multi_compile _ PIXELSNAP_ON #pragma shader_feature ETC1_EXTERNAL_ALPHA v2f vert(appdata_t IN) { return vert_outline(IN); } fixed4 frag(v2f IN) : SV_Target { fixed4 texColor = SampleSpriteTexture (IN.texcoord + fixed2(_MainTex_TexelSize.x* _OutlineSize,0)) * IN.color; fixed4 c = _OutlineColor * texColor.a; return c; } ENDCG } //outline right Pass { Offset 1, 1 CGPROGRAM #pragma vertex vert #pragma fragment frag #pragma multi_compile _ PIXELSNAP_ON #pragma shader_feature ETC1_EXTERNAL_ALPHA v2f vert(appdata_t IN) { return vert_outline(IN); } fixed4 frag(v2f IN) : SV_Target { fixed4 texColor = SampleSpriteTexture (IN.texcoord + fixed2(-_MainTex_TexelSize.x* _OutlineSize,0)) * IN.color; fixed4 c = _OutlineColor * texColor.a; return c; } ENDCG }// Pass { Offset 0, 0 CGPROGRAM #pragma vertex vert #pragma fragment frag #pragma multi_compile _ PIXELSNAP_ON #pragma shader_feature ETC1_EXTERNAL_ALPHA v2f vert(appdata_t IN) { v2f OUT; OUT.vertex = UnityObjectToClipPos(IN.vertex); OUT.texcoord = IN.texcoord; OUT.color = IN.color * _Color; #ifdef PIXELSNAP_ON OUT.vertex = UnityPixelSnap (OUT.vertex); #endif return OUT; } fixed4 frag(v2f IN) : SV_Target { fixed4 c = SampleSpriteTexture (IN.texcoord) * IN.color; c.rgb *= c.a; return c; } ENDCG } }} 总结： 这个方案思路简单明了，很好理解，但可以看到这直接产生了5个DC，损耗过大，舍去，但也不失为一个方案在此做一个记录。 方案三： 通过内收的方式描边，觉得也是个很好的思路，在这里做下记录 这里贴上代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182Shader &quot;Sprites/Outline&quot;{ Properties { [PerRendererData] _MainTex (&quot;Sprite Texture&quot;, 2D) = &quot;white&quot; {} _Color (&quot;Main texture Tint&quot;, Color) = (1,1,1,1) [Header(General Settings)] [MaterialToggle] _OutlineEnabled (&quot;Outline Enabled&quot;, Float) = 1 [MaterialToggle] _ConnectedAlpha (&quot;Connected Alpha&quot;, Float) = 0 [HideInInspector] _AlphaThreshold (&quot;Alpha clean&quot;, Range (0, 1)) = 0 _Thickness (&quot;Width (Max recommended 100)&quot;, float) = 10 [KeywordEnum(Contour, Frame)] _OutlineShape(&quot;Outline shape&quot;, Float) = 0 [KeywordEnum(Inside under sprite, Inside over sprite, Outside)] _OutlinePosition(&quot;Outline Position (Frame Only)&quot;, Float) = 0 _SolidOutline (&quot;Outline Color Base&quot;, Color) = (1,1,1,1) } SubShader { Tags { &quot;Queue&quot;=&quot;Transparent&quot; &quot;IgnoreProjector&quot;=&quot;True&quot; &quot;RenderType&quot;=&quot;Transparent&quot; &quot;PreviewType&quot;=&quot;Plane&quot; &quot;CanUseSpriteAtlas&quot;=&quot;True&quot; } Cull Off Lighting Off ZWrite Off Blend One OneMinusSrcAlpha Pass { CGPROGRAM #pragma vertex vert #pragma fragment frag #pragma multi_compile _ PIXELSNAP_ON #pragma exclude_renderers d3d11_9x #include &quot;UnityCG.cginc&quot; struct appdata_t { float4 vertex : POSITION; float4 color : COLOR; float2 texcoord : TEXCOORD0; }; struct v2f { float4 vertex : SV_POSITION; fixed4 color : COLOR; float2 texcoord : TEXCOORD0; }; fixed4 _Color; fixed _Thickness; fixed _OutlineEnabled; fixed _ConnectedAlpha; fixed _OutlineShape; //fixed _OutlineMode; fixed4 _SolidOutline; fixed _Weight; fixed _AlphaThreshold; v2f vert(appdata_t IN) { v2f OUT; OUT.vertex = UnityObjectToClipPos(IN.vertex); OUT.texcoord = IN.texcoord; OUT.color = IN.color * _Color; #ifdef PIXELSNAP_ON OUT.vertex = UnityPixelSnap (OUT.vertex); #endif return OUT; } sampler2D _MainTex; sampler2D _AlphaTex; float _AlphaSplitEnabled; uniform float4 _MainTex_TexelSize; //重新计算UV取样的 fixed4 SampleSpriteTexture (float2 uv) { float2 offsets; offsets = float2(_Thickness * 2, _Thickness * 2); float2 bigsize = float2(_MainTex_TexelSize.z, _MainTex_TexelSize.w); float2 smallsize = float2(_MainTex_TexelSize.z - offsets.x, _MainTex_TexelSize.w - offsets.y); float2 uv_changed = float2 ( uv.x * bigsize.x / smallsize.x - 0.5 * offsets.x / smallsize.x, uv.y * bigsize.y / smallsize.y - 0.5 * offsets.y / smallsize.y ); if(uv_changed.x &lt; 0 || uv_changed.x &gt; 1 || uv_changed.y &lt; 0 || uv_changed.y &gt; 1) { return float4(0, 0, 0, 0); } fixed4 color = tex2D (_MainTex, uv_changed);#if UNITY_TEXTURE_ALPHASPLIT_ALLOWED if (_AlphaSplitEnabled) color.a = tex2D (_AlphaTex, uv).r;#endif //UNITY_TEXTURE_ALPHASPLIT_ALLOWED return color; } //计算是否边为边缘 bool CheckOriginalSpriteTexture (float2 uv) { float thicknessX = _Thickness / _MainTex_TexelSize.z; float thicknessY = _Thickness / _MainTex_TexelSize.w; int steps = 100; float angle_step = 360.0 / steps; float alphaThreshold = _AlphaThreshold / 10; // check if the basic points has an alpha to speed up the process and not use the for loop bool outline = SampleSpriteTexture(uv + fixed2(0, +thicknessY)).a &gt; alphaThreshold || SampleSpriteTexture(uv + fixed2(0, -thicknessY)).a &gt; alphaThreshold || SampleSpriteTexture(uv + fixed2(+thicknessX, 0)).a &gt; alphaThreshold || SampleSpriteTexture(uv + fixed2(-thicknessX, 0)).a &gt; alphaThreshold || SampleSpriteTexture(uv + fixed2(+thicknessX * cos (3.14 / 4), -thicknessY * sin (3.14 / 4))).a &gt; alphaThreshold || SampleSpriteTexture(uv + fixed2(-thicknessX * cos (3.14 / 4), +thicknessY * sin (3.14 / 4))).a &gt; alphaThreshold || SampleSpriteTexture(uv + fixed2(-thicknessX * cos (3.14 / 4), -thicknessY * sin (3.14 / 4))).a &gt; alphaThreshold || SampleSpriteTexture(uv + fixed2(+thicknessX * cos (3.14 / 4), +thicknessY * sin (3.14 / 4))).a &gt; alphaThreshold; if(outline) return outline; for(int i = 0; i &lt; steps; i++) // high number and not a variable to avoid stupid compiler bugs { float angle = i * angle_step * 2 * 3.14 / 360; if( SampleSpriteTexture(uv + fixed2(thicknessX * cos(angle), thicknessY * sin(angle))).a &gt; alphaThreshold) { outline = true; break; } } return outline; } fixed4 frag(v2f IN) : SV_Target { fixed4 c = SampleSpriteTexture (IN.texcoord) * IN.color; c.rgb *= c.a; fixed4 outlineC = fixed4(0, 0, 0, 1); if(_OutlineEnabled != 0) { outlineC = _SolidOutline; if(_ConnectedAlpha != 0) { outlineC.a *= _Color.a; } outlineC.rgb *= outlineC.a; if( c.a == 0 &amp;&amp; CheckOriginalSpriteTexture(IN.texcoord)) { return outlineC; } return c; } return c; } ENDCG } }}","link":"/2022/08/27/SpriteOutline/"},{"title":"LeanrOpenGL1_DrawATriangle","text":"绘制一个三角形在OpenGL中是一个基本操作，，并假设已经设置了一个GLFW窗口和GLAD进行OpenGL的加载。 1. 定义三角形的顶点 定义三角形的三个顶点。下面是一个标准化设备坐标（Normalized Device Coordinates，NDC）中的三角形 12345float vertices[] = { -0.5f, -0.5f, 0.0f, // 左下角 0.5f, -0.5f, 0.0f, // 右下角 0.0f, 0.5f, 0.0f // 顶部}; 2. 创建顶点缓冲对象 (VBO) 和顶点数组对象 (VAO) VBO（Vertex Buffer Object） VBO是一个缓冲对象，用于在GPU上存储顶点数据。这些数据可以包括顶点的位置、颜色、法线、纹理坐标等。通过使用VBO，可以有效地将大量数据发送到GPU，并在图形管道中使用它们。当需要渲染几何体时，VBO允许GPU直接访问存储在显存中的数据，从而提供更快的性能。 在OpenGL中，VBO通常与GL_ARRAY_BUFFER目标一起使用，并通过以下方式创建和管理： glGenBuffers生成一个新的缓冲ID。 glBindBuffer绑定该缓冲ID。 glBufferData`或其他类似函数用于填充数据。 VAO（Vertex Array Object） VAO是一个对象，用于存储所有关于顶点属性的配置和与顶点缓冲对象（VBO）的关联。这意味着VAO存储了VBO的绑定信息和顶点属性指针配置。 VAO的优点是可以使用单个调用glBindVertexArray快速切换不同的顶点数据配置。这使得在相同的渲染循环中绘制具有不同顶点属性配置的多个对象更为容易和高效。 VAO本身并不存储任何顶点或索引数据。这些数据是存储在VBO和EBO中的。VAO更像是一个封装了如何从这些缓冲区读取数据的规则和设置的对象。 以下是创建和管理VAO的常用函数： glGenVertexArrays生成一个新的VAO ID。 glBindVertexArray绑定该VAO。 glVertexAttribPointer用于配置顶点属性。 VBO和VAO总结 VBO用于在GPU上存储顶点数据，如顶点的位置、颜色等。 VAO存储有关顶点属性配置和VBO关联的所有信息。 当绑定一个VAO并发起一个绘制调用时（比如glDrawArrays或glDrawElements），OpenGL会根据VAO中存储的状态配置来从相关联的VBO和/或EBO中获取数据。 VAO本身并不存储任何顶点或索引数据。这些数据是存储在VBO和EBO中的。VAO更像是一个封装了如何从这些缓冲区读取数据的规则和设置的对象。 这两者结合使用，使得组织和渲染具有复杂顶点属性的几何体变得更加高效和组织化。 首先，你需要创建一个顶点缓冲对象 (VBO) 来存储三角形的顶点数据，并创建一个顶点数组对象 (VAO) 来保存顶点属性配置。 123unsigned int VBO, VAO;glGenVertexArrays(1, &amp;VAO);glGenBuffers(1, &amp;VBO); 3. 绑定和配置VBO和VAO 将顶点数据上传到GPU，并配置顶点属性。 1234567891011121314// 绑定和配置VAOglBindVertexArray(VAO);// 绑定VBO并将顶点数据上传到GPUglBindBuffer(GL_ARRAY_BUFFER, VBO);glBufferData(GL_ARRAY_BUFFER, sizeof(vertices), vertices, GL_STATIC_DRAW);// 配置顶点属性glVertexAttribPointer(0, 3, GL_FLOAT, GL_FALSE, 3 * sizeof(float), (void*)0);glEnableVertexAttribArray(0);// 解绑glBindBuffer(GL_ARRAY_BUFFER, 0);glBindVertexArray(0); GL_ARRAY_BUFFER GL_ARRAY_BUFFER是OpenGL中的一个缓冲目标，通常与顶点缓冲对象（VBO）一起使用。当你在OpenGL中创建和操作缓冲对象时，你需要指定缓冲对象的类型或“目标”。GL_ARRAY_BUFFER就是这样的一个目标，专门用于顶点属性数据。 详细解释 GL_ARRAY_BUFFER: 这个目标用于存储顶点属性，如顶点位置、颜色、法线、纹理坐标等。当你使用如下命令绑定一个缓冲对象时： 1glBindBuffer(GL_ARRAY_BUFFER, bufferID); 你正在告诉OpenGL，后续的缓冲对象操作（例如，使用glBufferData上传数据）将影响与GL_ARRAY_BUFFER目标关联的缓冲对象。 绑定和解绑: 一旦你将缓冲对象绑定到GL_ARRAY_BUFFER，你可以进行许多操作，如上传数据、更改缓冲属性等。当你完成后，你可以通过将0传递给glBindBuffer来解绑缓冲对象： 1glBindBuffer(GL_ARRAY_BUFFER, 0); 为什么使用GL_ARRAY_BUFFER OpenGL定义了许多不同类型的缓冲目标，每个目标都用于特定的数据类型和操作。GL_ARRAY_BUFFER是其中之一，用于管理和使用顶点属性数据。其他缓冲目标可能用于索引数据（GL_ELEMENT_ARRAY_BUFFER）或其他特定用途。 通过使用GL_ARRAY_BUFFER等目标，OpenGL可以更好地理解如何使用数据，从而可能进行某些优化，同时也有助于更清晰地组织代码和理解数据的用途。 上传顶点数据 1glBufferData(GL_ARRAY_BUFFER, sizeof(vertices), vertices, GL_STATIC_DRAW); 这行代码将顶点数据从CPU内存上传到GPU显存。其中： GL_ARRAY_BUFFER：告诉OpenGL我们正在使用的缓冲类型。 sizeof(vertices)：定义了要上传的数据的大小（以字节为单位）。 vertices：是一个指向要上传的顶点数据的指针。 GL_STATIC_DRAW：这个标志提供了一些关于如何使用这些数据的提示。在这种情况下，它告诉OpenGL，我们不太可能更改这些数据（它是“静态”的），并且我们将主要用它来绘制（“绘制”）。 解绑 ​ 还没有渲染出图吗，怎么就能解绑，一般不是要渲染结束后才操作的吗？ 在OpenGL中，解绑操作通常是为了管理状态，确保不会产生意外的副作用。下面是详细解释为何在还没有进行渲染之前就进行解绑。 预配置与实际渲染的分离： 通常，在OpenGL中的渲染过程可以分为两个主要阶段：预配置和实际渲染。 在预配置阶段，你会创建和设置各种对象（如VBO、VAO、EBO、纹理、着色器等），但并不会进行实际的绘制操作。 在实际渲染阶段，你会绑定相应的对象和调用绘制命令。 VAO的作用与设计： VAO（Vertex Array Object）被设计为捕获与顶点属性相关的所有设置和状态。一旦这些信息被储存在VAO中，你可以随时通过绑定该VAO来恢复这些状态，无需重新绑定和设置VBO、EBO等。 状态恢复与防止副作用： 当你在实际渲染阶段绑定一个VAO，OpenGL会自动恢复与该VAO相关的所有状态和设置。因此，解绑在预配置阶段并不会影响实际渲染。 解绑是一种防范措施，用于避免在后续代码中不小心修改了这些对象的状态。 代码结构与可维护性： 通过将预配置和实际渲染逻辑分开，代码更为模块化，更易于维护和调试。 这种做法也使得你可以在多个渲染调用中重用相同的预配置，而不需要每次都重新设置。 专业实践与性能优化： 在复杂的OpenGL应用中，解绑操作可能还涉及到其他考虑因素，比如多线程环境中的状态管理，或者特定硬件和驱动程序对性能的影响。 4. 创建和编译着色器 你将需要一个顶点着色器来处理顶点数据，以及一个片段着色器来定义像素的颜色。 下面是顶点着色器和片段着色器的示例代码，这些代码可以直接插入你的主程序中。 1234567891011121314151617const char* vertexShaderSource = R&quot;glsl(#version 330 corelayout (location = 0) in vec3 aPos;void main(){ gl_Position = vec4(aPos.x, aPos.y, aPos.z, 1.0);})glsl&quot;;const char* fragmentShaderSource = R&quot;glsl(#version 330 coreout vec4 FragColor;void main(){ FragColor = vec4(1.0f, 0.5f, 0.2f, 1.0f);})glsl&quot;; 你可以使用以下代码段来编译和链接这些着色器（这一步与你提供的代码段相同）。 5. 渲染三角形 在你的渲染循环中，你可以绘制三角形如下： 123456789101112131415// 清除颜色缓冲glClearColor(0.2f, 0.3f, 0.3f, 1.0f);glClear(GL_COLOR_BUFFER_BIT);// 使用着色程序glUseProgram(shaderProgram);// 绑定VAOglBindVertexArray(VAO);// 绘制三角形glDrawArrays(GL_TRIANGLES, 0, 3);// 交换缓冲区glfwSwapBuffers(window); 总结 通过以上步骤，你应该能够在OpenGL窗口中绘制一个三角形。这包括定义三角形的顶点、创建和配置VBO和VAO、编写和编译着色器，以及在渲染循环中绘制三角形。这是许多更复杂的OpenGL项目的基础。 完整代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171#include &lt;glad/glad.h&gt;#include &lt;GLFW/glfw3.h&gt;#include &lt;iostream&gt;void framebuffer_size_callback(GLFWwindow* window, int width, int height);void processInput(GLFWwindow *window);// settingsconst unsigned int SCR_WIDTH = 800;const unsigned int SCR_HEIGHT = 600;const char* vertexShaderSource = &quot;#version 330 core\\n&quot;&quot;layout (location = 0) in vec3 aPos;\\n&quot;&quot;void main()\\n&quot;&quot;{\\n&quot;&quot; gl_Position = vec4(aPos.x, aPos.y, aPos.z, 1.0);\\n&quot;&quot;}\\0&quot;;const char* fragmentShaderSource = &quot;#version 330 core\\n&quot;&quot;out vec4 FragColor;\\n&quot;&quot;void main()\\n&quot;&quot;{\\n&quot;&quot; FragColor = vec4(1.0f, 0.5f, 0.2f, 1.0f);\\n&quot;&quot;}\\n\\0&quot;;int main(){ // glfw: initialize and configure // ------------------------------ glfwInit(); glfwWindowHint(GLFW_CONTEXT_VERSION_MAJOR, 3); glfwWindowHint(GLFW_CONTEXT_VERSION_MINOR, 3); glfwWindowHint(GLFW_OPENGL_PROFILE, GLFW_OPENGL_CORE_PROFILE); // glfw window creation // -------------------- GLFWwindow* window = glfwCreateWindow(SCR_WIDTH, SCR_HEIGHT, &quot;LearnOpenGL&quot;, NULL, NULL); if (window == NULL) { std::cout &lt;&lt; &quot;Failed to create GLFW window&quot; &lt;&lt; std::endl; glfwTerminate(); return -1; } glfwMakeContextCurrent(window); glfwSetFramebufferSizeCallback(window, framebuffer_size_callback); // glad: load all OpenGL function pointers // --------------------------------------- if (!gladLoadGLLoader((GLADloadproc)glfwGetProcAddress)) { std::cout &lt;&lt; &quot;Failed to initialize GLAD&quot; &lt;&lt; std::endl; return -1; } // build and compile our shader program // ------------------------------------ // vertex shader unsigned int vertexShader = glCreateShader(GL_VERTEX_SHADER); glShaderSource(vertexShader, 1, &amp;vertexShaderSource, NULL); glCompileShader(vertexShader); // check for shader compile errors int success; char infoLog[512]; glGetShaderiv(vertexShader, GL_COMPILE_STATUS, &amp;success); if (!success) { glGetShaderInfoLog(vertexShader, 512, NULL, infoLog); std::cout &lt;&lt; &quot;ERROR::SHADER::VERTEX::COMPILATION_FAILED\\n&quot; &lt;&lt; infoLog &lt;&lt; std::endl; } // fragment shader unsigned int fragmentShader = glCreateShader(GL_FRAGMENT_SHADER); glShaderSource(fragmentShader, 1, &amp;fragmentShaderSource, NULL); glCompileShader(fragmentShader); // check for shader compile errors glGetShaderiv(fragmentShader, GL_COMPILE_STATUS, &amp;success); if (!success) { glGetShaderInfoLog(fragmentShader, 512, NULL, infoLog); std::cout &lt;&lt; &quot;ERROR::SHADER::FRAGMENT::COMPILATION_FAILED\\n&quot; &lt;&lt; infoLog &lt;&lt; std::endl; } // link shaders unsigned int shaderProgram = glCreateProgram(); glAttachShader(shaderProgram, vertexShader); glAttachShader(shaderProgram, fragmentShader); glLinkProgram(shaderProgram); // check for linking errors glGetProgramiv(shaderProgram, GL_LINK_STATUS, &amp;success); if (!success) { glGetProgramInfoLog(shaderProgram, 512, NULL, infoLog); std::cout &lt;&lt; &quot;ERROR::SHADER::PROGRAM::LINKING_FAILED\\n&quot; &lt;&lt; infoLog &lt;&lt; std::endl; } glDeleteShader(vertexShader); glDeleteShader(fragmentShader); // set up vertex data (and buffer(s)) and configure vertex attributes // ------------------------------------------------------------------ float vertices[] = { -0.5f, -0.5f, 0.0f, // left 0.5f, -0.5f, 0.0f, // right 0.0f, 0.5f, 0.0f // top }; unsigned int VBO, VAO; glGenVertexArrays(1, &amp;VAO); glBindVertexArray(VAO);// bind the Vertex Array Object first, then bind and set vertex buffer(s), and then configure vertex attributes(s). glGenBuffers(1, &amp;VBO); glBindBuffer(GL_ARRAY_BUFFER, VBO); glBufferData(GL_ARRAY_BUFFER, sizeof(vertices), vertices, GL_STATIC_DRAW); glVertexAttribPointer(0, 3, GL_FLOAT, GL_FALSE, 3 * sizeof(float), (void*)0); glEnableVertexAttribArray(0); glBindBuffer(GL_ARRAY_BUFFER, 0); glBindVertexArray(0); // render loop // ----------- while (!glfwWindowShouldClose(window)) { // input // ----- processInput(window); // render // ------ glClearColor(0.2f, 0.3f, 0.3f, 1.0f); glClear(GL_COLOR_BUFFER_BIT); // draw our first triangle glUseProgram(shaderProgram); glBindVertexArray(VAO); // seeing as we only have a single VAO there's no need to bind it every time, but we'll do so to keep things a bit more organized glDrawArrays(GL_TRIANGLES, 0, 3); // glBindVertexArray(0); // no need to unbind it every time // glfw: swap buffers and poll IO events (keys pressed/released, mouse moved etc.) // ------------------------------------------------------------------------------- glfwSwapBuffers(window); glfwPollEvents(); } // optional: de-allocate all resources once they've outlived their purpose: // ------------------------------------------------------------------------ glDeleteVertexArrays(1, &amp;VAO); glDeleteBuffers(1, &amp;VBO); glDeleteProgram(shaderProgram); // glfw: terminate, clearing all previously allocated GLFW resources. // ------------------------------------------------------------------ glfwTerminate(); return 0;}// process all input: query GLFW whether relevant keys are pressed/released this frame and react accordingly// ---------------------------------------------------------------------------------------------------------void processInput(GLFWwindow *window){ if (glfwGetKey(window, GLFW_KEY_ESCAPE) == GLFW_PRESS) glfwSetWindowShouldClose(window, true);}// glfw: whenever the window size changed (by OS or user resize) this callback function executes// ---------------------------------------------------------------------------------------------void framebuffer_size_callback(GLFWwindow* window, int width, int height){ // make sure the viewport matches the new window dimensions; note that width and // height will be significantly larger than specified on retina displays. glViewport(0, 0, width, height);}","link":"/2023/07/20/LeanrOpenGL1-DrawATriangle/"},{"title":"关于做个人内容的一些想法","text":"想做一些个人内容 目前有个方面，就是做程序和生活相结合的方向 1、观察现实生活来实现一些东西：如实现音乐喷泉的效果 2、根据自己现在学到什么东西，然后做一些运用，如OpenGL中学到了混合，然后思考，能用混合做一些什么有趣的事情 3、在1上延伸，也可以观察网上，从网上然后得出一些idea 4、单纯是想实现一些酷炫的效果 5、参考https://www.shadertoy.com/上别人的作品 拍日常的学习生活 ​ 这部分可以不发在抖音，只发在B站和YouTuber，所有镜头记得要横屏 平时在学习中积累素材，如早上起来学习的过程，镜头可以电脑一直在录屏，还在拍一下侧面 平时的生活情况，加上运动情况，还有小猫咪, 茶茶 于处理第一个视频 在录视频的时候，可把边说边录，反正后面也可以再配，不用顾忌 关于能不能看到自己： 可以买专门拍vlogo的DV，或相机。能翻盖，就能看到自己了 关于视频处理","link":"/2023/11/06/%E5%85%B3%E4%BA%8E%E5%81%9A%E4%B8%AA%E4%BA%BA%E5%86%85%E5%AE%B9%E7%9A%84%E4%B8%80%E4%BA%9B%E6%83%B3%E6%B3%95/"},{"title":"读《高效能人人士的七个习惯》","text":"习惯二：以终为始 撰写个人使命宣言 清晰地定义个人的核心价值观、目标和目的，并将其表达出来的一种工具。通过撰写个人使命宣言，人们可以明确自己的生活目标、愿景和价值观，并以此为指导在各个领域做出决策和行动。通过表达和想像来加强个人价值观的理解和内化。反复思考和表达自己的核心价值观，并在心中进行反复演练，以达到潜移默化的转变。 个人使命宣言通常涵盖以下方面： 个人价值观和原则：明确自己的核心价值观和道德准则，例如诚实、正直、勇敢等。 长期目标和愿景：设定个人追求的长期目标和愿景，对未来有明确的方向和目标。 贡献和影响力：阐述自己希望通过自己的行为和工作对他人和社会产生的积极影响。 个人使命陈述：将以上内容整合成一个简洁而有力的陈述，概括自己的个人使命和意义。 撰写个人使命宣言有助于个人的自我认知和目标定位，可以提供清晰的方向，使个人在日常生活中更加有目标和意义。它也可以帮助人们更好地理解自己的价值观和优先级，并在面临抉择时做出更明智的决策。 思考： 读到这里时，感觉很有意思，在我工作生活中，特别是作为一个前端的程序员，工作，重复的工作占据了生活的大部分时间，当一有时候空闲下来就会感觉到很迷茫，不知道做什么，头脑经过长时间的工作也变得迟钝、很累想不出自己应该做什么。此时这些碎片的空余时间就会被无目地的刷刷论坛和短视频时里面消耗掉了。但与此同时作为一个996的前端工作者，是没有时间或很少会有成块的时间的。所以反复思考，反复演练，以达到潜移默化，对自己时间的见缝插针就变得尤其重要。 练习方式 练习表达和内化个人的核心价值观是一个逐渐深化和持续的过程。以下是一些方法和建议，可以帮助开始实践这个过程： 自省和反思：花时间自省，思考自己真正重要的事物和价值观。问自己一些关键问题，例如什么是对你来说最重要的？你希望在生活中实现什么？这些问题可以帮助你更好地理解自己的核心价值观。 撰写个人使命宣言：将你的核心价值观、长期目标和愿景写成一个简洁而有力的陈述。这个宣言可以作为你的指导原则，帮助你在决策和行动中保持一致性。 反复阅读和思考：将个人使命宣言作为参考，反复阅读和思考它的含义和意义。在日常生活中，抽出时间来仔细阅读和思考它，让这些价值观深入你的思维和意识。 想象和演练：在心中想象自己以这些价值观为指导的情景和行为。反复演练这些场景，想象自己如何应对和表现。通过想象和演练，你可以加强自己与这些价值观的联系，逐渐将它们融入到你的行为中。 实践和反馈：在日常生活中积极应用你的核心价值观。将它们应用于决策、行为和与他人的互动中。持续关注自己的实践，并反思自己的行为是否符合你的价值观。从反馈中学习和成长，并不断调整自己的行为。 记住，这个过程需要时间和持续的努力。通过不断地练习和实践，你可以逐渐加深对自己的价值观的理解，让它们在你的生活中起到更大的作用。 根据角色的不同，我们可能会有不同的使命或价值观。一个人在不同的角色中扮演不同的身份，例如作为家庭成员、职业人士、社区成员等，这些角色对应着不同的责任和期望。在每个角色中，你可以思考和界定相应的使命和价值观。例如，作为一个家庭成员，你的使命可能是关爱和支持家人，维系家庭的和谐；作为职业人士，你的使命可能是努力工作，为公司和社会做出贡献；作为社区成员，你的使命可能是参与社区活动，关注社会问题并积极参与改变。 通过对每个角色进行思考和界定，你可以明确每个角色的核心价值观和使命陈述。这有助于你在不同的角色中保持一致性，并更好地履行各个角色所带来的责任和使命。在实践中，要注意不同角色之间的平衡和协调。有时候，不同的角色可能会产生冲突或优先级的竞争，需要灵活处理和权衡。关键是要确保各个角色之间的关系相互支持和促进，使整个生活更加和谐和有意义。 习惯四：双赢思维 当谈到双赢思维时，它是指一种心态和方法，旨在寻求和创造双方都能从中受益的解决方案。以下是对双赢思维的总结： 合作而非竞争：双赢思维强调合作和协作，而非单纯的竞争。它鼓励人们以合作的方式与他人合作，寻求共同利益和共同成长，而不是通过竞争争取个人利益。 积极寻求解决方案：双赢思维的关键在于寻求解决方案，让各方都能获得满意的结果。它要求人们放下对抗和争斗的心态，积极探索创造性的解决方案，以满足各方的需求和利益。 尊重和理解他人：双赢思维强调对他人的尊重和理解。它鼓励人们倾听他人的观点和利益，尊重他人的权益和需要，从而建立积极的合作关系。 长期的眼光：双赢思维注重长期的发展和关系。它认识到通过建立良好的合作关系和互相支持，可以实现长期的持续性双赢局面，而不是短期的权宜之计。 创造价值：双赢思维强调创造共同的价值。它鼓励人们思考如何通过合作和协作，为各方带来更大的价值，从而推动共同的成功和成长。 总而言之，双赢思维是一种积极的心态和方法，它通过合作、寻求解决方案、尊重他人、长期发展和创造共同价值，致力于实现各方的共同利益和持续的成功。 如果无法实现双赢，那么放弃是一个可选的选择。这意味着在某些情况下，如果无法找到满足双方利益的解决方案，放弃当前的选择可能会更有益。放弃并不意味着放弃所有目标或牺牲自己的需要，而是指意识到当前的选择或争议不具备双赢的潜力，从而寻找更合适的解决方案或机会。这样做有助于避免陷入僵局、冲突或不健康的关系。 放弃的决策可能涉及一些权衡和考量，例如： 长远利益：评估当前选择是否会对个人或团体的长远利益产生负面影响。如果发现持续追求当前选择会导致不可弥补的损失或伤害，那么放弃可能是更明智的选择。 价值观和原则：将当前选择与自己的价值观和原则进行对比。如果当前选择违背了个人的核心价值观或原则，并且无法找到妥协或调和的方式，那么放弃可能更符合自己的内心需求。 替代机会：评估是否存在更好的替代机会或选择。有时候，放弃当前选择可能为更好的机会或解决方案打开了门，能够带来更大的利益和满足。 需要注意的是，放弃并不是一种逃避责任或避免困难的方式。它是在认识到当前选择无法实现双赢或对个人利益造成不利影响时，做出明智的决策。通过放弃，你为自己创造了更多的机会，寻找更符合自身价值观和目标的路径。 思考 这个观念乍一看有点空洞，假大空想感觉，对这些东西泛泛而谈，但似乎没有现实上的操作意义，什么都是理想上的太完美的想当然而以。但细想一下其实这个心态也确实可以运用到生活、工作、家庭中。如： 感情中：感情中是没有输赢的，当然也没有赢输。如果非要争个高低只有两败具伤，大家都不快乐。 工作中：我时常想如果我在公司中做多一点，花的时间多一点，那我的时间是不是就会少一点。但不花心思多一点，工作出来的结果永远都是差强人意，明明可以做到90分的，却只做80分出来自己满意，公司也不满意，甚至影响到别人对本人的判断，这又真的好吗。 等一下，我在公司工作，在工作时间发现了有些地方不完美，去完善它，公司给我了薪资，我做好工作，这本是我的本职上的事情，理所当然的事情啊。 为什么我会纠结呢？我纠结是忘记了什么是第一，什么是第二了。上班时间肯定工作第一，不是吗？ 我的个人使命： 健康第一 家庭第二 保持专注，注重效率，关注当下 去尝试，偿试所能想到的一切，人生不能重来，无所畏惧 保持成长，坚持阅读 6年挣100万，一年存15万，一个月存12k 与人相处，想其所想，思其所虑","link":"/2023/06/24/%E8%AF%BB%E3%80%8A%E9%AB%98%E6%95%88%E8%83%BD%E4%BA%BA%E4%BA%BA%E5%A3%AB%E7%9A%84%E4%B8%83%E4%B8%AA%E4%B9%A0%E6%83%AF%E3%80%8B/"}],"tags":[{"name":"生活微光","slug":"生活微光","link":"/tags/%E7%94%9F%E6%B4%BB%E5%BE%AE%E5%85%89/"},{"name":"openGL","slug":"openGL","link":"/tags/openGL/"},{"name":"书籍阅读","slug":"书籍阅读","link":"/tags/%E4%B9%A6%E7%B1%8D%E9%98%85%E8%AF%BB/"},{"name":"English","slug":"English","link":"/tags/English/"},{"name":"shader","slug":"shader","link":"/tags/shader/"},{"name":"match","slug":"match","link":"/tags/match/"}],"categories":[{"name":"生活微光","slug":"生活微光","link":"/categories/%E7%94%9F%E6%B4%BB%E5%BE%AE%E5%85%89/"},{"name":"OpenGL","slug":"OpenGL","link":"/categories/OpenGL/"},{"name":"书籍阅读","slug":"书籍阅读","link":"/categories/%E4%B9%A6%E7%B1%8D%E9%98%85%E8%AF%BB/"},{"name":"英文","slug":"英文","link":"/categories/%E8%8B%B1%E6%96%87/"},{"name":"shader","slug":"shader","link":"/categories/shader/"},{"name":"match","slug":"match","link":"/categories/match/"}],"pages":[{"title":"categories","text":"","link":"/categories/index.html"},{"title":"","text":"","link":"/js/custom.js"}]}